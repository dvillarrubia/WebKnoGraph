{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf_disxaDttM"
   },
   "source": [
    "# FINDING WORST PERFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzEVYK0Xlini"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_worst_candidates(file_path, folder_depth_level, n_worst):\n",
    "    df = pd.read_csv(file_path)\n",
    "    required_cols = {\"URL\", \"Folder_Depth\", \"PageRank\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {required_cols - set(df.columns)}\")\n",
    "    filtered_df = df[df[\"Folder_Depth\"] == folder_depth_level]\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=True)\n",
    "    worst_candidates = sorted_df.head(n_worst)\n",
    "    return worst_candidates[[\"URL\"]]\n",
    "\n",
    "\n",
    "# Inputs\n",
    "folder_depth_input = int(input(\"Enter folder depth level (integer): \"))\n",
    "n_worst_input = int(input(\"Enter number of worst candidates to retrieve: \"))\n",
    "\n",
    "# Run\n",
    "worst_pages = get_worst_candidates(file_path, folder_depth_input, n_worst_input)\n",
    "for url in worst_pages[\"URL\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40AizY_RDxmf"
   },
   "source": [
    "# FINDING BEST PERFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv2gtF1TDsf6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_top_performers(file_path, folder_depth_level, n_top):\n",
    "    \"\"\"\n",
    "    Retrieves the top N performing URLs (based on PageRank) for a specific folder depth level.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        folder_depth_level (int): The desired folder depth level to filter by.\n",
    "        n_top (int): The number of top performing URLs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the 'URL' column of the top performers.\n",
    "                          Returns an empty DataFrame if no matching data or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\", \"Folder_Depth\", \"PageRank\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Filter by folder depth\n",
    "    filtered_df = df[df[\"Folder_Depth\"] == folder_depth_level]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for folder depth level {folder_depth_level}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by 'PageRank' in descending order (highest PageRank = top performer)\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=False)\n",
    "\n",
    "    # Get the top N performers\n",
    "    top_performers = sorted_df.head(n_top)\n",
    "\n",
    "    return top_performers[[\"URL\"]]\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    folder_depth_input = int(input(\"Enter folder depth level (integer): \"))\n",
    "    n_top_input = int(input(\"Enter number of top performers to retrieve: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter integers for depth and count.\")\n",
    "    exit()  # Exit if inputs are not valid integers\n",
    "\n",
    "# --- Run Analysis ---\n",
    "print(\n",
    "    f\"\\nRetrieving top {n_top_input} performers for folder depth {folder_depth_input}:\"\n",
    ")\n",
    "top_pages = get_top_performers(file_path, folder_depth_input, n_top_input)\n",
    "\n",
    "if not top_pages.empty:\n",
    "    for url in top_pages[\"URL\"]:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No top performers found based on the provided criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbuR_a9ij-Hk"
   },
   "source": [
    "# FINDING RANDOM URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSs-MIP2j8pX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_random_urls(file_path, n_random):\n",
    "    \"\"\"\n",
    "    Retrieves a random sample of N URLs from the entire dataset, regardless of folder depth.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        n_random (int): The number of random URLs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the 'URL' column of the random URLs.\n",
    "                          Returns an empty DataFrame if no data or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\"}  # Only 'URL' is strictly required for this function\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"The CSV file is empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ensure n_random does not exceed the number of available URLs\n",
    "    if n_random > len(df):\n",
    "        print(\n",
    "            f\"Warning: Requested {n_random} random URLs, but only {len(df)} are available. Returning all available URLs.\"\n",
    "        )\n",
    "        n_random = len(df)\n",
    "\n",
    "    # Get a random sample of URLs\n",
    "    random_urls = df.sample(\n",
    "        n=n_random, random_state=None\n",
    "    )  # random_state=None ensures true randomness each run\n",
    "\n",
    "    return random_urls[[\"URL\"]]\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    n_random_input = int(input(\"Enter number of random URLs to retrieve: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer for the count.\")\n",
    "    exit()  # Exit if input is not a valid integer\n",
    "\n",
    "# --- Run Analysis ---\n",
    "print(f\"\\nRetrieving {n_random_input} random URLs:\")\n",
    "random_pages = get_random_urls(file_path, n_random_input)\n",
    "\n",
    "if not random_pages.empty:\n",
    "    for url in random_pages[\"URL\"]:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No random URLs found based on the provided criteria.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
