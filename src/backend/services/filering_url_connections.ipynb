{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf_disxaDttM"
   },
   "source": [
    "# FINDING WORST PERFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzEVYK0Xlini"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_worst_candidates(file_path, folder_depth_level, n_worst):\n",
    "    df = pd.read_csv(file_path)\n",
    "    required_cols = {\"URL\", \"Folder_Depth\", \"PageRank\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {required_cols - set(df.columns)}\")\n",
    "    filtered_df = df[df[\"Folder_Depth\"] == folder_depth_level]\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=True)\n",
    "    worst_candidates = sorted_df.head(n_worst)\n",
    "    return worst_candidates[[\"URL\"]]\n",
    "\n",
    "\n",
    "# Inputs\n",
    "folder_depth_input = int(input(\"Enter folder depth level (integer): \"))\n",
    "n_worst_input = int(input(\"Enter number of worst candidates to retrieve: \"))\n",
    "\n",
    "# Run\n",
    "worst_pages = get_worst_candidates(file_path, folder_depth_input, n_worst_input)\n",
    "for url in worst_pages[\"URL\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40AizY_RDxmf"
   },
   "source": [
    "# FINDING BEST PERFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv2gtF1TDsf6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_top_performers(file_path, folder_depth_level, n_top):\n",
    "    \"\"\"\n",
    "    Retrieves the top N performing URLs (based on PageRank) for a specific folder depth level.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        folder_depth_level (int): The desired folder depth level to filter by.\n",
    "        n_top (int): The number of top performing URLs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the 'URL' column of the top performers.\n",
    "                          Returns an empty DataFrame if no matching data or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\", \"Folder_Depth\", \"PageRank\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Filter by folder depth\n",
    "    filtered_df = df[df[\"Folder_Depth\"] == folder_depth_level]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for folder depth level {folder_depth_level}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort by 'PageRank' in descending order (highest PageRank = top performer)\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=False)\n",
    "\n",
    "    # Get the top N performers\n",
    "    top_performers = sorted_df.head(n_top)\n",
    "\n",
    "    return top_performers[[\"URL\"]]\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    folder_depth_input = int(input(\"Enter folder depth level (integer): \"))\n",
    "    n_top_input = int(input(\"Enter number of top performers to retrieve: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter integers for depth and count.\")\n",
    "    exit()  # Exit if inputs are not valid integers\n",
    "\n",
    "# --- Run Analysis ---\n",
    "print(\n",
    "    f\"\\nRetrieving top {n_top_input} performers for folder depth {folder_depth_input}:\"\n",
    ")\n",
    "top_pages = get_top_performers(file_path, folder_depth_input, n_top_input)\n",
    "\n",
    "if not top_pages.empty:\n",
    "    for url in top_pages[\"URL\"]:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No top performers found based on the provided criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbuR_a9ij-Hk"
   },
   "source": [
    "# FINDING RANDOM URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSs-MIP2j8pX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_random_urls(file_path, n_random):\n",
    "    \"\"\"\n",
    "    Retrieves a random sample of N URLs from the entire dataset, regardless of folder depth.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        n_random (int): The number of random URLs to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the 'URL' column of the random URLs.\n",
    "                          Returns an empty DataFrame if no data or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\"}  # Only 'URL' is strictly required for this function\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"The CSV file is empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ensure n_random does not exceed the number of available URLs\n",
    "    if n_random > len(df):\n",
    "        print(\n",
    "            f\"Warning: Requested {n_random} random URLs, but only {len(df)} are available. Returning all available URLs.\"\n",
    "        )\n",
    "        n_random = len(df)\n",
    "\n",
    "    # Get a random sample of URLs\n",
    "    random_urls = df.sample(\n",
    "        n=n_random, random_state=None\n",
    "    )  # random_state=None ensures true randomness each run\n",
    "\n",
    "    return random_urls[[\"URL\"]]\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    n_random_input = int(input(\"Enter number of random URLs to retrieve: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer for the count.\")\n",
    "    exit()  # Exit if input is not a valid integer\n",
    "\n",
    "# --- Run Analysis ---\n",
    "print(f\"\\nRetrieving {n_random_input} random URLs:\")\n",
    "random_pages = get_random_urls(file_path, n_random_input)\n",
    "\n",
    "if not random_pages.empty:\n",
    "    for url in random_pages[\"URL\"]:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No random URLs found based on the provided criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5SXzww0ujhd"
   },
   "source": [
    "# FILTERING URLS BY FOLDER/SUBPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgfiA5y8ujzB"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_best_pagerank_urls_in_folder(file_path, n_best, url_subpath_filter=None):\n",
    "    \"\"\"\n",
    "    Retrieves the top N best performing URLs (based on PageRank) from a specific folder/URL subpath.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        n_best (int): The number of best PageRank URLs to retrieve.\n",
    "        url_subpath_filter (str, optional): A URL subpath to filter by (e.g., '/learning-spaces/').\n",
    "                                            Only URLs containing this subpath will be considered. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the 'URL' column of the best PageRank URLs.\n",
    "                          Returns an empty DataFrame if no data, no matching URLs, or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\", \"PageRank\"}  # PageRank is still required for sorting\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"The CSV file is empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Apply URL subpath filter if provided\n",
    "    filtered_df = df\n",
    "    if url_subpath_filter:\n",
    "        filtered_df = df[\n",
    "            df[\"URL\"].astype(str).str.contains(url_subpath_filter, na=False)\n",
    "        ]\n",
    "        if filtered_df.empty:\n",
    "            print(f\"No URLs found matching the subpath filter: '{url_subpath_filter}'\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # Sort by 'PageRank' in descending order (highest PageRank = best performer)\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=False)\n",
    "\n",
    "    # Ensure n_best does not exceed the number of available URLs after filtering\n",
    "    if n_best > len(sorted_df):\n",
    "        print(\n",
    "            f\"Warning: Requested {n_best} best URLs, but only {len(sorted_df)} are available after filtering. Returning all available URLs.\"\n",
    "        )\n",
    "        n_best = len(sorted_df)\n",
    "\n",
    "    # Get the top N best PageRank URLs, only selecting the 'URL' column\n",
    "    best_pagerank_urls = sorted_df.head(n_best)[[\"URL\"]]\n",
    "\n",
    "    return best_pagerank_urls\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    n_best_input = int(input(\"Enter number of best PageRank URLs to retrieve: \"))\n",
    "    # Prompt for the subpath filter\n",
    "    url_subpath_input = input(\n",
    "        \"Enter URL subpath to filter by (e.g., /learning-spaces/, leave empty for no filter): \"\n",
    "    )\n",
    "    # Set to None if user leaves it empty\n",
    "    if not url_subpath_input.strip():\n",
    "        url_subpath_input = None\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer for the count.\")\n",
    "    exit()  # Exit if input is not a valid integer\n",
    "\n",
    "# --- Run Analysis ---\n",
    "if url_subpath_input:\n",
    "    print(\n",
    "        f\"\\nRetrieving {n_best_input} best PageRank URLs from subpath '{url_subpath_input}':\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nRetrieving {n_best_input} best PageRank URLs (no subpath filter):\")\n",
    "\n",
    "best_pages = get_best_pagerank_urls_in_folder(\n",
    "    file_path, n_best_input, url_subpath_input\n",
    ")\n",
    "\n",
    "if not best_pages.empty:\n",
    "    for url in best_pages[\"URL\"]:  # Iterate through the 'URL' column of the DataFrame\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No best PageRank URLs found based on the provided criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0huYdotnVlL"
   },
   "source": [
    "# FINDING BEST PERFORMERS ON FOLDER DEPTH INTERVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn5xIY1QnVWz"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Define path to CSV file\n",
    "base_path = \"/content/drive/My Drive/WebKnoGraph/data\"\n",
    "file_path = os.path.join(base_path, \"url_analysis_results.csv\")\n",
    "\n",
    "\n",
    "def get_overall_top_performers_in_range(file_path, min_depth, max_depth, n_total_top):\n",
    "    \"\"\"\n",
    "    Retrieves the overall top N performing URLs (based on PageRank) within a defined range of folder depth levels.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing URL analysis results.\n",
    "        min_depth (int): The minimum folder depth level in the range (inclusive).\n",
    "        max_depth (int): The maximum folder depth level in the range (inclusive).\n",
    "        n_total_top (int): The total number of top performing URLs to retrieve across the entire range.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the 'URL' column of the overall top performers.\n",
    "                          Returns an empty DataFrame if no matching data or missing columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "    required_cols = {\"URL\", \"Folder_Depth\", \"PageRank\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            f\"Missing one or more required columns: {required_cols - set(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Filter by the defined folder depth range\n",
    "    filtered_df = df[\n",
    "        (df[\"Folder_Depth\"] >= min_depth) & (df[\"Folder_Depth\"] <= max_depth)\n",
    "    ]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found for folder depths between {min_depth} and {max_depth}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Sort the entire filtered DataFrame by 'PageRank' in descending order\n",
    "    sorted_df = filtered_df.sort_values(by=\"PageRank\", ascending=False)\n",
    "\n",
    "    # Get the overall top N performers\n",
    "    overall_top_performers = sorted_df.head(n_total_top)\n",
    "\n",
    "    return overall_top_performers[[\"URL\"]]\n",
    "\n",
    "\n",
    "# --- User Inputs ---\n",
    "try:\n",
    "    min_depth_input = int(input(\"Enter minimum folder depth level (integer): \"))\n",
    "    max_depth_input = int(input(\"Enter maximum folder depth level (integer): \"))\n",
    "    n_total_top_input = int(input(\"Enter total number of top performers to retrieve: \"))\n",
    "\n",
    "    if min_depth_input > max_depth_input:\n",
    "        print(\"Error: Minimum depth cannot be greater than maximum depth.\")\n",
    "        exit()\n",
    "\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter integers for depth range and count.\")\n",
    "    exit()  # Exit if inputs are not valid integers\n",
    "\n",
    "# --- Run Analysis ---\n",
    "print(\n",
    "    f\"\\nRetrieving the overall top {n_total_top_input} performers from folder depths {min_depth_input} to {max_depth_input}:\"\n",
    ")\n",
    "overall_top_pages = get_overall_top_performers_in_range(\n",
    "    file_path, min_depth_input, max_depth_input, n_total_top_input\n",
    ")\n",
    "\n",
    "if not overall_top_pages.empty:\n",
    "    for url in overall_top_pages[\"URL\"]:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No overall top performers found based on the provided criteria.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
