{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYgtCa_8srDm",
    "outputId": "292da257-91f2-4be6-f844-24537c606e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkit in /usr/local/lib/python3.12/dist-packages (11.1.post1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from networkit) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "======================================================================\n",
      "NetworKit Multi-Range BA PageRank Simulation WITH CHECKPOINTS\n",
      "======================================================================\n",
      "NetworKit version: 11.1.post1\n",
      "Boosting Rounds: 10\n",
      "Bridgings per Round: 10\n",
      "Total Simulations per Comparison: 100\n",
      "\n",
      "==========================CONNECTION RANGES===========================\n",
      "  Range_5-35: 5-35 connections\n",
      "  Range_35-65: 35-65 connections\n",
      "  Range_65-95: 65-95 connections\n",
      "======================================================================\n",
      "\n",
      "=====================SEO OPTIMIZATION PARAMETERS======================\n",
      "PageRank Damping Factor: 0.8\n",
      "PageRank Tolerance: 1e-06\n",
      "Neutral Threshold: ¬±0.025%\n",
      "======================================================================\n",
      "\n",
      "Strategies: 5\n",
      "Connection Ranges: 3\n",
      "Total Strategy-Range Combinations: 15\n",
      "Mounted at /content/drive\n",
      "‚úì Google Drive mounted successfully!\n",
      "\n",
      "‚úì Baseline: link_graph_edges.csv\n",
      "\n",
      "Loading baseline graph...\n",
      "‚úì Loaded: 1,841 nodes, 122,066 edges\n",
      "‚úì Output folder: /content/drive/MyDrive/WebKnoGraph/results/automatic_led/bar_results_automatic\n",
      "\n",
      "======================================================================\n",
      "CHECKPOINT STATUS\n",
      "======================================================================\n",
      "Completed: 0/15 (0.0%)\n",
      "Remaining: 15\n",
      "======================================================================\n",
      "\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "CONNECTION RANGE: Range_5-35 (5-35)\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "[RUN 1/15]\n",
      "\n",
      "\n",
      "######################################################################\n",
      "STRATEGY: random_batches | RANGE: Range_5-35 (5-35)\n",
      "######################################################################\n",
      "‚úì Found 10 comparison files\n",
      "  Processing: 240_random_updated_link_graph_4 @ Range_5-35... "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NetworKit-based PageRank Simulation - Multi-Strategy with Checkpoints\n",
    "# with Automatic checkpoint saving and resume capability\n",
    "# ============================================================================\n",
    "\n",
    "!pip install networkit pandas numpy matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkit as nk\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# === BOOSTING CONFIGURATION ===\n",
    "NUM_BOOSTING_ROUNDS = 10\n",
    "BRIDGINGS_PER_ROUND = 10\n",
    "TOTAL_SIMULATIONS = NUM_BOOSTING_ROUNDS * BRIDGINGS_PER_ROUND\n",
    "\n",
    "# === THREE CONNECTION RANGES ===\n",
    "CONNECTION_RANGES = [\n",
    "    (5, 35, \"Range_5-35\"),\n",
    "    (35, 65, \"Range_35-65\"),\n",
    "    (65, 95, \"Range_65-95\"),\n",
    "]\n",
    "\n",
    "# === THRESHOLD CONFIGURATION ===\n",
    "NEUTRAL_THRESHOLD = 0.025  # ¬±0.025% considered neutral\n",
    "\n",
    "# === SIMULATION PARAMETERS ===\n",
    "TOTAL_NODES_WWW = 500000\n",
    "EDGES_PER_NEW_NODE = 3\n",
    "PAGERANK_TOLERANCE = 1e-6\n",
    "PAGERANK_DAMPING = 0.80\n",
    "\n",
    "# === FILE PATHS ===\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "\n",
    "COMPARISON_FOLDERS = [\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/random_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/high_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/folder_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/mixed_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/low_batches/\",\n",
    "]\n",
    "\n",
    "# === CACHE ===\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CHECKPOINT SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages checkpoint saving and loading for resumable simulations using pickle\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.checkpoint_file = self.checkpoint_dir / \"simulation_checkpoint.pkl\"\n",
    "        self.completed_combinations = set()\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"Load existing checkpoint if available\"\"\"\n",
    "        if self.checkpoint_file.exists():\n",
    "            try:\n",
    "                with open(self.checkpoint_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    self.completed_combinations = data.get(\"completed\", set())\n",
    "                print(\n",
    "                    f\"‚úì Loaded checkpoint: {len(self.completed_combinations)} combinations already completed\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Could not load checkpoint: {e}\")\n",
    "                self.completed_combinations = set()\n",
    "\n",
    "    def save_checkpoint(self, strategy_name, range_name, min_conn, max_conn):\n",
    "        \"\"\"Save checkpoint after completing a strategy-range combination\"\"\"\n",
    "        combination = (strategy_name, range_name, min_conn, max_conn)\n",
    "        self.completed_combinations.add(combination)\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"completed\": self.completed_combinations,\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"total_completed\": len(self.completed_combinations),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(self.checkpoint_file, \"wb\") as f:\n",
    "                pickle.dump(checkpoint_data, f)\n",
    "            print(f\"  ‚úì Checkpoint saved ({len(self.completed_combinations)} total)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö† Could not save checkpoint: {e}\")\n",
    "\n",
    "    def is_completed(self, strategy_name, range_name, min_conn, max_conn):\n",
    "        \"\"\"Check if a strategy-range combination has been completed\"\"\"\n",
    "        combination = (strategy_name, range_name, min_conn, max_conn)\n",
    "        return combination in self.completed_combinations\n",
    "\n",
    "    def get_progress(self, total_combinations):\n",
    "        \"\"\"Get progress statistics\"\"\"\n",
    "        completed = len(self.completed_combinations)\n",
    "        remaining = total_combinations - completed\n",
    "        percent = (\n",
    "            (completed / total_combinations * 100) if total_combinations > 0 else 0\n",
    "        )\n",
    "        return {\n",
    "            \"completed\": completed,\n",
    "            \"remaining\": remaining,\n",
    "            \"total\": total_combinations,\n",
    "            \"percent\": percent,\n",
    "        }\n",
    "\n",
    "    def reset_checkpoint(self):\n",
    "        \"\"\"Reset checkpoint (use with caution)\"\"\"\n",
    "        if self.checkpoint_file.exists():\n",
    "            self.checkpoint_file.unlink()\n",
    "        self.completed_combinations = set()\n",
    "        print(\"‚úì Checkpoint reset\")\n",
    "\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive in Colab environment\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        print(\"‚úì Google Drive mounted successfully!\")\n",
    "        return True\n",
    "    except:\n",
    "        print(\"‚ö† Not in Colab - skipping drive mount\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path):\n",
    "    \"\"\"Load directed graph from CSV with FROM/TO columns\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "    from_urls = df[\"FROM\"].values\n",
    "    to_urls = df[\"TO\"].values\n",
    "\n",
    "    if len(from_urls) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "    url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "    g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "    for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "        g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "    return g, all_urls, url_to_idx\n",
    "\n",
    "\n",
    "def create_www_graph_networkit(n_nodes, m_edges, seed=42):\n",
    "    \"\"\"Create Barab√°si-Albert graph representing WWW with caching\"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    cache_key = (n_nodes, m_edges, seed)\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph\n",
    "\n",
    "    nk.setSeed(seed, False)\n",
    "    generator = nk.generators.BarabasiAlbertGenerator(\n",
    "        k=m_edges, nMax=n_nodes, n0=m_edges\n",
    "    )\n",
    "    www_graph = generator.generate()\n",
    "\n",
    "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (cache_key, cached_graph)\n",
    "\n",
    "    return www_graph\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph, kalicube_edges, kalicube_nodes, min_connections, max_connections\n",
    "):\n",
    "    \"\"\"Merge Kalicube graph with WWW and calculate PageRank with specified connection range\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "\n",
    "    merged_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    n_connections = np.random.randint(min_connections, max_connections + 1)\n",
    "    n_www_sample = min(n_connections, TOTAL_NODES_WWW)\n",
    "    n_kalicube_sample = min(n_connections, len(kalicube_nodes))\n",
    "\n",
    "    www_nodes_sample = np.random.choice(\n",
    "        TOTAL_NODES_WWW, size=n_www_sample, replace=False\n",
    "    )\n",
    "    kalicube_indices = np.random.choice(\n",
    "        len(kalicube_nodes), size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        merged_graph.addEdge(www_node_id, kalicube_idx + kalicube_offset)\n",
    "\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=PAGERANK_DAMPING, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        pagerank_dict[url] = pagerank_scores[i + kalicube_offset]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def classify_delta(delta_pct):\n",
    "    \"\"\"Classify delta as positive, negative, or neutral\"\"\"\n",
    "    if delta_pct > NEUTRAL_THRESHOLD:\n",
    "        return \"positive\"\n",
    "    elif delta_pct < -NEUTRAL_THRESHOLD:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "def run_boosting_round(\n",
    "    round_id,\n",
    "    old_edges,\n",
    "    new_edges,\n",
    "    old_nodes,\n",
    "    new_nodes,\n",
    "    page_deltas_tracker,\n",
    "    min_conn,\n",
    "    max_conn,\n",
    "):\n",
    "    \"\"\"Run one boosting round with multiple bridging simulations\"\"\"\n",
    "    delta_pcts_all = []\n",
    "\n",
    "    for bridging_id in range(BRIDGINGS_PER_ROUND):\n",
    "        sim_id = round_id * BRIDGINGS_PER_ROUND + bridging_id\n",
    "        sim_seed = 42 + sim_id\n",
    "\n",
    "        np.random.seed(sim_seed)\n",
    "        random.seed(sim_seed)\n",
    "\n",
    "        www_graph = create_www_graph_networkit(\n",
    "            TOTAL_NODES_WWW, EDGES_PER_NEW_NODE, sim_seed\n",
    "        )\n",
    "\n",
    "        pagerank_old = process_configuration_networkit(\n",
    "            www_graph, old_edges, old_nodes, min_conn, max_conn\n",
    "        )\n",
    "        pagerank_new = process_configuration_networkit(\n",
    "            www_graph, new_edges, new_nodes, min_conn, max_conn\n",
    "        )\n",
    "\n",
    "        common_urls = set(pagerank_old.keys()) & set(pagerank_new.keys())\n",
    "        if not common_urls:\n",
    "            continue\n",
    "\n",
    "        for url in common_urls:\n",
    "            old_val = pagerank_old[url]\n",
    "            new_val = pagerank_new[url]\n",
    "            delta = new_val - old_val\n",
    "            delta_pct = (delta / max(old_val, 1e-10)) * 100\n",
    "            delta_pcts_all.append(delta_pct)\n",
    "            page_deltas_tracker[url].append(delta_pct)\n",
    "\n",
    "    return delta_pcts_all, page_deltas_tracker\n",
    "\n",
    "\n",
    "def run_boosted_comparison(\n",
    "    baseline_data, comparison_file, min_conn, max_conn, range_name\n",
    "):\n",
    "    \"\"\"Run complete boosted comparison with specific connection range\"\"\"\n",
    "    comparison_name = comparison_file.stem\n",
    "    print(f\"  Processing: {comparison_name} @ {range_name}...\", end=\" \", flush=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    g_old, nodes_old, _ = baseline_data\n",
    "    g_new, nodes_new, _ = load_graph_from_csv_networkit(comparison_file)\n",
    "\n",
    "    if g_new is None:\n",
    "        print(f\"‚úó Failed\")\n",
    "        return None\n",
    "\n",
    "    old_edges = [(u, v) for u, v in g_old.iterEdges()]\n",
    "    new_edges = [(u, v) for u, v in g_new.iterEdges()]\n",
    "    old_nodes = nodes_old\n",
    "    new_nodes = nodes_new\n",
    "\n",
    "    del g_new\n",
    "    gc.collect()\n",
    "\n",
    "    page_deltas_tracker = defaultdict(list)\n",
    "    all_deltas = []\n",
    "\n",
    "    for round_id in range(NUM_BOOSTING_ROUNDS):\n",
    "        round_deltas, page_deltas_tracker = run_boosting_round(\n",
    "            round_id,\n",
    "            old_edges,\n",
    "            new_edges,\n",
    "            old_nodes,\n",
    "            new_nodes,\n",
    "            page_deltas_tracker,\n",
    "            min_conn,\n",
    "            max_conn,\n",
    "        )\n",
    "        all_deltas.extend(round_deltas)\n",
    "\n",
    "    if not all_deltas:\n",
    "        print(\"‚úó No results\")\n",
    "        return None\n",
    "\n",
    "    pages_positive = 0\n",
    "    pages_negative = 0\n",
    "    pages_neutral = 0\n",
    "\n",
    "    for url, deltas in page_deltas_tracker.items():\n",
    "        avg_delta = np.mean(deltas)\n",
    "        classification = classify_delta(avg_delta)\n",
    "\n",
    "        if classification == \"positive\":\n",
    "            pages_positive += 1\n",
    "        elif classification == \"negative\":\n",
    "            pages_negative += 1\n",
    "        else:\n",
    "            pages_neutral += 1\n",
    "\n",
    "    total_pages = len(page_deltas_tracker)\n",
    "    all_deltas = np.array(all_deltas)\n",
    "\n",
    "    final_mean = np.mean(all_deltas)\n",
    "    final_max = np.max(all_deltas)\n",
    "    final_min = np.min(all_deltas)\n",
    "    final_std = np.std(all_deltas)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"‚úì Mean: {final_mean:+.3f}% [{duration:.1f}s]\")\n",
    "\n",
    "    return {\n",
    "        \"name\": comparison_name,\n",
    "        \"range_name\": range_name,\n",
    "        \"min_connections\": min_conn,\n",
    "        \"max_connections\": max_conn,\n",
    "        \"duration\": duration,\n",
    "        \"mean\": final_mean,\n",
    "        \"max\": final_max,\n",
    "        \"min\": final_min,\n",
    "        \"std\": final_std,\n",
    "        \"pages_up\": pages_positive,\n",
    "        \"pages_down\": pages_negative,\n",
    "        \"pages_neutral\": pages_neutral,\n",
    "        \"total_pages\": total_pages,\n",
    "        \"num_simulations\": TOTAL_SIMULATIONS,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_strategy_summary(all_results, output_folder, strategy_name, range_name):\n",
    "    \"\"\"Create summary for a single strategy at a specific range\"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"STRATEGY SUMMARY: {strategy_name} @ {range_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    data = []\n",
    "    for r in all_results:\n",
    "        if r:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Comparison\": r[\"name\"],\n",
    "                    \"Range\": r[\"range_name\"],\n",
    "                    \"Min_Conn\": r[\"min_connections\"],\n",
    "                    \"Max_Conn\": r[\"max_connections\"],\n",
    "                    \"Mean_Delta_%\": r[\"mean\"],\n",
    "                    \"Max_Delta_%\": r[\"max\"],\n",
    "                    \"Min_Delta_%\": r[\"min\"],\n",
    "                    \"Std_Delta_%\": r[\"std\"],\n",
    "                    \"Pages_Up\": r[\"pages_up\"],\n",
    "                    \"Pages_Down\": r[\"pages_down\"],\n",
    "                    \"Pages_Neutral\": r[\"pages_neutral\"],\n",
    "                    \"Total_Pages\": r[\"total_pages\"],\n",
    "                    \"Duration_m\": r[\"duration\"] / 60,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"‚úó No valid results\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(\"Mean_Delta_%\", ascending=False)\n",
    "\n",
    "    # Format strategy name to Title_Case\n",
    "    formatted_strategy = \"_\".join(\n",
    "        word.capitalize() for word in strategy_name.split(\"_\")\n",
    "    )\n",
    "\n",
    "    summary_path = output_folder / f\"{range_name}_{formatted_strategy}.csv\"\n",
    "    df.to_csv(summary_path, index=False)\n",
    "\n",
    "    print(\"\\nRankings by Mean Delta %:\")\n",
    "    for idx, row in df.iterrows():\n",
    "        symbol = (\n",
    "            \"‚Üë\"\n",
    "            if row[\"Mean_Delta_%\"] > NEUTRAL_THRESHOLD\n",
    "            else \"‚Üì\"\n",
    "            if row[\"Mean_Delta_%\"] < -NEUTRAL_THRESHOLD\n",
    "            else \"‚Üí\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  {symbol} {row['Comparison']}: {row['Mean_Delta_%']:+.3f}% \"\n",
    "            + f\"(‚Üë{row['Pages_Up']:.0f} ‚Üì{row['Pages_Down']:.0f} ‚Üí{row['Pages_Neutral']:.0f})\"\n",
    "        )\n",
    "\n",
    "    strategy_avg_mean = df[\"Mean_Delta_%\"].mean()\n",
    "    strategy_avg_max = df[\"Max_Delta_%\"].mean()\n",
    "    strategy_avg_min = df[\"Min_Delta_%\"].mean()\n",
    "    strategy_avg_up = df[\"Pages_Up\"].mean()\n",
    "    strategy_avg_down = df[\"Pages_Down\"].mean()\n",
    "    strategy_avg_neutral = df[\"Pages_Neutral\"].mean()\n",
    "\n",
    "    print(f\"\\nStrategy Averages for {range_name}:\")\n",
    "    print(f\"  Avg Mean Delta:   {strategy_avg_mean:+.3f}%\")\n",
    "    print(f\"  Avg Max Delta:    {strategy_avg_max:+.3f}%\")\n",
    "    print(f\"  Avg Min Delta:    {strategy_avg_min:+.3f}%\")\n",
    "    print(f\"  Avg Pages Up:     {strategy_avg_up:.1f}\")\n",
    "    print(f\"  Avg Pages Down:   {strategy_avg_down:.1f}\")\n",
    "    print(f\"  Avg Pages Neutral: {strategy_avg_neutral:.1f}\")\n",
    "    print(f\"\\n‚úì Saved: {summary_path.name}\")\n",
    "\n",
    "    return {\n",
    "        \"strategy_name\": strategy_name,\n",
    "        \"range_name\": range_name,\n",
    "        \"min_connections\": data[0][\"Min_Conn\"],\n",
    "        \"max_connections\": data[0][\"Max_Conn\"],\n",
    "        \"avg_mean\": strategy_avg_mean,\n",
    "        \"avg_max\": strategy_avg_max,\n",
    "        \"avg_min\": strategy_avg_min,\n",
    "        \"avg_up\": strategy_avg_up,\n",
    "        \"avg_down\": strategy_avg_down,\n",
    "        \"avg_neutral\": strategy_avg_neutral,\n",
    "        \"num_comparisons\": len(data),\n",
    "    }\n",
    "\n",
    "\n",
    "def update_overall_tracker(overall_tracker_path, strategy_result):\n",
    "    \"\"\"Update and save overall tracker\"\"\"\n",
    "    if overall_tracker_path.exists():\n",
    "        tracker_df = pd.read_csv(overall_tracker_path)\n",
    "        existing_data = tracker_df.to_dict(\"records\")\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.append(\n",
    "        {\n",
    "            \"Strategy\": strategy_result[\"strategy_name\"],\n",
    "            \"Range\": strategy_result[\"range_name\"],\n",
    "            \"Min_Connections\": strategy_result[\"min_connections\"],\n",
    "            \"Max_Connections\": strategy_result[\"max_connections\"],\n",
    "            \"Overall_Avg_Mean_%\": strategy_result[\"avg_mean\"],\n",
    "            \"Overall_Avg_Max_%\": strategy_result[\"avg_max\"],\n",
    "            \"Overall_Avg_Min_%\": strategy_result[\"avg_min\"],\n",
    "            \"Overall_Avg_Pages_Up\": strategy_result[\"avg_up\"],\n",
    "            \"Overall_Avg_Pages_Down\": strategy_result[\"avg_down\"],\n",
    "            \"Overall_Avg_Pages_Neutral\": strategy_result[\"avg_neutral\"],\n",
    "            \"Num_Comparisons\": strategy_result[\"num_comparisons\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tracker_df = pd.DataFrame(existing_data)\n",
    "    tracker_df.to_csv(overall_tracker_path, index=False)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\n",
    "        f\"OVERALL AVERAGES: {strategy_result['strategy_name']} @ {strategy_result['range_name']}\"\n",
    "    )\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(\n",
    "        f\"Connection Range:        {strategy_result['min_connections']}-{strategy_result['max_connections']}\"\n",
    "    )\n",
    "    print(f\"Comparison Files:        {strategy_result['num_comparisons']}\")\n",
    "    print(\n",
    "        f\"Total Simulations:       {strategy_result['num_comparisons'] * TOTAL_SIMULATIONS:,}\"\n",
    "    )\n",
    "    print(f\"\\nOverall Averages:\")\n",
    "    print(f\"  Overall Avg Mean:        {strategy_result['avg_mean']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Max:         {strategy_result['avg_max']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Min:         {strategy_result['avg_min']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Pages Up:    {strategy_result['avg_up']:.1f}\")\n",
    "    print(f\"  Overall Avg Pages Down:  {strategy_result['avg_down']:.1f}\")\n",
    "    print(f\"  Overall Avg Pages Neutral: {strategy_result['avg_neutral']:.1f}\")\n",
    "    print(f\"\\n‚úì Saved to: {overall_tracker_path.name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "\n",
    "def process_strategy_with_range(\n",
    "    baseline_data,\n",
    "    comparison_folder,\n",
    "    strategy_name,\n",
    "    min_conn,\n",
    "    max_conn,\n",
    "    range_name,\n",
    "    main_output_folder,\n",
    "    overall_tracker_path,\n",
    "    checkpoint_manager,\n",
    "):\n",
    "    \"\"\"Process a single strategy folder with specific connection range\"\"\"\n",
    "\n",
    "    # Check if already completed\n",
    "    if checkpoint_manager.is_completed(strategy_name, range_name, min_conn, max_conn):\n",
    "        print(f\"\\n‚è≠ SKIPPING (already completed): {strategy_name} @ {range_name}\")\n",
    "        return None\n",
    "\n",
    "    comparison_folder = Path(comparison_folder)\n",
    "\n",
    "    print(f\"\\n\\n{'#' * 70}\")\n",
    "    print(f\"STRATEGY: {strategy_name} | RANGE: {range_name} ({min_conn}-{max_conn})\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "\n",
    "    comparison_files = list(comparison_folder.glob(\"*.csv\"))\n",
    "\n",
    "    if not comparison_files:\n",
    "        print(f\"‚úó No CSV files in: {comparison_folder}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"‚úì Found {len(comparison_files)} comparison files\")\n",
    "\n",
    "    all_results = []\n",
    "    for i, comp_file in enumerate(comparison_files, 1):\n",
    "        result = run_boosted_comparison(\n",
    "            baseline_data, comp_file, min_conn, max_conn, range_name\n",
    "        )\n",
    "        all_results.append(result)\n",
    "\n",
    "        global _www_graph_cache\n",
    "        _www_graph_cache = None\n",
    "        gc.collect()\n",
    "\n",
    "    strategy_result = create_strategy_summary(\n",
    "        all_results, main_output_folder, strategy_name, range_name\n",
    "    )\n",
    "\n",
    "    if strategy_result:\n",
    "        update_overall_tracker(overall_tracker_path, strategy_result)\n",
    "        # Save checkpoint after successful completion\n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            strategy_name, range_name, min_conn, max_conn\n",
    "        )\n",
    "\n",
    "    return strategy_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"NetworKit Multi-Range BA PageRank Simulation WITH CHECKPOINTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"NetworKit version: {nk.__version__}\")\n",
    "    print(f\"Boosting Rounds: {NUM_BOOSTING_ROUNDS}\")\n",
    "    print(f\"Bridgings per Round: {BRIDGINGS_PER_ROUND}\")\n",
    "    print(f\"Total Simulations per Comparison: {TOTAL_SIMULATIONS}\")\n",
    "\n",
    "    print(f\"\\n{'CONNECTION RANGES':=^70}\")\n",
    "    for min_c, max_c, name in CONNECTION_RANGES:\n",
    "        print(f\"  {name}: {min_c}-{max_c} connections\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\n{'SEO OPTIMIZATION PARAMETERS':=^70}\")\n",
    "    print(f\"PageRank Damping Factor: {PAGERANK_DAMPING}\")\n",
    "    print(f\"PageRank Tolerance: {PAGERANK_TOLERANCE}\")\n",
    "    print(f\"Neutral Threshold: ¬±{NEUTRAL_THRESHOLD}%\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    total_runs = len(COMPARISON_FOLDERS) * len(CONNECTION_RANGES)\n",
    "    print(f\"\\nStrategies: {len(COMPARISON_FOLDERS)}\")\n",
    "    print(f\"Connection Ranges: {len(CONNECTION_RANGES)}\")\n",
    "    print(f\"Total Strategy-Range Combinations: {total_runs}\")\n",
    "\n",
    "    mount_google_drive()\n",
    "\n",
    "    baseline_path = Path(BASELINE_PATH)\n",
    "    if not baseline_path.exists():\n",
    "        print(f\"\\n‚úó Baseline not found: {BASELINE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\n‚úì Baseline: {baseline_path.name}\")\n",
    "\n",
    "    print(\"\\nLoading baseline graph...\")\n",
    "    baseline_data = load_graph_from_csv_networkit(baseline_path)\n",
    "    if baseline_data[0] is None:\n",
    "        print(\"‚úó Failed to load baseline\")\n",
    "        exit(1)\n",
    "\n",
    "    g, nodes, _ = baseline_data\n",
    "    print(f\"‚úì Loaded: {g.numberOfNodes():,} nodes, {g.numberOfEdges():,} edges\")\n",
    "\n",
    "    main_output_folder = Path(COMPARISON_FOLDERS[0]).parent / \"bar_results_automatic\"\n",
    "    main_output_folder.mkdir(exist_ok=True, parents=True)\n",
    "    print(f\"‚úì Output folder: {main_output_folder}\")\n",
    "\n",
    "    # Initialize checkpoint manager\n",
    "    checkpoint_manager = CheckpointManager(main_output_folder / \"checkpoints\")\n",
    "\n",
    "    # Show progress\n",
    "    progress = checkpoint_manager.get_progress(total_runs)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"CHECKPOINT STATUS\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(\n",
    "        f\"Completed: {progress['completed']}/{progress['total']} ({progress['percent']:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Remaining: {progress['remaining']}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    overall_tracker_path = main_output_folder / \"OVERALL_AVERAGES_TRACKER.csv\"\n",
    "\n",
    "    all_strategy_results = []\n",
    "    run_count = 0\n",
    "\n",
    "    for min_conn, max_conn, range_name in CONNECTION_RANGES:\n",
    "        print(f\"\\n\\n{'‚ñà' * 70}\")\n",
    "        print(f\"CONNECTION RANGE: {range_name} ({min_conn}-{max_conn})\")\n",
    "        print(f\"{'‚ñà' * 70}\")\n",
    "\n",
    "        for folder in COMPARISON_FOLDERS:\n",
    "            run_count += 1\n",
    "            strategy_name = Path(folder).name\n",
    "\n",
    "            print(f\"\\n[RUN {run_count}/{total_runs}]\")\n",
    "\n",
    "            strategy_result = process_strategy_with_range(\n",
    "                baseline_data,\n",
    "                folder,\n",
    "                strategy_name,\n",
    "                min_conn,\n",
    "                max_conn,\n",
    "                range_name,\n",
    "                main_output_folder,\n",
    "                overall_tracker_path,\n",
    "                checkpoint_manager,\n",
    "            )\n",
    "\n",
    "            if strategy_result:\n",
    "                all_strategy_results.append(strategy_result)\n",
    "\n",
    "    print(f\"\\n\\n{'=' * 70}\")\n",
    "    print(\"‚úì ALL STRATEGY-RANGE COMBINATIONS PROCESSED\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Combinations Completed: {len(all_strategy_results)}/{total_runs}\")\n",
    "    print(f\"\\nFinal output files:\")\n",
    "    print(f\"  ‚Ä¢ Strategy-range summaries: {len(all_strategy_results)} CSV files\")\n",
    "    print(f\"    Format: <Range>_<Strategy_Name>.csv\")\n",
    "    print(f\"    Example: Range_5-35_Folder_Batches.csv\")\n",
    "    print(f\"  ‚Ä¢ Overall tracker: 1 CSV file\")\n",
    "    print(f\"  ‚Ä¢ Checkpoint file: 1 pickle file (.pkl)\")\n",
    "    print(f\"\\nSaved to: {main_output_folder}\")\n",
    "\n",
    "    if all_strategy_results:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"BEST PERFORMERS BY RANGE\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        for min_conn, max_conn, range_name in CONNECTION_RANGES:\n",
    "            range_results = [\n",
    "                r for r in all_strategy_results if r[\"range_name\"] == range_name\n",
    "            ]\n",
    "            if range_results:\n",
    "                best = max(range_results, key=lambda x: x[\"avg_mean\"])\n",
    "                print(f\"\\n{range_name} ({min_conn}-{max_conn}):\")\n",
    "                print(f\"  üèÜ Best: {best['strategy_name']}\")\n",
    "                print(f\"     Mean Œî: {best['avg_mean']:+.3f}%\")\n",
    "                print(f\"     ‚ÜëPages: {best['avg_up']:.1f}\")\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}