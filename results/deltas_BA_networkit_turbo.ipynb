{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYgtCa_8srDm"
   },
   "outputs": [],
   "source": [
    "# NetworKit-based PageRank Simulation - Multi-Strategy (Final Results Only)\n",
    "# Processes multiple strategy folders with cumulative overall tracking\n",
    "# NO intermediate files - only final strategy summaries\n",
    "\n",
    "# === INSTALLATION ===\n",
    "!pip install networkit pandas numpy matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkit as nk\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# === BOOSTING CONFIGURATION ===\n",
    "NUM_BOOSTING_ROUNDS = 20\n",
    "BRIDGINGS_PER_ROUND = 20\n",
    "TOTAL_SIMULATIONS = NUM_BOOSTING_ROUNDS * BRIDGINGS_PER_ROUND\n",
    "\n",
    "# === THRESHOLD CONFIGURATION ===\n",
    "NEUTRAL_THRESHOLD = 0.01  # ±0.01% considered neutral\n",
    "\n",
    "# === SIMULATION PARAMETERS ===\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "TOTAL_NODES_WWW = 100000\n",
    "EDGES_PER_NEW_NODE = 2\n",
    "PAGERANK_TOLERANCE = 1e-3\n",
    "\n",
    "# === FILE PATHS ===\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "\n",
    "# List of comparison folders to process\n",
    "COMPARISON_FOLDERS = [\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/random_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/high_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/folder_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/mixed_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/low_batches/\",\n",
    "]\n",
    "\n",
    "# === CACHE ===\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive in Colab environment\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "\n",
    "        drive.mount(\"/content/drive\")\n",
    "        print(\"✓ Google Drive mounted successfully!\")\n",
    "        return True\n",
    "    except:\n",
    "        print(\"⚠ Not in Colab - skipping drive mount\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path):\n",
    "    \"\"\"Load directed graph from CSV with FROM/TO columns\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "    from_urls = df[\"FROM\"].values\n",
    "    to_urls = df[\"TO\"].values\n",
    "\n",
    "    if len(from_urls) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "    url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "    g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "    for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "        g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "    return g, all_urls, url_to_idx\n",
    "\n",
    "\n",
    "def create_www_graph_networkit(n_nodes, m_edges, seed=42):\n",
    "    \"\"\"Create Barabási-Albert graph representing WWW with caching\"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    cache_key = (n_nodes, m_edges, seed)\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph\n",
    "\n",
    "    nk.setSeed(seed, False)\n",
    "    generator = nk.generators.BarabasiAlbertGenerator(\n",
    "        k=m_edges, nMax=n_nodes, n0=m_edges\n",
    "    )\n",
    "    www_graph = generator.generate()\n",
    "\n",
    "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (cache_key, cached_graph)\n",
    "\n",
    "    return www_graph\n",
    "\n",
    "\n",
    "def process_configuration_networkit(www_graph, kalicube_edges, kalicube_nodes):\n",
    "    \"\"\"Merge Kalicube graph with WWW and calculate PageRank\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "\n",
    "    # Create merged graph\n",
    "    merged_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    # Add Kalicube nodes\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    # Add Kalicube internal edges\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    # Create bridging connections\n",
    "    n_www_sample = min(MIN_CONNECTIONS, TOTAL_NODES_WWW)\n",
    "    n_kalicube_sample = min(MIN_CONNECTIONS, len(kalicube_nodes))\n",
    "\n",
    "    www_nodes_sample = np.random.choice(\n",
    "        TOTAL_NODES_WWW, size=n_www_sample, replace=False\n",
    "    )\n",
    "    kalicube_indices = np.random.choice(\n",
    "        len(kalicube_nodes), size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        merged_graph.addEdge(www_node_id, kalicube_idx + kalicube_offset)\n",
    "\n",
    "    # Calculate PageRank\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    # Extract Kalicube PageRank scores\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        pagerank_dict[url] = pagerank_scores[i + kalicube_offset]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def classify_delta(delta_pct):\n",
    "    \"\"\"Classify delta as positive, negative, or neutral\"\"\"\n",
    "    if delta_pct > NEUTRAL_THRESHOLD:\n",
    "        return \"positive\"\n",
    "    elif delta_pct < -NEUTRAL_THRESHOLD:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "\n",
    "def run_boosting_round(\n",
    "    round_id, old_edges, new_edges, old_nodes, new_nodes, page_deltas_tracker\n",
    "):\n",
    "    \"\"\"Run one boosting round with multiple bridging simulations\"\"\"\n",
    "    delta_pcts_all = []\n",
    "\n",
    "    for bridging_id in range(BRIDGINGS_PER_ROUND):\n",
    "        sim_id = round_id * BRIDGINGS_PER_ROUND + bridging_id\n",
    "        sim_seed = 42 + sim_id\n",
    "\n",
    "        np.random.seed(sim_seed)\n",
    "        random.seed(sim_seed)\n",
    "\n",
    "        # Create WWW graph\n",
    "        www_graph = create_www_graph_networkit(\n",
    "            TOTAL_NODES_WWW, EDGES_PER_NEW_NODE, sim_seed\n",
    "        )\n",
    "\n",
    "        # Calculate PageRank for both configurations\n",
    "        pagerank_old = process_configuration_networkit(www_graph, old_edges, old_nodes)\n",
    "        pagerank_new = process_configuration_networkit(www_graph, new_edges, new_nodes)\n",
    "\n",
    "        # Get common URLs\n",
    "        common_urls = set(pagerank_old.keys()) & set(pagerank_new.keys())\n",
    "        if not common_urls:\n",
    "            continue\n",
    "\n",
    "        # Calculate percentage changes and track per page\n",
    "        for url in common_urls:\n",
    "            old_val = pagerank_old[url]\n",
    "            new_val = pagerank_new[url]\n",
    "            delta = new_val - old_val\n",
    "            delta_pct = (delta / max(old_val, 1e-10)) * 100\n",
    "            delta_pcts_all.append(delta_pct)\n",
    "\n",
    "            # Track this page's delta across all simulations\n",
    "            page_deltas_tracker[url].append(delta_pct)\n",
    "\n",
    "    return delta_pcts_all, page_deltas_tracker\n",
    "\n",
    "\n",
    "def run_boosted_comparison(baseline_data, comparison_file):\n",
    "    \"\"\"Run complete boosted comparison - returns only final statistics\"\"\"\n",
    "    comparison_name = comparison_file.stem\n",
    "    print(f\"  Processing: {comparison_name}...\", end=\" \", flush=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load comparison graph\n",
    "    g_old, nodes_old, _ = baseline_data\n",
    "    g_new, nodes_new, _ = load_graph_from_csv_networkit(comparison_file)\n",
    "\n",
    "    if g_new is None:\n",
    "        print(f\"✗ Failed\")\n",
    "        return None\n",
    "\n",
    "    # Convert to edge lists and get node lists\n",
    "    old_edges = [(u, v) for u, v in g_old.iterEdges()]\n",
    "    new_edges = [(u, v) for u, v in g_new.iterEdges()]\n",
    "    old_nodes = nodes_old\n",
    "    new_nodes = nodes_new\n",
    "\n",
    "    del g_new\n",
    "    gc.collect()\n",
    "\n",
    "    # Track deltas per page across all simulations\n",
    "    page_deltas_tracker = defaultdict(list)\n",
    "    all_deltas = []\n",
    "\n",
    "    # Run boosting rounds (no intermediate storage)\n",
    "    for round_id in range(NUM_BOOSTING_ROUNDS):\n",
    "        round_deltas, page_deltas_tracker = run_boosting_round(\n",
    "            round_id, old_edges, new_edges, old_nodes, new_nodes, page_deltas_tracker\n",
    "        )\n",
    "        all_deltas.extend(round_deltas)\n",
    "\n",
    "    if not all_deltas:\n",
    "        print(\"✗ No results\")\n",
    "        return None\n",
    "\n",
    "    # Classify pages based on their average delta\n",
    "    pages_positive = 0\n",
    "    pages_negative = 0\n",
    "    pages_neutral = 0\n",
    "\n",
    "    for url, deltas in page_deltas_tracker.items():\n",
    "        avg_delta = np.mean(deltas)\n",
    "        classification = classify_delta(avg_delta)\n",
    "\n",
    "        if classification == \"positive\":\n",
    "            pages_positive += 1\n",
    "        elif classification == \"negative\":\n",
    "            pages_negative += 1\n",
    "        else:\n",
    "            pages_neutral += 1\n",
    "\n",
    "    total_pages = len(page_deltas_tracker)\n",
    "    all_deltas = np.array(all_deltas)\n",
    "\n",
    "    # Calculate final statistics\n",
    "    final_mean = np.mean(all_deltas)\n",
    "    final_max = np.max(all_deltas)\n",
    "    final_min = np.min(all_deltas)\n",
    "    final_std = np.std(all_deltas)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"✓ Mean: {final_mean:+.3f}% [{duration:.1f}s]\")\n",
    "\n",
    "    return {\n",
    "        \"name\": comparison_name,\n",
    "        \"duration\": duration,\n",
    "        \"mean\": final_mean,\n",
    "        \"max\": final_max,\n",
    "        \"min\": final_min,\n",
    "        \"std\": final_std,\n",
    "        \"pages_up\": pages_positive,\n",
    "        \"pages_down\": pages_negative,\n",
    "        \"pages_neutral\": pages_neutral,\n",
    "        \"total_pages\": total_pages,\n",
    "        \"num_simulations\": TOTAL_SIMULATIONS,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_strategy_summary(all_results, output_folder, strategy_name):\n",
    "    \"\"\"Create summary for a single strategy - ONLY file saved per strategy\"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"STRATEGY SUMMARY: {strategy_name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    data = []\n",
    "    for r in all_results:\n",
    "        if r:\n",
    "            data.append(\n",
    "                {\n",
    "                    \"Comparison\": r[\"name\"],\n",
    "                    \"Mean_Delta_%\": r[\"mean\"],\n",
    "                    \"Max_Delta_%\": r[\"max\"],\n",
    "                    \"Min_Delta_%\": r[\"min\"],\n",
    "                    \"Std_Delta_%\": r[\"std\"],\n",
    "                    \"Pages_Up\": r[\"pages_up\"],\n",
    "                    \"Pages_Down\": r[\"pages_down\"],\n",
    "                    \"Pages_Neutral\": r[\"pages_neutral\"],\n",
    "                    \"Total_Pages\": r[\"total_pages\"],\n",
    "                    \"Duration_m\": r[\"duration\"] / 60,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not data:\n",
    "        print(\"✗ No valid results\")\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(\"Mean_Delta_%\", ascending=False)\n",
    "\n",
    "    # ONLY save this one file per strategy\n",
    "    summary_path = output_folder / f\"STRATEGY_SUMMARY_{strategy_name}.csv\"\n",
    "    df.to_csv(summary_path, index=False)\n",
    "\n",
    "    print(\"\\nRankings by Mean Delta %:\")\n",
    "    for idx, row in df.iterrows():\n",
    "        symbol = (\n",
    "            \"↑\"\n",
    "            if row[\"Mean_Delta_%\"] > NEUTRAL_THRESHOLD\n",
    "            else \"↓\"\n",
    "            if row[\"Mean_Delta_%\"] < -NEUTRAL_THRESHOLD\n",
    "            else \"→\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  {symbol} {row['Comparison']}: {row['Mean_Delta_%']:+.3f}% \"\n",
    "            + f\"(↑{row['Pages_Up']:.0f} ↓{row['Pages_Down']:.0f} →{row['Pages_Neutral']:.0f})\"\n",
    "        )\n",
    "\n",
    "    # Calculate strategy averages\n",
    "    strategy_avg_mean = df[\"Mean_Delta_%\"].mean()\n",
    "    strategy_avg_max = df[\"Max_Delta_%\"].mean()\n",
    "    strategy_avg_min = df[\"Min_Delta_%\"].mean()\n",
    "    strategy_avg_up = df[\"Pages_Up\"].mean()\n",
    "    strategy_avg_down = df[\"Pages_Down\"].mean()\n",
    "    strategy_avg_neutral = df[\"Pages_Neutral\"].mean()\n",
    "\n",
    "    print(f\"\\nStrategy Averages:\")\n",
    "    print(f\"  Avg Mean Delta:   {strategy_avg_mean:+.3f}%\")\n",
    "    print(f\"  Avg Max Delta:    {strategy_avg_max:+.3f}%\")\n",
    "    print(f\"  Avg Min Delta:    {strategy_avg_min:+.3f}%\")\n",
    "    print(f\"  Avg Pages Up:     {strategy_avg_up:.1f}\")\n",
    "    print(f\"  Avg Pages Down:   {strategy_avg_down:.1f}\")\n",
    "    print(f\"  Avg Pages Neutral: {strategy_avg_neutral:.1f}\")\n",
    "    print(f\"\\n✓ Saved: {summary_path.name}\")\n",
    "\n",
    "    return {\n",
    "        \"strategy_name\": strategy_name,\n",
    "        \"avg_mean\": strategy_avg_mean,\n",
    "        \"avg_max\": strategy_avg_max,\n",
    "        \"avg_min\": strategy_avg_min,\n",
    "        \"avg_up\": strategy_avg_up,\n",
    "        \"avg_down\": strategy_avg_down,\n",
    "        \"avg_neutral\": strategy_avg_neutral,\n",
    "        \"num_comparisons\": len(data),\n",
    "    }\n",
    "\n",
    "\n",
    "def update_overall_tracker(overall_tracker_path, strategy_result):\n",
    "    \"\"\"Update and save overall tracker - one row per strategy with its overall averages\"\"\"\n",
    "\n",
    "    # Load existing tracker or create new\n",
    "    if overall_tracker_path.exists():\n",
    "        tracker_df = pd.read_csv(overall_tracker_path)\n",
    "        existing_data = tracker_df.to_dict(\"records\")\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    # Add new strategy data (these are already averages across all comparisons in the strategy)\n",
    "    existing_data.append(\n",
    "        {\n",
    "            \"Strategy\": strategy_result[\"strategy_name\"],\n",
    "            \"Overall_Avg_Mean_%\": strategy_result[\"avg_mean\"],\n",
    "            \"Overall_Avg_Max_%\": strategy_result[\"avg_max\"],\n",
    "            \"Overall_Avg_Min_%\": strategy_result[\"avg_min\"],\n",
    "            \"Overall_Avg_Pages_Up\": strategy_result[\"avg_up\"],\n",
    "            \"Overall_Avg_Pages_Down\": strategy_result[\"avg_down\"],\n",
    "            \"Overall_Avg_Pages_Neutral\": strategy_result[\"avg_neutral\"],\n",
    "            \"Num_Comparisons\": strategy_result[\"num_comparisons\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create DataFrame\n",
    "    tracker_df = pd.DataFrame(existing_data)\n",
    "\n",
    "    # Save updated tracker\n",
    "    tracker_df.to_csv(overall_tracker_path, index=False)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"OVERALL AVERAGES FOR STRATEGY: {strategy_result['strategy_name']}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Comparison Files:        {strategy_result['num_comparisons']}\")\n",
    "    print(\n",
    "        f\"Total Simulations:       {strategy_result['num_comparisons'] * TOTAL_SIMULATIONS:,}\"\n",
    "    )\n",
    "    print(f\"\\nOverall Averages (across all simulations in this strategy):\")\n",
    "    print(f\"  Overall Avg Mean:        {strategy_result['avg_mean']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Max:         {strategy_result['avg_max']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Min:         {strategy_result['avg_min']:+.3f}%\")\n",
    "    print(f\"  Overall Avg Pages Up:    {strategy_result['avg_up']:.1f}\")\n",
    "    print(f\"  Overall Avg Pages Down:  {strategy_result['avg_down']:.1f}\")\n",
    "    print(f\"  Overall Avg Pages Neutral: {strategy_result['avg_neutral']:.1f}\")\n",
    "    print(f\"\\n✓ Saved to: {overall_tracker_path.name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "\n",
    "def process_strategy_folder(\n",
    "    baseline_data, comparison_folder, main_output_folder, overall_tracker_path\n",
    "):\n",
    "    \"\"\"Process a single strategy folder\"\"\"\n",
    "    comparison_folder = Path(comparison_folder)\n",
    "    strategy_name = comparison_folder.name\n",
    "\n",
    "    print(f\"\\n\\n{'#' * 70}\")\n",
    "    print(f\"STRATEGY: {strategy_name}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "\n",
    "    # Find comparison files\n",
    "    comparison_files = list(comparison_folder.glob(\"*.csv\"))\n",
    "\n",
    "    if not comparison_files:\n",
    "        print(f\"✗ No CSV files in: {comparison_folder}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"✓ Found {len(comparison_files)} comparison files\")\n",
    "\n",
    "    # Run comparisons for this strategy (no intermediate files)\n",
    "    all_results = []\n",
    "    for i, comp_file in enumerate(comparison_files, 1):\n",
    "        result = run_boosted_comparison(baseline_data, comp_file)\n",
    "        all_results.append(result)\n",
    "\n",
    "        # Clear cache\n",
    "        global _www_graph_cache\n",
    "        _www_graph_cache = None\n",
    "        gc.collect()\n",
    "\n",
    "    # Create strategy summary (ONLY file saved)\n",
    "    strategy_result = create_strategy_summary(\n",
    "        all_results, main_output_folder, strategy_name\n",
    "    )\n",
    "\n",
    "    if strategy_result:\n",
    "        # Update overall tracker\n",
    "        update_overall_tracker(overall_tracker_path, strategy_result)\n",
    "\n",
    "    return strategy_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"NetworKit PageRank Simulation - MULTI-STRATEGY (FINAL RESULTS ONLY)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"NetworKit version: {nk.__version__}\")\n",
    "    print(f\"Boosting Rounds: {NUM_BOOSTING_ROUNDS}\")\n",
    "    print(f\"Bridgings per Round: {BRIDGINGS_PER_ROUND}\")\n",
    "    print(f\"Total Simulations per Comparison: {TOTAL_SIMULATIONS}\")\n",
    "    print(f\"Neutral Threshold: ±{NEUTRAL_THRESHOLD}%\")\n",
    "    print(f\"Strategies to Process: {len(COMPARISON_FOLDERS)}\")\n",
    "    print(\"\\n⚠ NO INTERMEDIATE FILES - Only final strategy summaries will be saved\")\n",
    "\n",
    "    mount_google_drive()\n",
    "\n",
    "    # Verify baseline\n",
    "    baseline_path = Path(BASELINE_PATH)\n",
    "    if not baseline_path.exists():\n",
    "        print(f\"\\n✗ Baseline not found: {BASELINE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\n✓ Baseline: {baseline_path.name}\")\n",
    "\n",
    "    # Load baseline once\n",
    "    print(\"\\nLoading baseline graph...\")\n",
    "    baseline_data = load_graph_from_csv_networkit(baseline_path)\n",
    "    if baseline_data[0] is None:\n",
    "        print(\"✗ Failed to load baseline\")\n",
    "        exit(1)\n",
    "\n",
    "    g, nodes, _ = baseline_data\n",
    "    print(f\"✓ Loaded: {g.numberOfNodes():,} nodes, {g.numberOfEdges():,} edges\")\n",
    "\n",
    "    # Create main output folder\n",
    "    main_output_folder = Path(COMPARISON_FOLDERS[0]).parent / \"FINAL_STRATEGY_RESULTS\"\n",
    "    main_output_folder.mkdir(exist_ok=True, parents=True)\n",
    "    print(f\"✓ Output folder: {main_output_folder}\")\n",
    "\n",
    "    # Initialize overall tracker path\n",
    "    overall_tracker_path = main_output_folder / \"OVERALL_AVERAGES_TRACKER.csv\"\n",
    "\n",
    "    # Process each strategy folder\n",
    "    all_strategy_results = []\n",
    "    for i, folder in enumerate(COMPARISON_FOLDERS, 1):\n",
    "        print(f\"\\n\\n{'█' * 70}\")\n",
    "        print(f\"PROCESSING STRATEGY {i}/{len(COMPARISON_FOLDERS)}\")\n",
    "        print(f\"{'█' * 70}\")\n",
    "\n",
    "        strategy_result = process_strategy_folder(\n",
    "            baseline_data, folder, main_output_folder, overall_tracker_path\n",
    "        )\n",
    "\n",
    "        if strategy_result:\n",
    "            all_strategy_results.append(strategy_result)\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"\\n\\n{'=' * 70}\")\n",
    "    print(\"✓ ALL STRATEGIES PROCESSED\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(\n",
    "        f\"Strategies Completed: {len(all_strategy_results)}/{len(COMPARISON_FOLDERS)}\"\n",
    "    )\n",
    "    print(f\"\\nFinal output files:\")\n",
    "    print(f\"  • 1 summary CSV per strategy ({len(all_strategy_results)} files)\")\n",
    "    print(f\"  • 1 overall tracker CSV (cumulative)\")\n",
    "    print(f\"\\nSaved to: {main_output_folder}\")\n",
    "    print(f\"{'=' * 70}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}