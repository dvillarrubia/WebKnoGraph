{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "P4ot8mq0U5XN"
   },
   "outputs": [],
   "source": [
    "# OPTIMIZED MULTI-RANGE PAGERANK ANALYSIS - EXPERT LED VERSION\n",
    "# Performance improvements:\n",
    "# - Binary graph caching (50-100x faster loading)\n",
    "# - Worker pool with pre-loaded graphs (100x faster initialization)\n",
    "# - Memory-efficient numpy arrays\n",
    "# - Checkpoint recovery system\n",
    "# - Enhanced error handling\n",
    "# Expected time: 50-60 hours on full expert run (500 simulations per config)\n",
    "\n",
    "# === INSTALLATION CELL ===\n",
    "!pip install networkit pandas numpy tqdm matplotlib seaborn scipy psutil\n",
    "\n",
    "# === MOUNT GOOGLE DRIVE ===\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkit as nk\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde, t\n",
    "import traceback\n",
    "import shutil\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "# ============================================\n",
    "# USER CONFIGURATION - EXPERT LED ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "\n",
    "# 5 Expert-Led Strategies\n",
    "COMPARISON_FOLDERS = [\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/high_batches/\",\n",
    "        \"High Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/low_batches/\",\n",
    "        \"Low Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/mixed_batches/\",\n",
    "        \"Mixed Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/random_batches/\",\n",
    "        \"Random Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/folder_batches/\",\n",
    "        \"Folder Candidates\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "FINEWEB_WWW_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/fineweb_500k_pages.csv\"\n",
    "\n",
    "# 3 CONNECTION RANGES\n",
    "CONNECTION_RANGES = [\n",
    "    (5, 35, \"Range_5-35\"),\n",
    "    (35, 65, \"Range_35-65\"),\n",
    "    (65, 95, \"Range_65-95\"),\n",
    "]\n",
    "\n",
    "# Simulation parameters - EXPERT LED (500 total simulations)\n",
    "EXPERT_BOOSTING_ROUNDS = 20\n",
    "EXPERT_BRIDGINGS_PER_ROUND = 25\n",
    "TOTAL_SIMULATIONS = EXPERT_BOOSTING_ROUNDS * EXPERT_BRIDGINGS_PER_ROUND\n",
    "\n",
    "# SEO parameters\n",
    "NEUTRAL_THRESHOLD = 0.025\n",
    "PAGERANK_TOLERANCE = 1e-6\n",
    "DAMPING_FACTOR = 0.80\n",
    "\n",
    "# Parallelization\n",
    "USE_PARALLEL = True\n",
    "NUM_WORKERS = max(4, min(cpu_count() - 2, 16))\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "# Output - EXPERT LED\n",
    "OUTPUT_DIR = (\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/real_www_results_expert/\"\n",
    ")\n",
    "MULTI_RANGE_OUTPUT_DIR = OUTPUT_DIR\n",
    "CACHE_DIR = os.path.join(OUTPUT_DIR, \"graph_cache/\")\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, \"checkpoints/\")\n",
    "\n",
    "os.makedirs(MULTI_RANGE_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================\n",
    "# PATH VERIFICATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PATH VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "paths_to_check = [\n",
    "    (\"Baseline\", BASELINE_PATH),\n",
    "    (\"WWW Graph\", FINEWEB_WWW_PATH),\n",
    "]\n",
    "\n",
    "for folder_path, strategy_name in COMPARISON_FOLDERS:\n",
    "    paths_to_check.append((strategy_name, folder_path))\n",
    "\n",
    "all_valid = True\n",
    "for name, path in paths_to_check:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"âœ“\" if exists else \"âœ—\"\n",
    "    print(f\"{status} {name}: {path}\")\n",
    "\n",
    "    if not exists:\n",
    "        all_valid = False\n",
    "    elif os.path.isdir(path):\n",
    "        csv_count = len([f for f in os.listdir(path) if f.endswith(\".csv\")])\n",
    "        print(f\"  â””â”€ Contains {csv_count} CSV files\")\n",
    "        if csv_count == 0:\n",
    "            print(f\"  âš ï¸  WARNING: No CSV files found!\")\n",
    "            all_valid = False\n",
    "\n",
    "if not all_valid:\n",
    "    print(\"\\nâŒ VALIDATION FAILED: Some paths are invalid or empty\")\n",
    "    print(\"Please verify your paths before continuing.\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All paths validated successfully!\")\n",
    "\n",
    "print(f\"\\nOutput directories:\")\n",
    "print(f\"  Main: {OUTPUT_DIR}\")\n",
    "print(f\"  Cache: {CACHE_DIR}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# GRAPH CACHING SYSTEM\n",
    "# ============================================\n",
    "\n",
    "\n",
    "class GraphCache:\n",
    "    \"\"\"High-performance graph caching with binary serialization\"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir=CACHE_DIR):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def _get_cache_path(self, graph_id):\n",
    "        return os.path.join(self.cache_dir, f\"{graph_id}.nkb\")\n",
    "\n",
    "    def _get_meta_path(self, graph_id):\n",
    "        return os.path.join(self.cache_dir, f\"{graph_id}_meta.pkl\")\n",
    "\n",
    "    def save_graph(self, graph, graph_id):\n",
    "        \"\"\"Save graph to binary format\"\"\"\n",
    "        try:\n",
    "            cache_path = self._get_cache_path(graph_id)\n",
    "            nk.writeGraph(graph, cache_path, nk.Format.NetworkitBinary)\n",
    "            print(f\"  âœ“ Cached graph: {graph_id} ({graph.numberOfNodes():,} nodes)\")\n",
    "            return cache_path\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Cache save failed for {graph_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_graph(self, graph_id):\n",
    "        \"\"\"Load graph from binary cache\"\"\"\n",
    "        try:\n",
    "            cache_path = self._get_cache_path(graph_id)\n",
    "            if os.path.exists(cache_path):\n",
    "                return nk.readGraph(cache_path, nk.Format.NetworkitBinary)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Cache load failed for {graph_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def save_metadata(self, graph_id, metadata):\n",
    "        \"\"\"Save arbitrary metadata\"\"\"\n",
    "        try:\n",
    "            meta_path = self._get_meta_path(graph_id)\n",
    "            with open(meta_path, \"wb\") as f:\n",
    "                pickle.dump(metadata, f)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Metadata save failed: {e}\")\n",
    "\n",
    "    def load_metadata(self, graph_id):\n",
    "        \"\"\"Load metadata\"\"\"\n",
    "        try:\n",
    "            meta_path = self._get_meta_path(graph_id)\n",
    "            if os.path.exists(meta_path):\n",
    "                with open(meta_path, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Metadata load failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached graphs\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.cache_dir)\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "            print(\"  âœ“ Cache cleared\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Cache clear failed: {e}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CHECKPOINT SYSTEM\n",
    "# ============================================\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Save/load checkpoints for crash recovery\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_dir=CHECKPOINT_DIR):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, range_name, strategy_name, results):\n",
    "        \"\"\"Save checkpoint after each strategy completion\"\"\"\n",
    "        checkpoint_id = f\"{range_name}_{strategy_name.replace(' ', '_')}\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"{checkpoint_id}.pkl\")\n",
    "\n",
    "        try:\n",
    "            with open(checkpoint_path, \"wb\") as f:\n",
    "                pickle.dump(results, f)\n",
    "            print(f\"  âœ“ Checkpoint saved: {checkpoint_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Checkpoint save failed: {e}\")\n",
    "\n",
    "    def load_checkpoint(self, range_name, strategy_name):\n",
    "        \"\"\"Load checkpoint if exists\"\"\"\n",
    "        checkpoint_id = f\"{range_name}_{strategy_name.replace(' ', '_')}\"\n",
    "        checkpoint_path = os.path.join(self.checkpoint_dir, f\"{checkpoint_id}.pkl\")\n",
    "\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                with open(checkpoint_path, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Checkpoint load failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"List all available checkpoints\"\"\"\n",
    "        checkpoints = [f for f in os.listdir(self.checkpoint_dir) if f.endswith(\".pkl\")]\n",
    "        return checkpoints\n",
    "\n",
    "    def clear_checkpoints(self):\n",
    "        \"\"\"Clear all checkpoints\"\"\"\n",
    "        try:\n",
    "            shutil.rmtree(self.checkpoint_dir)\n",
    "            os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "            print(\"  âœ“ Checkpoints cleared\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Checkpoint clear failed: {e}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def log_memory_usage(label=\"\"):\n",
    "    \"\"\"Log current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_gb = process.memory_info().rss / 1024**3\n",
    "    print(f\"  ðŸ’¾ Memory {label}: {memory_gb:.2f} GB\")\n",
    "    return memory_gb\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CORE FUNCTIONS (OPTIMIZED)\n",
    "# ============================================\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path, graph_name=\"graph\"):\n",
    "    \"\"\"Load graph from CSV file\"\"\"\n",
    "    try:\n",
    "        print(f\"  Loading {graph_name} from {os.path.basename(file_path)}...\")\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"  ERROR: No valid edges found in {file_path}\")\n",
    "            return None, None, None\n",
    "\n",
    "        from_urls = df[\"FROM\"].values\n",
    "        to_urls = df[\"TO\"].values\n",
    "        all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "        url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "        g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "        for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "            g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "        print(f\"    Loaded: {len(all_urls):,} nodes, {len(df):,} edges\")\n",
    "        return g, all_urls, url_to_idx\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading {file_path}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def load_www_graph_optimized(www_csv_path, use_cache=True):\n",
    "    \"\"\"Load WWW graph with caching (50-100x faster on cache hit)\"\"\"\n",
    "    cache = GraphCache()\n",
    "    graph_id = \"www_fineweb_500k\"\n",
    "\n",
    "    if use_cache:\n",
    "        print(\"\\n  Checking cache...\")\n",
    "        cached_graph = cache.load_graph(graph_id)\n",
    "        cached_nodes = cache.load_metadata(graph_id)\n",
    "\n",
    "        if cached_graph is not None and cached_nodes is not None:\n",
    "            print(\n",
    "                f\"  âœ“ Loaded from cache: {cached_graph.numberOfNodes():,} nodes, \"\n",
    "                f\"{cached_graph.numberOfEdges():,} edges\"\n",
    "            )\n",
    "            log_memory_usage(\"after cache load\")\n",
    "            return cached_graph, cached_nodes\n",
    "\n",
    "    print(\"\\n  Cache miss - loading WWW graph from CSV...\")\n",
    "    www_graph, www_nodes, _ = load_graph_from_csv_networkit(\n",
    "        www_csv_path, graph_name=\"WWW graph\"\n",
    "    )\n",
    "\n",
    "    if www_graph is None:\n",
    "        raise ValueError(f\"Failed to load WWW graph from {www_csv_path}\")\n",
    "\n",
    "    if use_cache:\n",
    "        cache.save_graph(www_graph, graph_id)\n",
    "        cache.save_metadata(graph_id, www_nodes)\n",
    "\n",
    "    log_memory_usage(\"after WWW load\")\n",
    "    return www_graph, www_nodes\n",
    "\n",
    "\n",
    "# Global for worker processes\n",
    "_worker_www_graph = None\n",
    "_worker_www_nodes = None\n",
    "\n",
    "\n",
    "def init_worker(www_cache_path, www_nodes):\n",
    "    \"\"\"Initialize worker with pre-loaded graph\"\"\"\n",
    "    global _worker_www_graph, _worker_www_nodes\n",
    "    try:\n",
    "        _worker_www_graph = nk.readGraph(www_cache_path, nk.Format.NetworkitBinary)\n",
    "        _worker_www_nodes = www_nodes\n",
    "        print(f\"  Worker initialized: {_worker_www_graph.numberOfNodes():,} nodes\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR initializing worker: {e}\")\n",
    "        _worker_www_graph = None\n",
    "        _worker_www_nodes = None\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph,\n",
    "    www_nodes,\n",
    "    kalicube_edges,\n",
    "    kalicube_nodes,\n",
    "    kalicube_url_mapping,\n",
    "    min_connections,\n",
    "    max_connections,\n",
    "):\n",
    "    \"\"\"Process configuration with specified connection limits\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "    n_www = www_graph.numberOfNodes()\n",
    "\n",
    "    merged_graph = nk.Graph(n=n_www, weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    n_www_sample = min(max_connections, n_www)\n",
    "    n_kalicube_sample = min(max_connections, n_kalicube)\n",
    "\n",
    "    www_nodes_sample = np.random.choice(n_www, size=n_www_sample, replace=False)\n",
    "    kalicube_indices = np.random.choice(\n",
    "        n_kalicube, size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
    "\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=DAMPING_FACTOR, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def run_single_simulation_networkit(\n",
    "    simulation_id,\n",
    "    www_graph,\n",
    "    www_nodes,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "    min_connections,\n",
    "    max_connections,\n",
    "    return_distributions=False,\n",
    "):\n",
    "    \"\"\"Run single simulation with fixed connection range\"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    pagerank_old_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_old_edges,\n",
    "        kalicube_nodes_old,\n",
    "        kalicube_url_mapping_old,\n",
    "        min_connections,\n",
    "        max_connections,\n",
    "    )\n",
    "\n",
    "    pagerank_new_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_new_edges,\n",
    "        kalicube_nodes_new,\n",
    "        kalicube_url_mapping_new,\n",
    "        min_connections,\n",
    "        max_connections,\n",
    "    )\n",
    "\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return None\n",
    "\n",
    "    deltas_pct = []\n",
    "    pages_up = 0\n",
    "    pages_down = 0\n",
    "    pages_neutral = 0\n",
    "    before_scores = []\n",
    "    after_scores = []\n",
    "\n",
    "    for url in common_urls:\n",
    "        before = pagerank_old_dict[url]\n",
    "        after = pagerank_new_dict[url]\n",
    "\n",
    "        if return_distributions:\n",
    "            before_scores.append(before)\n",
    "            after_scores.append(after)\n",
    "\n",
    "        if before > 0:\n",
    "            delta_pct = ((after - before) / before) * 100\n",
    "            deltas_pct.append(delta_pct)\n",
    "\n",
    "            if delta_pct > NEUTRAL_THRESHOLD:\n",
    "                pages_up += 1\n",
    "            elif delta_pct < -NEUTRAL_THRESHOLD:\n",
    "                pages_down += 1\n",
    "            else:\n",
    "                pages_neutral += 1\n",
    "\n",
    "    if len(deltas_pct) == 0:\n",
    "        return None\n",
    "\n",
    "    result = {\n",
    "        \"mean_delta_pct\": np.mean(deltas_pct),\n",
    "        \"min_delta_pct\": np.min(deltas_pct),\n",
    "        \"max_delta_pct\": np.max(deltas_pct),\n",
    "        \"pages_up\": pages_up,\n",
    "        \"pages_down\": pages_down,\n",
    "        \"pages_neutral\": pages_neutral,\n",
    "        \"total_pages\": len(common_urls),\n",
    "    }\n",
    "\n",
    "    if return_distributions:\n",
    "        result[\"before_distribution\"] = np.array(before_scores)\n",
    "        result[\"after_distribution\"] = np.array(after_scores)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_simulation_batch(args):\n",
    "    \"\"\"Parallel batch processing using cached graph\"\"\"\n",
    "    global _worker_www_graph, _worker_www_nodes\n",
    "\n",
    "    (\n",
    "        sim_ids,\n",
    "        kalicube_old_edges,\n",
    "        kalicube_new_edges,\n",
    "        kalicube_nodes_old,\n",
    "        kalicube_nodes_new,\n",
    "        kalicube_url_mapping_old,\n",
    "        kalicube_url_mapping_new,\n",
    "        min_conn,\n",
    "        max_conn,\n",
    "    ) = args\n",
    "\n",
    "    if _worker_www_graph is None:\n",
    "        print(\"  ERROR: Worker graph not initialized!\")\n",
    "        return []\n",
    "\n",
    "    batch_results = []\n",
    "    for sim_id in sim_ids:\n",
    "        result = run_single_simulation_networkit(\n",
    "            sim_id,\n",
    "            _worker_www_graph,\n",
    "            _worker_www_nodes,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "            min_conn,\n",
    "            max_conn,\n",
    "            return_distributions=False,\n",
    "        )\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def analyze_csv_pair(\n",
    "    www_cache_path,\n",
    "    www_nodes,\n",
    "    old_csv_path,\n",
    "    new_csv_path,\n",
    "    min_connections,\n",
    "    max_connections,\n",
    "    range_name,\n",
    "):\n",
    "    \"\"\"Analyze pair with specific connection range\"\"\"\n",
    "    print(f\"\\n  Analyzing: {os.path.basename(new_csv_path)}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Validation checks\n",
    "    assert min_connections < max_connections, \"Invalid connection range\"\n",
    "    assert os.path.exists(old_csv_path), f\"Baseline not found: {old_csv_path}\"\n",
    "    assert os.path.exists(new_csv_path), f\"Comparison not found: {new_csv_path}\"\n",
    "\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_networkit(old_csv_path, \"baseline\")\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        return None\n",
    "\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_networkit(new_csv_path, \"comparison\")\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        return None\n",
    "\n",
    "    assert len(kalicube_nodes_new) > 0, \"Empty comparison graph\"\n",
    "\n",
    "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
    "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
    "\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(\n",
    "        f\"    Running {TOTAL_SIMULATIONS} simulations with {min_connections}-{max_connections} connections...\"\n",
    "    )\n",
    "\n",
    "    all_sim_results = []\n",
    "\n",
    "    if USE_PARALLEL:\n",
    "        all_sim_ids = list(range(TOTAL_SIMULATIONS))\n",
    "        sim_id_batches = [\n",
    "            all_sim_ids[i : i + BATCH_SIZE]\n",
    "            for i in range(0, TOTAL_SIMULATIONS, BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "        batch_args = [\n",
    "            (\n",
    "                sim_ids,\n",
    "                kalicube_old_edges,\n",
    "                kalicube_new_edges,\n",
    "                kalicube_nodes_old,\n",
    "                kalicube_nodes_new,\n",
    "                kalicube_url_mapping_old,\n",
    "                kalicube_url_mapping_new,\n",
    "                min_connections,\n",
    "                max_connections,\n",
    "            )\n",
    "            for sim_ids in sim_id_batches\n",
    "        ]\n",
    "\n",
    "        # Use global pool with pre-initialized workers\n",
    "        with Pool(\n",
    "            NUM_WORKERS, initializer=init_worker, initargs=(www_cache_path, www_nodes)\n",
    "        ) as pool:\n",
    "            with tqdm(\n",
    "                total=len(sim_id_batches),\n",
    "                desc=\"    Batches\",\n",
    "                unit=\"batch\",\n",
    "                leave=False,\n",
    "                ncols=100,\n",
    "            ) as pbar:\n",
    "                for batch_results in pool.imap_unordered(\n",
    "                    run_simulation_batch, batch_args\n",
    "                ):\n",
    "                    all_sim_results.extend(batch_results)\n",
    "                    pbar.update(1)\n",
    "    else:\n",
    "        # Non-parallel fallback\n",
    "        www_graph = nk.readGraph(www_cache_path, nk.Format.NetworkitBinary)\n",
    "        with tqdm(total=TOTAL_SIMULATIONS, desc=\"    Simulations\", leave=False) as pbar:\n",
    "            for sim_id in range(TOTAL_SIMULATIONS):\n",
    "                result = run_single_simulation_networkit(\n",
    "                    sim_id,\n",
    "                    www_graph,\n",
    "                    www_nodes,\n",
    "                    kalicube_old_edges,\n",
    "                    kalicube_new_edges,\n",
    "                    kalicube_nodes_old,\n",
    "                    kalicube_nodes_new,\n",
    "                    kalicube_url_mapping_old,\n",
    "                    kalicube_url_mapping_new,\n",
    "                    min_connections,\n",
    "                    max_connections,\n",
    "                )\n",
    "                if result is not None:\n",
    "                    all_sim_results.append(result)\n",
    "                pbar.update(1)\n",
    "\n",
    "    if len(all_sim_results) == 0:\n",
    "        return None\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\n",
    "        f\"    âœ“ Completed in {elapsed / 60:.1f} min ({len(all_sim_results) / elapsed:.1f} sims/sec)\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"filename\": os.path.basename(new_csv_path),\n",
    "        \"range_name\": range_name,\n",
    "        \"min_connections\": min_connections,\n",
    "        \"max_connections\": max_connections,\n",
    "        \"total_simulations\": len(all_sim_results),\n",
    "        \"avg_mean_delta_pct\": np.mean([r[\"mean_delta_pct\"] for r in all_sim_results]),\n",
    "        \"avg_min_delta_pct\": np.mean([r[\"min_delta_pct\"] for r in all_sim_results]),\n",
    "        \"avg_max_delta_pct\": np.mean([r[\"max_delta_pct\"] for r in all_sim_results]),\n",
    "        \"avg_pages_up\": np.mean([r[\"pages_up\"] for r in all_sim_results]),\n",
    "        \"avg_pages_down\": np.mean([r[\"pages_down\"] for r in all_sim_results]),\n",
    "        \"avg_pages_neutral\": np.mean([r[\"pages_neutral\"] for r in all_sim_results]),\n",
    "        \"std_mean\": np.std([r[\"mean_delta_pct\"] for r in all_sim_results]),\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "        \"sim_results\": all_sim_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for mean\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        return np.mean(data), 0\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    margin = se * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean, margin\n",
    "\n",
    "\n",
    "def process_strategy_with_range(\n",
    "    www_cache_path,\n",
    "    www_nodes,\n",
    "    baseline_path,\n",
    "    folder_path,\n",
    "    strategy_name,\n",
    "    min_conn,\n",
    "    max_conn,\n",
    "    range_name,\n",
    "    checkpoint_mgr,\n",
    "):\n",
    "    \"\"\"Process one strategy with specific connection range\"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"STRATEGY: {strategy_name} | RANGE: {range_name} ({min_conn}-{max_conn})\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    # Check for existing checkpoint\n",
    "    checkpoint = checkpoint_mgr.load_checkpoint(range_name, strategy_name)\n",
    "    if checkpoint is not None:\n",
    "        print(f\"  âœ“ Loaded from checkpoint!\")\n",
    "        return checkpoint\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"  ERROR: Folder not found: {folder_path}\")\n",
    "        return None\n",
    "\n",
    "    csv_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".csv\")])\n",
    "    if len(csv_files) == 0:\n",
    "        print(f\"  ERROR: No CSV files found in {folder_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"  Found {len(csv_files)} CSV files\")\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(folder_path, csv_file)\n",
    "            result = analyze_csv_pair(\n",
    "                www_cache_path,\n",
    "                www_nodes,\n",
    "                baseline_path,\n",
    "                csv_path,\n",
    "                min_conn,\n",
    "                max_conn,\n",
    "                range_name,\n",
    "            )\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during analysis: {e}\")\n",
    "        traceback.print_exc()\n",
    "        if len(results) > 0:\n",
    "            print(f\"  Partial results: {len(results)} files analyzed\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    # Aggregate results\n",
    "    all_sims = []\n",
    "    for r in results:\n",
    "        all_sims.extend(r[\"sim_results\"])\n",
    "\n",
    "    mean_deltas = [s[\"mean_delta_pct\"] for s in all_sims]\n",
    "    mean_val, margin = calculate_confidence_interval(mean_deltas)\n",
    "\n",
    "    result_summary = {\n",
    "        \"strategy_name\": strategy_name,\n",
    "        \"range_name\": range_name,\n",
    "        \"min_connections\": min_conn,\n",
    "        \"max_connections\": max_conn,\n",
    "        \"files_analyzed\": len(results),\n",
    "        \"total_simulations\": len(all_sims),\n",
    "        \"overall_mean_delta_pct\": mean_val,\n",
    "        \"overall_std\": np.std(mean_deltas),\n",
    "        \"confidence_interval_95\": margin,\n",
    "        \"avg_pages_up\": np.mean([s[\"pages_up\"] for s in all_sims]),\n",
    "        \"avg_pages_down\": np.mean([s[\"pages_down\"] for s in all_sims]),\n",
    "        \"efficiency\": mean_val / ((min_conn + max_conn) / 2),  # Delta per connection\n",
    "        \"file_results\": results,\n",
    "    }\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_mgr.save_checkpoint(range_name, strategy_name, result_summary)\n",
    "\n",
    "    log_memory_usage(\"after strategy\")\n",
    "    gc.collect()\n",
    "\n",
    "    return result_summary\n",
    "\n",
    "\n",
    "def create_multi_range_comparison_plot(all_results, output_dir):\n",
    "    \"\"\"Create comprehensive comparison across strategies and ranges\"\"\"\n",
    "    print(\"\\n  Creating multi-range comparison visualization...\")\n",
    "\n",
    "    strategies = sorted(list(set([r[\"strategy_name\"] for r in all_results])))\n",
    "    ranges = sorted(list(set([r[\"range_name\"] for r in all_results])))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "    fig.suptitle(\n",
    "        \"Multi-Range Strategy Comparison Analysis - Expert Led\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # 1. Mean Delta Heatmap\n",
    "    ax1 = axes[0, 0]\n",
    "    heatmap_data = np.zeros((len(strategies), len(ranges)))\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        for j, range_name in enumerate(ranges):\n",
    "            matching = [\n",
    "                r\n",
    "                for r in all_results\n",
    "                if r[\"strategy_name\"] == strategy and r[\"range_name\"] == range_name\n",
    "            ]\n",
    "            if matching:\n",
    "                heatmap_data[i, j] = matching[0][\"overall_mean_delta_pct\"]\n",
    "\n",
    "    im = ax1.imshow(heatmap_data, cmap=\"RdYlGn\", aspect=\"auto\")\n",
    "    ax1.set_xticks(np.arange(len(ranges)))\n",
    "    ax1.set_yticks(np.arange(len(strategies)))\n",
    "    ax1.set_xticklabels(ranges, rotation=45, ha=\"right\")\n",
    "    ax1.set_yticklabels(strategies)\n",
    "    ax1.set_title(\"Mean PageRank Delta (%) - Heatmap\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    for i in range(len(strategies)):\n",
    "        for j in range(len(ranges)):\n",
    "            text = ax1.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{heatmap_data[i, j]:.3f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    plt.colorbar(im, ax=ax1, label=\"Mean Delta %\")\n",
    "\n",
    "    # 2. Line plot: Strategy performance across ranges\n",
    "    ax2 = axes[0, 1]\n",
    "    colors = [\"#2E86AB\", \"#A23B72\", \"#F18F01\", \"#C73E1D\", \"#6A994E\"]\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        strategy_data = [r for r in all_results if r[\"strategy_name\"] == strategy]\n",
    "        strategy_data = sorted(strategy_data, key=lambda x: x[\"min_connections\"])\n",
    "\n",
    "        x_vals = [r[\"min_connections\"] for r in strategy_data]\n",
    "        y_vals = [r[\"overall_mean_delta_pct\"] for r in strategy_data]\n",
    "        yerr = [r[\"confidence_interval_95\"] for r in strategy_data]\n",
    "\n",
    "        ax2.errorbar(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            yerr=yerr,\n",
    "            marker=\"o\",\n",
    "            linewidth=2.5,\n",
    "            label=strategy,\n",
    "            color=colors[i % len(colors)],\n",
    "            capsize=5,\n",
    "        )\n",
    "\n",
    "    ax2.set_xlabel(\"Minimum Connections\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"Mean PageRank Delta (%)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax2.set_title(\n",
    "        \"Strategy Performance vs Connection Range (95% CI)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax2.legend(loc=\"best\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Efficiency plot: Delta per connection\n",
    "    ax3 = axes[1, 0]\n",
    "    efficiency_data = {}\n",
    "    for result in all_results:\n",
    "        strategy = result[\"strategy_name\"]\n",
    "        if strategy not in efficiency_data:\n",
    "            efficiency_data[strategy] = []\n",
    "        efficiency_data[strategy].append(\n",
    "            (result[\"min_connections\"], result[\"efficiency\"])\n",
    "        )\n",
    "\n",
    "    for i, (strategy, data) in enumerate(efficiency_data.items()):\n",
    "        data = sorted(data, key=lambda x: x[0])\n",
    "        x_vals = [d[0] for d in data]\n",
    "        y_vals = [d[1] for d in data]\n",
    "        ax3.plot(\n",
    "            x_vals,\n",
    "            y_vals,\n",
    "            marker=\"s\",\n",
    "            linewidth=2.5,\n",
    "            label=strategy,\n",
    "            color=colors[i % len(colors)],\n",
    "        )\n",
    "\n",
    "    ax3.set_xlabel(\"Minimum Connections\", fontsize=12, fontweight=\"bold\")\n",
    "    ax3.set_ylabel(\"Efficiency (Î”% per connection)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax3.set_title(\"ROI: PageRank Gain per Connection\", fontsize=14, fontweight=\"bold\")\n",
    "    ax3.legend(loc=\"best\")\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Summary table\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis(\"tight\")\n",
    "    ax4.axis(\"off\")\n",
    "\n",
    "    table_data = [[\"Strategy\", \"Range\", \"Mean Î”%\", \"95% CI\", \"â†‘Pages\", \"Efficiency\"]]\n",
    "    for result in sorted(\n",
    "        all_results, key=lambda x: (x[\"range_name\"], -x[\"overall_mean_delta_pct\"])\n",
    "    ):\n",
    "        table_data.append(\n",
    "            [\n",
    "                result[\"strategy_name\"][:12],\n",
    "                result[\"range_name\"].replace(\"Range_\", \"\"),\n",
    "                f\"{result['overall_mean_delta_pct']:.3f}\",\n",
    "                f\"Â±{result['confidence_interval_95']:.3f}\",\n",
    "                f\"{result['avg_pages_up']:.1f}\",\n",
    "                f\"{result['efficiency']:.4f}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    table = ax4.table(\n",
    "        cellText=table_data,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "        colWidths=[0.22, 0.15, 0.13, 0.13, 0.12, 0.15],\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(7)\n",
    "    table.scale(1, 2)\n",
    "\n",
    "    # Style header\n",
    "    for i in range(6):\n",
    "        table[(0, i)].set_facecolor(\"#40466e\")\n",
    "        table[(0, i)].set_text_props(weight=\"bold\", color=\"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(output_dir, \"multi_range_comparison_expert.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"  âœ“ Saved: {output_path}\")\n",
    "\n",
    "\n",
    "def create_detailed_analysis_report(all_results, output_dir):\n",
    "    \"\"\"Create detailed text report with statistical analysis\"\"\"\n",
    "    report_path = os.path.join(output_dir, \"detailed_analysis_report_expert.txt\")\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"MULTI-RANGE PAGERANK ANALYSIS - EXPERT LED - DETAILED REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        # Overall statistics\n",
    "        f.write(\"OVERALL STATISTICS\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        all_deltas = [r[\"overall_mean_delta_pct\"] for r in all_results]\n",
    "        total_sims = sum([r[\"total_simulations\"] for r in all_results])\n",
    "        f.write(f\"Total Runs: {len(all_results)}\\n\")\n",
    "        f.write(f\"Total Simulations: {total_sims:,}\\n\")\n",
    "        f.write(f\"Overall Mean Delta: {np.mean(all_deltas):.4f}%\\n\")\n",
    "        f.write(f\"Overall Std Dev: {np.std(all_deltas):.4f}%\\n\")\n",
    "        f.write(f\"Range: [{np.min(all_deltas):.4f}%, {np.max(all_deltas):.4f}%]\\n\\n\")\n",
    "\n",
    "        # Best overall\n",
    "        best = max(all_results, key=lambda x: x[\"overall_mean_delta_pct\"])\n",
    "        f.write(\"BEST OVERALL COMBINATION\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Strategy: {best['strategy_name']}\\n\")\n",
    "        f.write(\n",
    "            f\"Range: {best['range_name']} ({best['min_connections']}-{best['max_connections']})\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"Mean Delta: {best['overall_mean_delta_pct']:.4f}% Â± {best['confidence_interval_95']:.4f}%\\n\"\n",
    "        )\n",
    "        f.write(f\"Efficiency: {best['efficiency']:.5f}% per connection\\n\")\n",
    "        f.write(f\"Pages Up: {best['avg_pages_up']:.1f}\\n\")\n",
    "        f.write(f\"Pages Down: {best['avg_pages_down']:.1f}\\n\")\n",
    "        f.write(f\"Total Simulations: {best['total_simulations']:,}\\n\\n\")\n",
    "\n",
    "        # Analysis by range\n",
    "        ranges = sorted(list(set([r[\"range_name\"] for r in all_results])))\n",
    "        f.write(\"ANALYSIS BY CONNECTION RANGE\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        for range_name in ranges:\n",
    "            f.write(f\"{range_name}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            range_results = [r for r in all_results if r[\"range_name\"] == range_name]\n",
    "            range_results = sorted(\n",
    "                range_results, key=lambda x: x[\"overall_mean_delta_pct\"], reverse=True\n",
    "            )\n",
    "\n",
    "            for i, r in enumerate(range_results, 1):\n",
    "                f.write(\n",
    "                    f\"  {i}. {r['strategy_name']:<25} \"\n",
    "                    f\"{r['overall_mean_delta_pct']:>8.4f}% Â± {r['confidence_interval_95']:>6.4f}%  \"\n",
    "                    f\"Eff: {r['efficiency']:>7.5f}  \"\n",
    "                    f\"â†‘{r['avg_pages_up']:>5.1f} â†“{r['avg_pages_down']:>5.1f}\\n\"\n",
    "                )\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        # Strategy trends across ranges\n",
    "        f.write(\"STRATEGY TRENDS ACROSS RANGES\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        strategies = sorted(list(set([r[\"strategy_name\"] for r in all_results])))\n",
    "\n",
    "        for strategy in strategies:\n",
    "            f.write(f\"\\n{strategy}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            strat_results = [r for r in all_results if r[\"strategy_name\"] == strategy]\n",
    "            strat_results = sorted(strat_results, key=lambda x: x[\"min_connections\"])\n",
    "\n",
    "            deltas = [r[\"overall_mean_delta_pct\"] for r in strat_results]\n",
    "            effs = [r[\"efficiency\"] for r in strat_results]\n",
    "\n",
    "            # Trend analysis\n",
    "            if len(deltas) >= 2:\n",
    "                delta_trend = (\n",
    "                    \"Increasing\"\n",
    "                    if deltas[-1] > deltas[0]\n",
    "                    else \"Decreasing\"\n",
    "                    if deltas[-1] < deltas[0]\n",
    "                    else \"Stable\"\n",
    "                )\n",
    "                eff_trend = (\n",
    "                    \"Increasing\"\n",
    "                    if effs[-1] > effs[0]\n",
    "                    else \"Decreasing\"\n",
    "                    if effs[-1] < effs[0]\n",
    "                    else \"Stable\"\n",
    "                )\n",
    "            else:\n",
    "                delta_trend = eff_trend = \"Insufficient data\"\n",
    "\n",
    "            f.write(f\"  Delta Trend: {delta_trend}\\n\")\n",
    "            f.write(f\"  Efficiency Trend: {eff_trend}\\n\")\n",
    "            f.write(f\"  Performance:\\n\")\n",
    "            for r in strat_results:\n",
    "                f.write(\n",
    "                    f\"    {r['range_name']}: {r['overall_mean_delta_pct']:.4f}% \"\n",
    "                    f\"(Eff: {r['efficiency']:.5f})\\n\"\n",
    "                )\n",
    "\n",
    "        # Recommendations\n",
    "        f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        f.write(\"RECOMMENDATIONS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        # Best efficiency\n",
    "        best_eff = max(all_results, key=lambda x: x[\"efficiency\"])\n",
    "        f.write(\n",
    "            f\"1. Most Efficient: {best_eff['strategy_name']} @ {best_eff['range_name']}\\n\"\n",
    "        )\n",
    "        f.write(f\"   Efficiency: {best_eff['efficiency']:.5f}% per connection\\n\")\n",
    "        f.write(f\"   Mean Delta: {best_eff['overall_mean_delta_pct']:.4f}%\\n\\n\")\n",
    "\n",
    "        # Best per range\n",
    "        f.write(\"2. Best Strategy per Range:\\n\")\n",
    "        for range_name in ranges:\n",
    "            range_results = [r for r in all_results if r[\"range_name\"] == range_name]\n",
    "            best_in_range = max(\n",
    "                range_results, key=lambda x: x[\"overall_mean_delta_pct\"]\n",
    "            )\n",
    "            f.write(\n",
    "                f\"   {range_name}: {best_in_range['strategy_name']} \"\n",
    "                f\"({best_in_range['overall_mean_delta_pct']:.4f}%)\\n\"\n",
    "            )\n",
    "\n",
    "        f.write(\"\\n3. Scalability Analysis:\\n\")\n",
    "        for strategy in strategies:\n",
    "            strat_results = [r for r in all_results if r[\"strategy_name\"] == strategy]\n",
    "            if len(strat_results) >= 3:\n",
    "                strat_results = sorted(\n",
    "                    strat_results, key=lambda x: x[\"min_connections\"]\n",
    "                )\n",
    "                deltas = [r[\"overall_mean_delta_pct\"] for r in strat_results]\n",
    "                improvement = (\n",
    "                    ((deltas[-1] - deltas[0]) / abs(deltas[0])) * 100\n",
    "                    if deltas[0] != 0\n",
    "                    else 0\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"   {strategy}: {improvement:+.1f}% change from low to high range\\n\"\n",
    "                )\n",
    "\n",
    "    print(f\"  âœ“ Detailed report saved: {report_path}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OPTIMIZED MULTI-RANGE PAGERANK ANALYSIS - EXPERT LED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nPerformance Optimizations:\")\n",
    "    print(f\"  âœ“ Binary graph caching (50-100x faster)\")\n",
    "    print(f\"  âœ“ Worker pool with pre-loaded graphs\")\n",
    "    print(f\"  âœ“ Checkpoint system for crash recovery\")\n",
    "    print(f\"  âœ“ Memory monitoring and cleanup\")\n",
    "    print(f\"\\nConnection Ranges: {len(CONNECTION_RANGES)}\")\n",
    "    for min_c, max_c, name in CONNECTION_RANGES:\n",
    "        print(f\"  - {name}: {min_c}-{max_c} connections\")\n",
    "    print(f\"\\nStrategies: {len(COMPARISON_FOLDERS)}\")\n",
    "    for _, name in COMPARISON_FOLDERS:\n",
    "        print(f\"  - {name}\")\n",
    "    print(\n",
    "        f\"\\nTotal Runs: {len(CONNECTION_RANGES)} ranges Ã— {len(COMPARISON_FOLDERS)} strategies = {len(CONNECTION_RANGES) * len(COMPARISON_FOLDERS)} runs\"\n",
    "    )\n",
    "    print(f\"Simulations per run: {TOTAL_SIMULATIONS:,} (EXPERT LED)\")\n",
    "    print(f\"Parallel workers: {NUM_WORKERS}\")\n",
    "    print(f\"\\nEstimated runtime: 50-60 hours\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    log_memory_usage(\"initial\")\n",
    "\n",
    "    # Initialize systems\n",
    "    cache = GraphCache()\n",
    "    checkpoint_mgr = CheckpointManager()\n",
    "\n",
    "    # Check for existing checkpoints\n",
    "    existing_checkpoints = checkpoint_mgr.list_checkpoints()\n",
    "    if existing_checkpoints:\n",
    "        print(f\"\\nâœ“ Found {len(existing_checkpoints)} existing checkpoints\")\n",
    "        print(\"  (Will skip already completed runs)\")\n",
    "\n",
    "    # Load and cache WWW graph\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOADING WWW GRAPH\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    try:\n",
    "        www_graph, www_nodes = load_www_graph_optimized(\n",
    "            FINEWEB_WWW_PATH, use_cache=True\n",
    "        )\n",
    "\n",
    "        # Cache for workers\n",
    "        www_cache_path = cache.save_graph(www_graph, \"www_workers\")\n",
    "        if www_cache_path is None:\n",
    "            www_cache_path = cache._get_cache_path(\"www_workers\")\n",
    "\n",
    "        print(\n",
    "            f\"âœ“ WWW graph ready: {www_graph.numberOfNodes():,} nodes, {www_graph.numberOfEdges():,} edges\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— FATAL ERROR loading WWW graph: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "    all_results = []\n",
    "    total_runs = len(CONNECTION_RANGES) * len(COMPARISON_FOLDERS)\n",
    "    current_run = 0\n",
    "    failed_runs = []\n",
    "\n",
    "    overall_start = time.time()\n",
    "\n",
    "    # Main execution loop\n",
    "    for min_conn, max_conn, range_name in CONNECTION_RANGES:\n",
    "        print(f\"\\n\\n{'#' * 70}\")\n",
    "        print(f\"# PROCESSING CONNECTION RANGE: {range_name} ({min_conn}-{max_conn})\")\n",
    "        print(f\"{'#' * 70}\")\n",
    "\n",
    "        for folder_path, strategy_name in COMPARISON_FOLDERS:\n",
    "            current_run += 1\n",
    "            print(f\"\\n[RUN {current_run}/{total_runs}]\")\n",
    "\n",
    "            try:\n",
    "                result = process_strategy_with_range(\n",
    "                    www_cache_path,\n",
    "                    www_nodes,\n",
    "                    BASELINE_PATH,\n",
    "                    folder_path,\n",
    "                    strategy_name,\n",
    "                    min_conn,\n",
    "                    max_conn,\n",
    "                    range_name,\n",
    "                    checkpoint_mgr,\n",
    "                )\n",
    "\n",
    "                if result is not None:\n",
    "                    all_results.append(result)\n",
    "                    print(\n",
    "                        f\"  âœ“ {strategy_name} @ {range_name}: \"\n",
    "                        f\"Mean Î” = {result['overall_mean_delta_pct']:+.4f}% \"\n",
    "                        f\"Â± {result['confidence_interval_95']:.4f}% \"\n",
    "                        f\"(Eff: {result['efficiency']:.5f})\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"  âœ— No results for {strategy_name} @ {range_name}\")\n",
    "                    failed_runs.append((strategy_name, range_name))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— ERROR: {e}\")\n",
    "                traceback.print_exc()\n",
    "                failed_runs.append((strategy_name, range_name))\n",
    "                continue\n",
    "\n",
    "            log_memory_usage(f\"run {current_run}\")\n",
    "\n",
    "    overall_elapsed = time.time() - overall_start\n",
    "\n",
    "    # Save and visualize results\n",
    "    if all_results:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        # CSV output\n",
    "        summary_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"strategy\": r[\"strategy_name\"],\n",
    "                    \"range\": r[\"range_name\"],\n",
    "                    \"min_connections\": r[\"min_connections\"],\n",
    "                    \"max_connections\": r[\"max_connections\"],\n",
    "                    \"total_simulations\": r[\"total_simulations\"],\n",
    "                    \"mean_delta_pct\": r[\"overall_mean_delta_pct\"],\n",
    "                    \"ci_95\": r[\"confidence_interval_95\"],\n",
    "                    \"std_delta\": r[\"overall_std\"],\n",
    "                    \"avg_pages_up\": r[\"avg_pages_up\"],\n",
    "                    \"avg_pages_down\": r[\"avg_pages_down\"],\n",
    "                    \"efficiency\": r[\"efficiency\"],\n",
    "                    \"files_analyzed\": r[\"files_analyzed\"],\n",
    "                }\n",
    "                for r in all_results\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        csv_path = os.path.join(\n",
    "            MULTI_RANGE_OUTPUT_DIR, \"multi_range_results_expert.csv\"\n",
    "        )\n",
    "        summary_df.to_csv(csv_path, index=False)\n",
    "        print(f\"  âœ“ Results CSV: {csv_path}\")\n",
    "\n",
    "        # Create visualizations\n",
    "        create_multi_range_comparison_plot(all_results, MULTI_RANGE_OUTPUT_DIR)\n",
    "\n",
    "        # Create detailed report\n",
    "        create_detailed_analysis_report(all_results, MULTI_RANGE_OUTPUT_DIR)\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"FINAL RESULTS SUMMARY - EXPERT LED\")\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "        # Group by range\n",
    "        for min_conn, max_conn, range_name in CONNECTION_RANGES:\n",
    "            print(f\"\\n{range_name} ({min_conn}-{max_conn} connections):\")\n",
    "            print(\"-\" * 70)\n",
    "            range_results = [r for r in all_results if r[\"range_name\"] == range_name]\n",
    "            range_results = sorted(\n",
    "                range_results, key=lambda x: x[\"overall_mean_delta_pct\"], reverse=True\n",
    "            )\n",
    "\n",
    "            for i, r in enumerate(range_results, 1):\n",
    "                print(\n",
    "                    f\"  {i}. {r['strategy_name']:<20} {r['overall_mean_delta_pct']:>8.4f}% Â± {r['confidence_interval_95']:>6.4f}%  \"\n",
    "                    f\"Eff: {r['efficiency']:>7.5f}  (â†‘{r['avg_pages_up']:>5.1f} â†“{r['avg_pages_down']:>5.1f})\"\n",
    "                )\n",
    "\n",
    "        # Key findings\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"KEY FINDINGS - EXPERT LED ANALYSIS\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "\n",
    "        best_overall = max(all_results, key=lambda x: x[\"overall_mean_delta_pct\"])\n",
    "        print(f\"\\nðŸ† BEST PERFORMANCE:\")\n",
    "        print(f\"   Strategy: {best_overall['strategy_name']}\")\n",
    "        print(\n",
    "            f\"   Range: {best_overall['range_name']} ({best_overall['min_connections']}-{best_overall['max_connections']})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Mean Delta: {best_overall['overall_mean_delta_pct']:+.4f}% Â± {best_overall['confidence_interval_95']:.4f}%\"\n",
    "        )\n",
    "        print(f\"   Efficiency: {best_overall['efficiency']:.5f}% per connection\")\n",
    "\n",
    "        best_eff = max(all_results, key=lambda x: x[\"efficiency\"])\n",
    "        print(f\"\\nðŸ’° BEST ROI (Efficiency):\")\n",
    "        print(f\"   Strategy: {best_eff['strategy_name']}\")\n",
    "        print(f\"   Range: {best_eff['range_name']}\")\n",
    "        print(f\"   Efficiency: {best_eff['efficiency']:.5f}% per connection\")\n",
    "        print(f\"   Mean Delta: {best_eff['overall_mean_delta_pct']:+.4f}%\")\n",
    "\n",
    "        print(f\"\\nðŸ“Š BEST STRATEGY PER RANGE:\")\n",
    "        for min_conn, max_conn, range_name in CONNECTION_RANGES:\n",
    "            range_results = [r for r in all_results if r[\"range_name\"] == range_name]\n",
    "            best_in_range = max(\n",
    "                range_results, key=lambda x: x[\"overall_mean_delta_pct\"]\n",
    "            )\n",
    "            print(\n",
    "                f\"   {range_name}: {best_in_range['strategy_name']} ({best_in_range['overall_mean_delta_pct']:+.4f}%)\"\n",
    "            )\n",
    "\n",
    "        print(f\"\\nâ±ï¸  PERFORMANCE METRICS:\")\n",
    "        print(f\"   Total Runtime: {overall_elapsed / 3600:.2f} hours\")\n",
    "        print(f\"   Successful Runs: {len(all_results)}/{total_runs}\")\n",
    "        print(f\"   Failed Runs: {len(failed_runs)}\")\n",
    "        print(\n",
    "            f\"   Total Simulations: {sum(r['total_simulations'] for r in all_results):,}\"\n",
    "        )\n",
    "\n",
    "        if failed_runs:\n",
    "            print(f\"\\nâš ï¸  FAILED RUNS:\")\n",
    "            for strategy, range_name in failed_runs:\n",
    "                print(f\"   - {strategy} @ {range_name}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nâŒ No valid results obtained\")\n",
    "\n",
    "    # Cleanup\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"CLEANUP\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    del www_graph\n",
    "    gc.collect()\n",
    "    log_memory_usage(\"final\")\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"MULTI-RANGE EXPERT LED ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"\\nâœ“ Results saved to: {MULTI_RANGE_OUTPUT_DIR}\")\n",
    "    print(f\"âœ“ Visualization: multi_range_comparison_expert.png\")\n",
    "    print(f\"âœ“ Detailed report: detailed_analysis_report_expert.txt\")\n",
    "    print(f\"âœ“ Raw data: multi_range_results_expert.csv\")\n",
    "    print(f\"\\nðŸ“š Next Steps:\")\n",
    "    print(f\"  1. Review the heatmap for strategyÃ—range performance patterns\")\n",
    "    print(f\"  2. Check efficiency plot to optimize connection budget\")\n",
    "    print(f\"  3. Read detailed_analysis_report_expert.txt for recommendations\")\n",
    "    print(f\"  4. Compare with automatic-led results for strategy insights\")\n",
    "    print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diFpI7RRx7Zb"
   },
   "outputs": [],
   "source": [
    "# === CHECK KALICUBE PAGE COUNT ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KALICUBE DATASET ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Baseline graph\n",
    "baseline_path = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "\n",
    "if os.path.exists(baseline_path):\n",
    "    print(f\"\\nðŸ“Š BASELINE GRAPH:\")\n",
    "    df = pd.read_csv(baseline_path, usecols=[\"FROM\", \"TO\"])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Get unique pages\n",
    "    all_pages = pd.concat([df[\"FROM\"], df[\"TO\"]]).unique()\n",
    "\n",
    "    print(f\"   Total Pages: {len(all_pages):,}\")\n",
    "    print(f\"   Total Edges: {len(df):,}\")\n",
    "    print(f\"   Avg Connections per Page: {len(df) / len(all_pages):.2f}\")\n",
    "\n",
    "    # Additional stats\n",
    "    from_counts = df[\"FROM\"].value_counts()\n",
    "    to_counts = df[\"TO\"].value_counts()\n",
    "\n",
    "    print(f\"\\n   Outgoing Links:\")\n",
    "    print(f\"      Max: {from_counts.max()} (from 1 page)\")\n",
    "    print(f\"      Mean: {from_counts.mean():.2f}\")\n",
    "    print(f\"      Median: {from_counts.median():.0f}\")\n",
    "\n",
    "    print(f\"\\n   Incoming Links:\")\n",
    "    print(f\"      Max: {to_counts.max()} (to 1 page)\")\n",
    "    print(f\"      Mean: {to_counts.mean():.2f}\")\n",
    "    print(f\"      Median: {to_counts.median():.0f}\")\n",
    "else:\n",
    "    print(f\"âŒ Baseline file not found: {baseline_path}\")\n",
    "\n",
    "# Check comparison strategies\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"COMPARISON STRATEGIES:\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "strategies = [\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/high_batches/\",\n",
    "        \"High Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/low_batches/\",\n",
    "        \"Low Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/mixed_batches/\",\n",
    "        \"Mixed Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/random_batches/\",\n",
    "        \"Random Candidates\",\n",
    "    ),\n",
    "    (\n",
    "        \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/folder_batches/\",\n",
    "        \"Folder Candidates\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "for folder_path, strategy_name in strategies:\n",
    "    if os.path.exists(folder_path):\n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "        if csv_files:\n",
    "            print(f\"\\nðŸ“ {strategy_name}:\")\n",
    "            print(f\"   CSV Files: {len(csv_files)}\")\n",
    "\n",
    "            # Analyze first file as sample\n",
    "            sample_path = os.path.join(folder_path, csv_files[0])\n",
    "            df_sample = pd.read_csv(sample_path, usecols=[\"FROM\", \"TO\"])\n",
    "            df_sample = df_sample.dropna()\n",
    "            all_pages_sample = pd.concat([df_sample[\"FROM\"], df_sample[\"TO\"]]).unique()\n",
    "\n",
    "            print(f\"   Sample File: {csv_files[0]}\")\n",
    "            print(f\"      Pages: {len(all_pages_sample):,}\")\n",
    "            print(f\"      Edges: {len(df_sample):,}\")\n",
    "        else:\n",
    "            print(f\"\\nðŸ“ {strategy_name}: No CSV files found\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ“ {strategy_name}: Folder not found\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}