{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4ot8mq0U5XN"
   },
   "outputs": [],
   "source": [
    "# Google Drive Folder-Level PageRank Analysis\n",
    "# Processes all CSV pairs in a mounted Google Drive folder\n",
    "# Calculates overall averages across all files in the strategy\n",
    "\n",
    "# === INSTALLATION CELL (Run first) ===\n",
    "# !pip install networkit pandas numpy\n",
    "\n",
    "# === MOUNT GOOGLE DRIVE ===\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# === MAIN CODE ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkit as nk\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# USER CONFIGURATION\n",
    "# ============================================\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "COMPARISON_FOLDER = \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/low_batches/\"\n",
    "NUM_SIMULATIONS = 100\n",
    "\n",
    "# Simulation Parameters\n",
    "TOTAL_NODES_WWW = 100000\n",
    "EDGES_PER_NEW_NODE = 2\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "PAGERANK_TOLERANCE = 1e-6\n",
    "\n",
    "# ============================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path):\n",
    "    \"\"\"Load graph from CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        from_urls = df[\"FROM\"].values\n",
    "        to_urls = df[\"TO\"].values\n",
    "        all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "        url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "        g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "        for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "            g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "        return g, all_urls, url_to_idx\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def create_www_graph_networkit(n_nodes, m_edges, seed=42):\n",
    "    \"\"\"Create WWW graph with caching.\"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    cache_key = (n_nodes, m_edges, seed)\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph\n",
    "\n",
    "    nk.setSeed(seed, False)\n",
    "    generator = nk.generators.BarabasiAlbertGenerator(\n",
    "        k=m_edges, nMax=n_nodes, n0=m_edges\n",
    "    )\n",
    "    www_graph = generator.generate()\n",
    "\n",
    "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (cache_key, cached_graph)\n",
    "    return www_graph\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph, kalicube_edges, kalicube_nodes, kalicube_url_mapping\n",
    "):\n",
    "    \"\"\"Process configuration and calculate PageRank.\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "\n",
    "    merged_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    n_www_sample = min(MIN_CONNECTIONS, TOTAL_NODES_WWW)\n",
    "    n_kalicube_sample = min(MIN_CONNECTIONS, len(kalicube_nodes))\n",
    "\n",
    "    www_nodes_sample = np.random.choice(\n",
    "        TOTAL_NODES_WWW, size=n_www_sample, replace=False\n",
    "    )\n",
    "    kalicube_indices = np.random.choice(\n",
    "        len(kalicube_nodes), size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
    "\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def run_single_simulation_networkit(\n",
    "    simulation_id,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"Run single simulation.\"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    www_graph = create_www_graph_networkit(\n",
    "        TOTAL_NODES_WWW, EDGES_PER_NEW_NODE, sim_seed\n",
    "    )\n",
    "\n",
    "    pagerank_old_dict = process_configuration_networkit(\n",
    "        www_graph, kalicube_old_edges, kalicube_nodes_old, kalicube_url_mapping_old\n",
    "    )\n",
    "\n",
    "    pagerank_new_dict = process_configuration_networkit(\n",
    "        www_graph, kalicube_new_edges, kalicube_nodes_new, kalicube_url_mapping_new\n",
    "    )\n",
    "\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return None\n",
    "\n",
    "    deltas_pct = []\n",
    "    for url in common_urls:\n",
    "        before = pagerank_old_dict[url]\n",
    "        after = pagerank_new_dict[url]\n",
    "        if before > 0:\n",
    "            delta_pct = ((after - before) / before) * 100\n",
    "            deltas_pct.append(delta_pct)\n",
    "\n",
    "    if len(deltas_pct) == 0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"mean_delta_pct\": np.mean(deltas_pct),\n",
    "        \"min_delta_pct\": np.min(deltas_pct),\n",
    "        \"max_delta_pct\": np.max(deltas_pct),\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_csv_pair(old_csv_path, new_csv_path):\n",
    "    \"\"\"Analyze a pair of CSV files.\"\"\"\n",
    "    print(f\"\\nAnalyzing: {os.path.basename(new_csv_path)}\")\n",
    "\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_networkit(old_csv_path)\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        print(f\"  Failed to load old graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_networkit(new_csv_path)\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        print(f\"  Failed to load new graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
    "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
    "\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"  Running {NUM_SIMULATIONS} simulations...\")\n",
    "    sim_results = []\n",
    "\n",
    "    for sim_id in range(NUM_SIMULATIONS):\n",
    "        result = run_single_simulation_networkit(\n",
    "            sim_id,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "        if result is not None:\n",
    "            sim_results.append(result)\n",
    "\n",
    "        if (sim_id + 1) % 20 == 0:\n",
    "            print(f\"    Progress: {sim_id + 1}/{NUM_SIMULATIONS}\")\n",
    "\n",
    "    if len(sim_results) == 0:\n",
    "        print(f\"  No valid results\")\n",
    "        return None\n",
    "\n",
    "    avg_mean = np.mean([r[\"mean_delta_pct\"] for r in sim_results])\n",
    "    avg_min = np.mean([r[\"min_delta_pct\"] for r in sim_results])\n",
    "    avg_max = np.mean([r[\"max_delta_pct\"] for r in sim_results])\n",
    "\n",
    "    return {\n",
    "        \"filename\": os.path.basename(new_csv_path),\n",
    "        \"avg_mean_delta_pct\": avg_mean,\n",
    "        \"avg_min_delta_pct\": avg_min,\n",
    "        \"avg_max_delta_pct\": avg_max,\n",
    "        \"sim_results\": sim_results,  # Store individual simulation results\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_results(result):\n",
    "    \"\"\"Validate that max >= mean >= min.\"\"\"\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    avg_max = result[\"avg_max_delta_pct\"]\n",
    "    avg_mean = result[\"avg_mean_delta_pct\"]\n",
    "    avg_min = result[\"avg_min_delta_pct\"]\n",
    "\n",
    "    if not (avg_max >= avg_mean >= avg_min):\n",
    "        print(f\"  WARNING: Validation failed for {result['filename']}\")\n",
    "        print(f\"     Max: {avg_max:.2f}%, Mean: {avg_mean:.2f}%, Min: {avg_min:.2f}%\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FOLDER-LEVEL PAGERANK ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not os.path.exists(BASELINE_PATH):\n",
    "        print(f\"\\nERROR: Baseline file not found: {BASELINE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\nBaseline: {os.path.basename(BASELINE_PATH)}\")\n",
    "\n",
    "    if not os.path.exists(COMPARISON_FOLDER):\n",
    "        print(f\"\\nERROR: Comparison folder not found: {COMPARISON_FOLDER}\")\n",
    "        exit(1)\n",
    "\n",
    "    csv_files = sorted([f for f in os.listdir(COMPARISON_FOLDER) if f.endswith(\".csv\")])\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        print(f\"\\nERROR: No CSV files found in {COMPARISON_FOLDER}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files in comparison folder\")\n",
    "    print(f\"Analyzing {len(csv_files)} comparison files\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "    all_simulation_results = []  # Collect all simulation results across all files\n",
    "\n",
    "    for new_csv_filename in csv_files:\n",
    "        new_csv_path = os.path.join(COMPARISON_FOLDER, new_csv_filename)\n",
    "        result = analyze_csv_pair(BASELINE_PATH, new_csv_path)\n",
    "\n",
    "        if result is not None and validate_results(result):\n",
    "            results.append(result)\n",
    "            all_simulation_results.extend(\n",
    "                result[\"sim_results\"]\n",
    "            )  # Aggregate simulation results\n",
    "            print(f\"  Valid results obtained\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INDIVIDUAL FILE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"\\nNo valid results obtained\")\n",
    "    else:\n",
    "        print(f\"\\n{'File':<50} {'Avg Mean':<12} {'Avg Min':<12} {'Avg Max':<12}\")\n",
    "        print(\"-\" * 90)\n",
    "\n",
    "        for result in results:\n",
    "            print(\n",
    "                f\"{result['filename']:<50} \"\n",
    "                f\"{result['avg_mean_delta_pct']:>10.2f}% \"\n",
    "                f\"{result['avg_min_delta_pct']:>10.2f}% \"\n",
    "                f\"{result['avg_max_delta_pct']:>10.2f}%\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"\\nSuccessfully analyzed {len(results)}/{len(csv_files)} files\")\n",
    "\n",
    "    # Calculate overall averages across all simulations\n",
    "    if len(all_simulation_results) > 0:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"OVERALL AVERAGES\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        overall_mean = np.mean([r[\"mean_delta_pct\"] for r in all_simulation_results])\n",
    "        overall_min = np.mean([r[\"min_delta_pct\"] for r in all_simulation_results])\n",
    "        overall_max = np.mean([r[\"max_delta_pct\"] for r in all_simulation_results])\n",
    "\n",
    "        print(f\"\\nTotal simulations across all files: {len(all_simulation_results)}\")\n",
    "        print(f\"Total files analyzed: {len(results)}\")\n",
    "        print(f\"\\nOverall Average Mean Delta: {overall_mean:>10.2f}%\")\n",
    "        print(f\"Overall Average Min Delta:  {overall_min:>10.2f}%\")\n",
    "        print(f\"Overall Average Max Delta:  {overall_max:>10.2f}%\")\n",
    "\n",
    "        # Additional statistics\n",
    "        all_means = [r[\"mean_delta_pct\"] for r in all_simulation_results]\n",
    "        print(f\"\\nStandard Deviation (Mean): {np.std(all_means):>10.2f}%\")\n",
    "        print(f\"Median (Mean):             {np.median(all_means):>10.2f}%\")\n",
    "\n",
    "    _www_graph_cache = None\n",
    "    gc.collect()\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Analysis complete!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
