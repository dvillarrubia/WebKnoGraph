{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "P4ot8mq0U5XN"
   },
   "outputs": [],
   "source": [
    "# Google Drive Folder-Level PageRank Analysis with REAL WWW DATA\n",
    "# Enhanced version with page movement tracking (up/down/neutral)\n",
    "# Multi-folder version - processes multiple comparison folders\n",
    "# Uses FineWeb dataset CSV with FROM and TO columns as WWW graph\n",
    "# Modified: 20 boosting rounds × 100 bridgings each = 2,000 simulations\n",
    "\n",
    "# === INSTALLATION CELL (Run first) ===\n",
    "!pip install networkit pandas numpy tqdm\n",
    "\n",
    "# === MOUNT GOOGLE DRIVE ===\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# === MAIN CODE ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkit as nk\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ============================================\n",
    "# USER CONFIGURATION\n",
    "# ============================================\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "\n",
    "# NEW: List of comparison folders to process\n",
    "COMPARISON_FOLDERS = [\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/random_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/high_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/folder_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/mixed_batches/\",\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/expert_led/low_batches/\",\n",
    "]\n",
    "\n",
    "# Path to FineWeb WWW graph CSV\n",
    "FINEWEB_WWW_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/fineweb_500k_pages.csv\"\n",
    "\n",
    "# NEW: Boosting configuration\n",
    "NUM_BOOSTING_ROUNDS = 20\n",
    "BRIDGINGS_PER_ROUND = 20\n",
    "TOTAL_SIMULATIONS = NUM_BOOSTING_ROUNDS * BRIDGINGS_PER_ROUND\n",
    "\n",
    "# NEW: Threshold for neutral changes (percentage)\n",
    "NEUTRAL_THRESHOLD = 0.01  # ±0.01% considered neutral\n",
    "\n",
    "# Simulation Parameters\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "PAGERANK_TOLERANCE = 1e-3\n",
    "\n",
    "# Parallelization settings\n",
    "USE_PARALLEL = True\n",
    "NUM_WORKERS = max(2, cpu_count())\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/WebKnoGraph/results/\"\n",
    "STRATEGY_SUMMARY_FILE = \"strategy_comparison_results.csv\"\n",
    "DETAILED_RESULTS_FILE = \"detailed_strategy_results.csv\"\n",
    "\n",
    "# ============================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path, graph_name=\"graph\"):\n",
    "    \"\"\"Load graph from CSV file.\"\"\"\n",
    "    try:\n",
    "        print(f\"  Loading {graph_name} from {os.path.basename(file_path)}...\")\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"  ERROR: No valid edges found in {file_path}\")\n",
    "            return None, None, None\n",
    "\n",
    "        from_urls = df[\"FROM\"].values\n",
    "        to_urls = df[\"TO\"].values\n",
    "        all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "        url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "        g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "        for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "            g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "        print(f\"    Loaded: {len(all_urls):,} nodes, {len(df):,} edges\")\n",
    "        return g, all_urls, url_to_idx\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def load_www_graph_networkit(www_csv_path):\n",
    "    \"\"\"Load REAL WWW graph from FineWeb dataset with caching.\"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == www_csv_path:\n",
    "        print(\"  Using cached WWW graph\")\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph, _www_graph_cache[2]\n",
    "\n",
    "    print(\"\\nLoading REAL WWW graph from FineWeb dataset...\")\n",
    "    www_graph, www_nodes, www_url_mapping = load_graph_from_csv_networkit(\n",
    "        www_csv_path, graph_name=\"WWW graph\"\n",
    "    )\n",
    "\n",
    "    if www_graph is None:\n",
    "        raise ValueError(f\"Failed to load WWW graph from {www_csv_path}\")\n",
    "\n",
    "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (www_csv_path, cached_graph, www_nodes)\n",
    "\n",
    "    print(f\"  WWW graph cached successfully\")\n",
    "    return www_graph, www_nodes\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph, www_nodes, kalicube_edges, kalicube_nodes, kalicube_url_mapping\n",
    "):\n",
    "    \"\"\"Process configuration and calculate PageRank.\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "    n_www = www_graph.numberOfNodes()\n",
    "\n",
    "    merged_graph = nk.Graph(n=n_www, weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    n_www_sample = min(MAX_CONNECTIONS, n_www)\n",
    "    n_kalicube_sample = min(MAX_CONNECTIONS, n_kalicube)\n",
    "\n",
    "    www_nodes_sample = np.random.choice(n_www, size=n_www_sample, replace=False)\n",
    "    kalicube_indices = np.random.choice(\n",
    "        n_kalicube, size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
    "\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def run_single_simulation_networkit(\n",
    "    simulation_id,\n",
    "    www_graph,\n",
    "    www_nodes,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"Run single simulation with page movement tracking.\"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    pagerank_old_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_old_edges,\n",
    "        kalicube_nodes_old,\n",
    "        kalicube_url_mapping_old,\n",
    "    )\n",
    "\n",
    "    pagerank_new_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_new_edges,\n",
    "        kalicube_nodes_new,\n",
    "        kalicube_url_mapping_new,\n",
    "    )\n",
    "\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return None\n",
    "\n",
    "    deltas_pct = []\n",
    "    pages_up = 0\n",
    "    pages_down = 0\n",
    "    pages_neutral = 0\n",
    "\n",
    "    for url in common_urls:\n",
    "        before = pagerank_old_dict[url]\n",
    "        after = pagerank_new_dict[url]\n",
    "        if before > 0:\n",
    "            delta_pct = ((after - before) / before) * 100\n",
    "            deltas_pct.append(delta_pct)\n",
    "\n",
    "            # Count page movement\n",
    "            if delta_pct > NEUTRAL_THRESHOLD:\n",
    "                pages_up += 1\n",
    "            elif delta_pct < -NEUTRAL_THRESHOLD:\n",
    "                pages_down += 1\n",
    "            else:\n",
    "                pages_neutral += 1\n",
    "\n",
    "    if len(deltas_pct) == 0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"mean_delta_pct\": np.mean(deltas_pct),\n",
    "        \"min_delta_pct\": np.min(deltas_pct),\n",
    "        \"max_delta_pct\": np.max(deltas_pct),\n",
    "        \"pages_up\": pages_up,\n",
    "        \"pages_down\": pages_down,\n",
    "        \"pages_neutral\": pages_neutral,\n",
    "        \"total_pages\": len(common_urls),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_simulation_batch(args):\n",
    "    \"\"\"Helper function for parallel processing of simulation batches.\"\"\"\n",
    "    (\n",
    "        sim_ids,\n",
    "        www_graph_data,\n",
    "        www_nodes,\n",
    "        kalicube_old_edges,\n",
    "        kalicube_new_edges,\n",
    "        kalicube_nodes_old,\n",
    "        kalicube_nodes_new,\n",
    "        kalicube_url_mapping_old,\n",
    "        kalicube_url_mapping_new,\n",
    "    ) = args\n",
    "\n",
    "    www_graph = nk.Graph(n=www_graph_data[\"n_nodes\"], weighted=False, directed=True)\n",
    "    for u, v in www_graph_data[\"edges\"]:\n",
    "        www_graph.addEdge(u, v)\n",
    "\n",
    "    batch_results = []\n",
    "    for sim_id in sim_ids:\n",
    "        result = run_single_simulation_networkit(\n",
    "            sim_id,\n",
    "            www_graph,\n",
    "            www_nodes,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def analyze_csv_pair(www_graph, www_nodes, old_csv_path, new_csv_path):\n",
    "    \"\"\"Analyze a pair of CSV files with boosting rounds and page tracking.\"\"\"\n",
    "    print(f\"\\nAnalyzing: {os.path.basename(new_csv_path)}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_networkit(old_csv_path, \"baseline Kalicube\")\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        print(f\"  Failed to load old graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_networkit(new_csv_path, \"comparison Kalicube\")\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        print(f\"  Failed to load new graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
    "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
    "\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(\n",
    "        f\"  Running {NUM_BOOSTING_ROUNDS} boosting rounds ({BRIDGINGS_PER_ROUND} bridgings each)...\"\n",
    "    )\n",
    "    print(f\"  Total simulations: {TOTAL_SIMULATIONS}\")\n",
    "\n",
    "    www_graph_data = {\n",
    "        \"n_nodes\": www_graph.numberOfNodes(),\n",
    "        \"edges\": [(u, v) for u, v in www_graph.iterEdges()],\n",
    "    }\n",
    "\n",
    "    all_sim_results = []\n",
    "    boosting_round_results = []\n",
    "\n",
    "    if USE_PARALLEL:\n",
    "        print(\n",
    "            f\"  Using parallel processing with {NUM_WORKERS} workers (batch size: {BATCH_SIZE})\"\n",
    "        )\n",
    "\n",
    "        all_sim_ids = list(range(TOTAL_SIMULATIONS))\n",
    "        sim_id_batches = []\n",
    "        for i in range(0, TOTAL_SIMULATIONS, BATCH_SIZE):\n",
    "            sim_ids = all_sim_ids[i : i + BATCH_SIZE]\n",
    "            sim_id_batches.append(sim_ids)\n",
    "\n",
    "        print(f\"  Total batches to process: {len(sim_id_batches)}\")\n",
    "\n",
    "        batch_args = [\n",
    "            (\n",
    "                sim_ids,\n",
    "                www_graph_data,\n",
    "                www_nodes,\n",
    "                kalicube_old_edges,\n",
    "                kalicube_new_edges,\n",
    "                kalicube_nodes_old,\n",
    "                kalicube_nodes_new,\n",
    "                kalicube_url_mapping_old,\n",
    "                kalicube_url_mapping_new,\n",
    "            )\n",
    "            for sim_ids in sim_id_batches\n",
    "        ]\n",
    "\n",
    "        with Pool(NUM_WORKERS) as pool:\n",
    "            with tqdm(\n",
    "                total=len(sim_id_batches),\n",
    "                desc=\"  Processing batches\",\n",
    "                unit=\"batch\",\n",
    "                ncols=100,\n",
    "            ) as pbar:\n",
    "                for batch_results in pool.imap_unordered(\n",
    "                    run_simulation_batch, batch_args\n",
    "                ):\n",
    "                    all_sim_results.extend(batch_results)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    elapsed = time.time() - start_time\n",
    "                    completed_sims = len(all_sim_results)\n",
    "                    rate = completed_sims / elapsed if elapsed > 0 else 0\n",
    "                    pbar.set_postfix(\n",
    "                        {\n",
    "                            \"sims\": completed_sims,\n",
    "                            \"rate\": f\"{rate:.1f}/s\",\n",
    "                            \"ETA\": f\"{(TOTAL_SIMULATIONS - completed_sims) / rate / 60:.1f}m\"\n",
    "                            if rate > 0\n",
    "                            else \"N/A\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        print(f\"  Organizing results into {NUM_BOOSTING_ROUNDS} rounds...\")\n",
    "        for round_num in range(NUM_BOOSTING_ROUNDS):\n",
    "            round_size = len(all_sim_results) // NUM_BOOSTING_ROUNDS\n",
    "            start_idx = round_num * round_size\n",
    "            end_idx = (\n",
    "                start_idx + round_size\n",
    "                if round_num < NUM_BOOSTING_ROUNDS - 1\n",
    "                else len(all_sim_results)\n",
    "            )\n",
    "            round_results = all_sim_results[start_idx:end_idx]\n",
    "\n",
    "            if len(round_results) > 0:\n",
    "                round_summary = {\n",
    "                    \"round\": round_num + 1,\n",
    "                    \"num_bridgings\": len(round_results),\n",
    "                    \"avg_mean_delta\": np.mean(\n",
    "                        [r[\"mean_delta_pct\"] for r in round_results]\n",
    "                    ),\n",
    "                    \"avg_min_delta\": np.mean(\n",
    "                        [r[\"min_delta_pct\"] for r in round_results]\n",
    "                    ),\n",
    "                    \"avg_max_delta\": np.mean(\n",
    "                        [r[\"max_delta_pct\"] for r in round_results]\n",
    "                    ),\n",
    "                    \"avg_pages_up\": np.mean([r[\"pages_up\"] for r in round_results]),\n",
    "                    \"avg_pages_down\": np.mean([r[\"pages_down\"] for r in round_results]),\n",
    "                    \"avg_pages_neutral\": np.mean(\n",
    "                        [r[\"pages_neutral\"] for r in round_results]\n",
    "                    ),\n",
    "                }\n",
    "                boosting_round_results.append(round_summary)\n",
    "\n",
    "    else:\n",
    "        print(f\"  Using sequential processing\")\n",
    "\n",
    "        with tqdm(\n",
    "            total=NUM_BOOSTING_ROUNDS, desc=\"  Boosting rounds\", unit=\"round\", ncols=100\n",
    "        ) as pbar_rounds:\n",
    "            for round_num in range(NUM_BOOSTING_ROUNDS):\n",
    "                round_results = []\n",
    "\n",
    "                with tqdm(\n",
    "                    total=BRIDGINGS_PER_ROUND,\n",
    "                    desc=f\"    Round {round_num + 1}\",\n",
    "                    unit=\"bridging\",\n",
    "                    leave=False,\n",
    "                    ncols=100,\n",
    "                ) as pbar_bridgings:\n",
    "                    for bridging_num in range(BRIDGINGS_PER_ROUND):\n",
    "                        sim_id = round_num * BRIDGINGS_PER_ROUND + bridging_num\n",
    "\n",
    "                        result = run_single_simulation_networkit(\n",
    "                            sim_id,\n",
    "                            www_graph,\n",
    "                            www_nodes,\n",
    "                            kalicube_old_edges,\n",
    "                            kalicube_new_edges,\n",
    "                            kalicube_nodes_old,\n",
    "                            kalicube_nodes_new,\n",
    "                            kalicube_url_mapping_old,\n",
    "                            kalicube_url_mapping_new,\n",
    "                        )\n",
    "                        if result is not None:\n",
    "                            result[\"round\"] = round_num + 1\n",
    "                            result[\"bridging\"] = bridging_num + 1\n",
    "                            round_results.append(result)\n",
    "                            all_sim_results.append(result)\n",
    "\n",
    "                        pbar_bridgings.update(1)\n",
    "\n",
    "                        elapsed = time.time() - start_time\n",
    "                        completed = len(all_sim_results)\n",
    "                        rate = completed / elapsed if elapsed > 0 else 0\n",
    "                        pbar_bridgings.set_postfix({\"rate\": f\"{rate:.1f}/s\"})\n",
    "\n",
    "                if len(round_results) > 0:\n",
    "                    round_summary = {\n",
    "                        \"round\": round_num + 1,\n",
    "                        \"num_bridgings\": len(round_results),\n",
    "                        \"avg_mean_delta\": np.mean(\n",
    "                            [r[\"mean_delta_pct\"] for r in round_results]\n",
    "                        ),\n",
    "                        \"avg_min_delta\": np.mean(\n",
    "                            [r[\"min_delta_pct\"] for r in round_results]\n",
    "                        ),\n",
    "                        \"avg_max_delta\": np.mean(\n",
    "                            [r[\"max_delta_pct\"] for r in round_results]\n",
    "                        ),\n",
    "                        \"avg_pages_up\": np.mean([r[\"pages_up\"] for r in round_results]),\n",
    "                        \"avg_pages_down\": np.mean(\n",
    "                            [r[\"pages_down\"] for r in round_results]\n",
    "                        ),\n",
    "                        \"avg_pages_neutral\": np.mean(\n",
    "                            [r[\"pages_neutral\"] for r in round_results]\n",
    "                        ),\n",
    "                    }\n",
    "                    boosting_round_results.append(round_summary)\n",
    "\n",
    "                pbar_rounds.update(1)\n",
    "\n",
    "                elapsed = time.time() - start_time\n",
    "                completed = len(all_sim_results)\n",
    "                rate = completed / elapsed if elapsed > 0 else 0\n",
    "                remaining = TOTAL_SIMULATIONS - completed\n",
    "                eta = remaining / rate if rate > 0 else 0\n",
    "                pbar_rounds.set_postfix(\n",
    "                    {\n",
    "                        \"sims\": completed,\n",
    "                        \"rate\": f\"{rate:.1f}/s\",\n",
    "                        \"ETA\": f\"{eta / 60:.1f}m\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if len(all_sim_results) == 0:\n",
    "        print(f\"  No valid results\")\n",
    "        return None\n",
    "\n",
    "    # Calculate overall statistics including page movements\n",
    "    avg_mean = np.mean([r[\"mean_delta_pct\"] for r in all_sim_results])\n",
    "    avg_min = np.mean([r[\"min_delta_pct\"] for r in all_sim_results])\n",
    "    avg_max = np.mean([r[\"max_delta_pct\"] for r in all_sim_results])\n",
    "    avg_pages_up = np.mean([r[\"pages_up\"] for r in all_sim_results])\n",
    "    avg_pages_down = np.mean([r[\"pages_down\"] for r in all_sim_results])\n",
    "    avg_pages_neutral = np.mean([r[\"pages_neutral\"] for r in all_sim_results])\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(\n",
    "        f\"  ✓ Completed in {elapsed_total / 60:.1f} minutes \"\n",
    "        f\"({len(all_sim_results) / elapsed_total:.1f} bridgings/sec)\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"filename\": os.path.basename(new_csv_path),\n",
    "        \"num_boosting_rounds\": NUM_BOOSTING_ROUNDS,\n",
    "        \"bridgings_per_round\": BRIDGINGS_PER_ROUND,\n",
    "        \"total_bridgings\": len(all_sim_results),\n",
    "        \"avg_mean_delta_pct\": avg_mean,\n",
    "        \"avg_min_delta_pct\": avg_min,\n",
    "        \"avg_max_delta_pct\": avg_max,\n",
    "        \"avg_pages_up\": avg_pages_up,\n",
    "        \"avg_pages_down\": avg_pages_down,\n",
    "        \"avg_pages_neutral\": avg_pages_neutral,\n",
    "        \"elapsed_time_seconds\": elapsed_total,\n",
    "        \"boosting_round_results\": boosting_round_results,\n",
    "        \"sim_results\": all_sim_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_results(result):\n",
    "    \"\"\"Validate that max >= mean >= min.\"\"\"\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    avg_max = result[\"avg_max_delta_pct\"]\n",
    "    avg_mean = result[\"avg_mean_delta_pct\"]\n",
    "    avg_min = result[\"avg_min_delta_pct\"]\n",
    "\n",
    "    if not (avg_max >= avg_mean >= avg_min):\n",
    "        print(f\"  WARNING: Validation failed for {result['filename']}\")\n",
    "        print(f\"     Max: {avg_max:.2f}%, Mean: {avg_mean:.2f}%, Min: {avg_min:.2f}%\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def process_folder(www_graph, www_nodes, baseline_path, comparison_folder):\n",
    "    \"\"\"Process a single comparison folder.\"\"\"\n",
    "    folder_name = os.path.basename(comparison_folder.rstrip(\"/\"))\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"PROCESSING FOLDER: {folder_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not os.path.exists(comparison_folder):\n",
    "        print(f\"ERROR: Folder not found: {comparison_folder}\")\n",
    "        return None\n",
    "\n",
    "    csv_files = sorted([f for f in os.listdir(comparison_folder) if f.endswith(\".csv\")])\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        print(f\"ERROR: No CSV files found in {comparison_folder}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "    results = []\n",
    "    all_simulation_results = []\n",
    "\n",
    "    for new_csv_filename in csv_files:\n",
    "        new_csv_path = os.path.join(comparison_folder, new_csv_filename)\n",
    "        result = analyze_csv_pair(www_graph, www_nodes, baseline_path, new_csv_path)\n",
    "\n",
    "        if result is not None and validate_results(result):\n",
    "            results.append(result)\n",
    "            all_simulation_results.extend(result[\"sim_results\"])\n",
    "            print(f\"  ✓ Valid results obtained\")\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"No valid results obtained for this folder\")\n",
    "        return None\n",
    "\n",
    "    # Calculate overall statistics\n",
    "    overall_mean = np.mean([r[\"mean_delta_pct\"] for r in all_simulation_results])\n",
    "    overall_min = np.mean([r[\"min_delta_pct\"] for r in all_simulation_results])\n",
    "    overall_max = np.mean([r[\"max_delta_pct\"] for r in all_simulation_results])\n",
    "    total_avg_pages_up = np.mean([r[\"pages_up\"] for r in all_simulation_results])\n",
    "    total_avg_pages_down = np.mean([r[\"pages_down\"] for r in all_simulation_results])\n",
    "    total_avg_pages_neutral = np.mean(\n",
    "        [r[\"pages_neutral\"] for r in all_simulation_results]\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(f\"FOLDER SUMMARY: {folder_name}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Files analyzed: {len(results)}/{len(csv_files)}\")\n",
    "    print(f\"Total boosting rounds per file: {NUM_BOOSTING_ROUNDS}\")\n",
    "    print(f\"Bridgings per round: {BRIDGINGS_PER_ROUND}\")\n",
    "    print(f\"Total simulations: {len(all_simulation_results)}\")\n",
    "    print(f\"Overall Average Mean Delta: {overall_mean:>10.2f}%\")\n",
    "    print(f\"Overall Average Min Delta:  {overall_min:>10.2f}%\")\n",
    "    print(f\"Overall Average Max Delta:  {overall_max:>10.2f}%\")\n",
    "    print(f\"Average Pages Up:           {total_avg_pages_up:>10.1f}\")\n",
    "    print(f\"Average Pages Down:         {total_avg_pages_down:>10.1f}\")\n",
    "    print(f\"Average Pages Neutral:      {total_avg_pages_neutral:>10.1f}\")\n",
    "\n",
    "    return {\n",
    "        \"folder_name\": folder_name,\n",
    "        \"folder_path\": comparison_folder,\n",
    "        \"files_analyzed\": len(results),\n",
    "        \"total_files\": len(csv_files),\n",
    "        \"num_boosting_rounds\": NUM_BOOSTING_ROUNDS,\n",
    "        \"bridgings_per_round\": BRIDGINGS_PER_ROUND,\n",
    "        \"total_simulations\": len(all_simulation_results),\n",
    "        \"overall_mean_delta_pct\": overall_mean,\n",
    "        \"overall_min_delta_pct\": overall_min,\n",
    "        \"overall_max_delta_pct\": overall_max,\n",
    "        \"total_avg_pages_up\": total_avg_pages_up,\n",
    "        \"total_avg_pages_down\": total_avg_pages_down,\n",
    "        \"total_avg_pages_neutral\": total_avg_pages_neutral,\n",
    "        \"std_mean\": np.std([r[\"mean_delta_pct\"] for r in all_simulation_results]),\n",
    "        \"median_mean\": np.median([r[\"mean_delta_pct\"] for r in all_simulation_results]),\n",
    "        \"file_results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MULTI-FOLDER PAGERANK ANALYSIS WITH PAGE MOVEMENT TRACKING\")\n",
    "    print(\n",
    "        f\"Configuration: {NUM_BOOSTING_ROUNDS} boosting rounds × {BRIDGINGS_PER_ROUND} bridgings\"\n",
    "    )\n",
    "    print(f\"Total simulations per comparison: {TOTAL_SIMULATIONS}\")\n",
    "    print(f\"Neutral threshold: ±{NEUTRAL_THRESHOLD}%\")\n",
    "    if USE_PARALLEL:\n",
    "        print(f\"Parallel processing: ENABLED ({NUM_WORKERS} workers)\")\n",
    "    else:\n",
    "        print(f\"Parallel processing: DISABLED (sequential mode)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not os.path.exists(BASELINE_PATH):\n",
    "        print(f\"\\nERROR: Baseline file not found: {BASELINE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(FINEWEB_WWW_PATH):\n",
    "        print(f\"\\nERROR: FineWeb WWW file not found: {FINEWEB_WWW_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\nWWW Graph Source: {os.path.basename(FINEWEB_WWW_PATH)}\")\n",
    "    print(f\"Baseline: {os.path.basename(BASELINE_PATH)}\")\n",
    "    print(f\"Number of folders to process: {len(COMPARISON_FOLDERS)}\")\n",
    "\n",
    "    www_graph, www_nodes = load_www_graph_networkit(FINEWEB_WWW_PATH)\n",
    "\n",
    "    folder_summaries = []\n",
    "\n",
    "    for folder_path in COMPARISON_FOLDERS:\n",
    "        folder_summary = process_folder(\n",
    "            www_graph, www_nodes, BASELINE_PATH, folder_path\n",
    "        )\n",
    "        if folder_summary is not None:\n",
    "            folder_summaries.append(folder_summary)\n",
    "\n",
    "    # Print final summary table\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL SUMMARY - ALL STRATEGIES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if len(folder_summaries) == 0:\n",
    "        print(\"\\nNo valid results obtained from any folder\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\n{'Strategy':<40} {'Mean %':<10} {'Up':<8} {'Down':<8} {'Neutral':<10}\"\n",
    "        )\n",
    "        print(\"-\" * 90)\n",
    "\n",
    "        for summary in folder_summaries:\n",
    "            print(\n",
    "                f\"{summary['folder_name']:<40} \"\n",
    "                f\"{summary['overall_mean_delta_pct']:>8.2f}% \"\n",
    "                f\"{summary['total_avg_pages_up']:>6.1f} \"\n",
    "                f\"{summary['total_avg_pages_down']:>6.1f} \"\n",
    "                f\"{summary['total_avg_pages_neutral']:>8.1f}\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 90)\n",
    "        print(\n",
    "            f\"\\nTotal strategies analyzed: {len(folder_summaries)}/{len(COMPARISON_FOLDERS)}\"\n",
    "        )\n",
    "\n",
    "        # Save strategy summary CSV\n",
    "        summary_output_path = os.path.join(OUTPUT_DIR, STRATEGY_SUMMARY_FILE)\n",
    "        summary_df = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"strategy\": s[\"folder_name\"],\n",
    "                    \"mean_delta_pct\": s[\"overall_mean_delta_pct\"],\n",
    "                    \"min_delta_pct\": s[\"overall_min_delta_pct\"],\n",
    "                    \"max_delta_pct\": s[\"overall_max_delta_pct\"],\n",
    "                    \"total_avg_pages_up\": s[\"total_avg_pages_up\"],\n",
    "                    \"total_avg_pages_down\": s[\"total_avg_pages_down\"],\n",
    "                    \"total_avg_pages_neutral\": s[\"total_avg_pages_neutral\"],\n",
    "                    \"files_analyzed\": s[\"files_analyzed\"],\n",
    "                    \"num_boosting_rounds\": s[\"num_boosting_rounds\"],\n",
    "                    \"bridgings_per_round\": s[\"bridgings_per_round\"],\n",
    "                    \"total_simulations\": s[\"total_simulations\"],\n",
    "                    \"std_mean\": s[\"std_mean\"],\n",
    "                    \"median_mean\": s[\"median_mean\"],\n",
    "                }\n",
    "                for s in folder_summaries\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        summary_df.to_csv(summary_output_path, index=False)\n",
    "        print(f\"\\n✓ Strategy summary saved to: {summary_output_path}\")\n",
    "\n",
    "        # Save detailed per-file results CSV\n",
    "        detailed_records = []\n",
    "        for folder_summary in folder_summaries:\n",
    "            for file_result in folder_summary[\"file_results\"]:\n",
    "                detailed_records.append(\n",
    "                    {\n",
    "                        \"strategy\": folder_summary[\"folder_name\"],\n",
    "                        \"filename\": file_result[\"filename\"],\n",
    "                        \"total_bridgings\": file_result[\"total_bridgings\"],\n",
    "                        \"avg_mean_delta_pct\": file_result[\"avg_mean_delta_pct\"],\n",
    "                        \"avg_min_delta_pct\": file_result[\"avg_min_delta_pct\"],\n",
    "                        \"avg_max_delta_pct\": file_result[\"avg_max_delta_pct\"],\n",
    "                        \"avg_pages_up\": file_result[\"avg_pages_up\"],\n",
    "                        \"avg_pages_down\": file_result[\"avg_pages_down\"],\n",
    "                        \"avg_pages_neutral\": file_result[\"avg_pages_neutral\"],\n",
    "                        \"elapsed_time_seconds\": file_result[\"elapsed_time_seconds\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if detailed_records:\n",
    "            detailed_output_path = os.path.join(OUTPUT_DIR, DETAILED_RESULTS_FILE)\n",
    "            detailed_df = pd.DataFrame(detailed_records)\n",
    "            detailed_df.to_csv(detailed_output_path, index=False)\n",
    "            print(f\"✓ Detailed results saved to: {detailed_output_path}\")\n",
    "\n",
    "    # Cleanup\n",
    "    _www_graph_cache = None\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Analysis complete!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}