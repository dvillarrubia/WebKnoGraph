{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4ot8mq0U5XN"
   },
   "outputs": [],
   "source": [
    "# Google Drive Folder-Level PageRank Analysis with REAL WWW Data\n",
    "# Uses FineWeb dataset CSV with FROM and TO columns as WWW graph\n",
    "# Calculates overall averages across all files in the strategy\n",
    "\n",
    "# === INSTALLATION CELL (Run first) ===\n",
    "!pip install networkit pandas numpy\n",
    "\n",
    "# === MOUNT GOOGLE DRIVE ===\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# === MAIN CODE ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkit as nk\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# USER CONFIGURATION\n",
    "# ============================================\n",
    "BASELINE_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/link_graph_edges.csv\"\n",
    "COMPARISON_FOLDER = (\n",
    "    \"/content/drive/MyDrive/WebKnoGraph/results/automatic_led/high_batches/\"\n",
    ")\n",
    "\n",
    "# NEW: Path to FineWeb WWW graph CSV\n",
    "FINEWEB_WWW_PATH = \"/content/drive/MyDrive/WebKnoGraph/results/fineweb_500k_pages.csv\"\n",
    "\n",
    "NUM_SIMULATIONS = 10\n",
    "\n",
    "# Simulation Parameters\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "PAGERANK_TOLERANCE = 1e-6\n",
    "\n",
    "# ============================================\n",
    "# CORE FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_path, graph_name=\"graph\"):\n",
    "    \"\"\"Load graph from CSV file.\"\"\"\n",
    "    try:\n",
    "        print(f\"  Loading {graph_name} from {os.path.basename(file_path)}...\")\n",
    "        df = pd.read_csv(file_path, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "        if len(df) == 0:\n",
    "            print(f\"  ERROR: No valid edges found in {file_path}\")\n",
    "            return None, None, None\n",
    "\n",
    "        from_urls = df[\"FROM\"].values\n",
    "        to_urls = df[\"TO\"].values\n",
    "        all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "        url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "        g = nk.Graph(n=len(all_urls), weighted=False, directed=True)\n",
    "        for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "            g.addEdge(url_to_idx[src_url], url_to_idx[tgt_url])\n",
    "\n",
    "        print(f\"    Loaded: {len(all_urls):,} nodes, {len(df):,} edges\")\n",
    "        return g, all_urls, url_to_idx\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading {file_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def load_www_graph_networkit(www_csv_path):\n",
    "    \"\"\"Load REAL WWW graph from FineWeb dataset with caching.\"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == www_csv_path:\n",
    "        print(\"  Using cached WWW graph\")\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph, _www_graph_cache[2]\n",
    "\n",
    "    print(\"\\nLoading REAL WWW graph from FineWeb dataset...\")\n",
    "    www_graph, www_nodes, www_url_mapping = load_graph_from_csv_networkit(\n",
    "        www_csv_path, graph_name=\"WWW graph\"\n",
    "    )\n",
    "\n",
    "    if www_graph is None:\n",
    "        raise ValueError(f\"Failed to load WWW graph from {www_csv_path}\")\n",
    "\n",
    "    # Cache the graph\n",
    "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (www_csv_path, cached_graph, www_nodes)\n",
    "\n",
    "    print(f\"  WWW graph cached successfully\")\n",
    "    return www_graph, www_nodes\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph, www_nodes, kalicube_edges, kalicube_nodes, kalicube_url_mapping\n",
    "):\n",
    "    \"\"\"Process configuration and calculate PageRank.\"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "    n_www = www_graph.numberOfNodes()\n",
    "\n",
    "    # Create merged graph\n",
    "    merged_graph = nk.Graph(n=n_www, weighted=False, directed=True)\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    # Add Kalicube nodes\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    # Add Kalicube internal edges\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    # Connect WWW to Kalicube\n",
    "    n_www_sample = min(MAX_CONNECTIONS, n_www)\n",
    "    n_kalicube_sample = min(MAX_CONNECTIONS, n_kalicube)\n",
    "\n",
    "    www_nodes_sample = np.random.choice(n_www, size=n_www_sample, replace=False)\n",
    "    kalicube_indices = np.random.choice(\n",
    "        n_kalicube, size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
    "\n",
    "    # Calculate PageRank\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    # Extract Kalicube PageRank scores\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def run_single_simulation_networkit(\n",
    "    simulation_id,\n",
    "    www_graph,\n",
    "    www_nodes,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"Run single simulation.\"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    # Calculate PageRank for old configuration\n",
    "    pagerank_old_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_old_edges,\n",
    "        kalicube_nodes_old,\n",
    "        kalicube_url_mapping_old,\n",
    "    )\n",
    "\n",
    "    # Calculate PageRank for new configuration\n",
    "    pagerank_new_dict = process_configuration_networkit(\n",
    "        www_graph,\n",
    "        www_nodes,\n",
    "        kalicube_new_edges,\n",
    "        kalicube_nodes_new,\n",
    "        kalicube_url_mapping_new,\n",
    "    )\n",
    "\n",
    "    # Compare results\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return None\n",
    "\n",
    "    deltas_pct = []\n",
    "    for url in common_urls:\n",
    "        before = pagerank_old_dict[url]\n",
    "        after = pagerank_new_dict[url]\n",
    "        if before > 0:\n",
    "            delta_pct = ((after - before) / before) * 100\n",
    "            deltas_pct.append(delta_pct)\n",
    "\n",
    "    if len(deltas_pct) == 0:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"mean_delta_pct\": np.mean(deltas_pct),\n",
    "        \"min_delta_pct\": np.min(deltas_pct),\n",
    "        \"max_delta_pct\": np.max(deltas_pct),\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_csv_pair(www_graph, www_nodes, old_csv_path, new_csv_path):\n",
    "    \"\"\"Analyze a pair of CSV files.\"\"\"\n",
    "    print(f\"\\nAnalyzing: {os.path.basename(new_csv_path)}\")\n",
    "\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_networkit(old_csv_path, \"baseline Kalicube\")\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        print(f\"  Failed to load old graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_networkit(new_csv_path, \"comparison Kalicube\")\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        print(f\"  Failed to load new graph\")\n",
    "        return None\n",
    "\n",
    "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
    "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
    "\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"  Running {NUM_SIMULATIONS} simulations...\")\n",
    "    sim_results = []\n",
    "\n",
    "    for sim_id in range(NUM_SIMULATIONS):\n",
    "        result = run_single_simulation_networkit(\n",
    "            sim_id,\n",
    "            www_graph,\n",
    "            www_nodes,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "        if result is not None:\n",
    "            sim_results.append(result)\n",
    "\n",
    "        if (sim_id + 1) % 20 == 0:\n",
    "            print(f\"    Progress: {sim_id + 1}/{NUM_SIMULATIONS}\")\n",
    "\n",
    "    if len(sim_results) == 0:\n",
    "        print(f\"  No valid results\")\n",
    "        return None\n",
    "\n",
    "    avg_mean = np.mean([r[\"mean_delta_pct\"] for r in sim_results])\n",
    "    avg_min = np.mean([r[\"min_delta_pct\"] for r in sim_results])\n",
    "    avg_max = np.mean([r[\"max_delta_pct\"] for r in sim_results])\n",
    "\n",
    "    return {\n",
    "        \"filename\": os.path.basename(new_csv_path),\n",
    "        \"avg_mean_delta_pct\": avg_mean,\n",
    "        \"avg_min_delta_pct\": avg_min,\n",
    "        \"avg_max_delta_pct\": avg_max,\n",
    "        \"sim_results\": sim_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_results(result):\n",
    "    \"\"\"Validate that max >= mean >= min.\"\"\"\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    avg_max = result[\"avg_max_delta_pct\"]\n",
    "    avg_mean = result[\"avg_mean_delta_pct\"]\n",
    "    avg_min = result[\"avg_min_delta_pct\"]\n",
    "\n",
    "    if not (avg_max >= avg_mean >= avg_min):\n",
    "        print(f\"  WARNING: Validation failed for {result['filename']}\")\n",
    "        print(f\"     Max: {avg_max:.2f}%, Mean: {avg_mean:.2f}%, Min: {avg_min:.2f}%\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PAGERANK ANALYSIS WITH REAL WWW DATA (FineWeb)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Validate paths\n",
    "    if not os.path.exists(BASELINE_PATH):\n",
    "        print(f\"\\nERROR: Baseline file not found: {BASELINE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(FINEWEB_WWW_PATH):\n",
    "        print(f\"\\nERROR: FineWeb WWW file not found: {FINEWEB_WWW_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    if not os.path.exists(COMPARISON_FOLDER):\n",
    "        print(f\"\\nERROR: Comparison folder not found: {COMPARISON_FOLDER}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\nWWW Graph Source: {os.path.basename(FINEWEB_WWW_PATH)}\")\n",
    "    print(f\"Baseline: {os.path.basename(BASELINE_PATH)}\")\n",
    "\n",
    "    # Load REAL WWW graph (only once, then cached)\n",
    "    www_graph, www_nodes = load_www_graph_networkit(FINEWEB_WWW_PATH)\n",
    "\n",
    "    # Find comparison files\n",
    "    csv_files = sorted([f for f in os.listdir(COMPARISON_FOLDER) if f.endswith(\".csv\")])\n",
    "\n",
    "    if len(csv_files) == 0:\n",
    "        print(f\"\\nERROR: No CSV files found in {COMPARISON_FOLDER}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\"\\nFound {len(csv_files)} CSV files in comparison folder\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "    all_simulation_results = []\n",
    "\n",
    "    for new_csv_filename in csv_files:\n",
    "        new_csv_path = os.path.join(COMPARISON_FOLDER, new_csv_filename)\n",
    "        result = analyze_csv_pair(www_graph, www_nodes, BASELINE_PATH, new_csv_path)\n",
    "\n",
    "        if result is not None and validate_results(result):\n",
    "            results.append(result)\n",
    "            all_simulation_results.extend(result[\"sim_results\"])\n",
    "            print(f\"  âœ“ Valid results obtained\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INDIVIDUAL FILE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"\\nNo valid results obtained\")\n",
    "    else:\n",
    "        print(f\"\\n{'File':<50} {'Avg Mean':<12} {'Avg Min':<12} {'Avg Max':<12}\")\n",
    "        print(\"-\" * 90)\n",
    "\n",
    "        for result in results:\n",
    "            print(\n",
    "                f\"{result['filename']:<50} \"\n",
    "                f\"{result['avg_mean_delta_pct']:>10.2f}% \"\n",
    "                f\"{result['avg_min_delta_pct']:>10.2f}% \"\n",
    "                f\"{result['avg_max_delta_pct']:>10.2f}%\"\n",
    "            )\n",
    "\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"\\nSuccessfully analyzed {len(results)}/{len(csv_files)} files\")\n",
    "\n",
    "    # Calculate overall averages\n",
    "    if len(all_simulation_results) > 0:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"OVERALL AVERAGES\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        overall_mean = np.mean([r[\"mean_delta_pct\"] for r in all_simulation_results])\n",
    "        overall_min = np.mean([r[\"min_delta_pct\"] for r in all_simulation_results])\n",
    "        overall_max = np.mean([r[\"max_delta_pct\"] for r in all_simulation_results])\n",
    "\n",
    "        print(f\"\\nTotal simulations across all files: {len(all_simulation_results)}\")\n",
    "        print(f\"Total files analyzed: {len(results)}\")\n",
    "        print(f\"\\nOverall Average Mean Delta: {overall_mean:>10.2f}%\")\n",
    "        print(f\"Overall Average Min Delta:  {overall_min:>10.2f}%\")\n",
    "        print(f\"Overall Average Max Delta:  {overall_max:>10.2f}%\")\n",
    "\n",
    "        # Additional statistics\n",
    "        all_means = [r[\"mean_delta_pct\"] for r in all_simulation_results]\n",
    "        print(f\"\\nStandard Deviation (Mean): {np.std(all_means):>10.2f}%\")\n",
    "        print(f\"Median (Mean):             {np.median(all_means):>10.2f}%\")\n",
    "\n",
    "    _www_graph_cache = None\n",
    "    gc.collect()\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Analysis complete!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}