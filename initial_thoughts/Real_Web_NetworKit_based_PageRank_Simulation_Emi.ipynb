{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4ot8mq0U5XN"
   },
   "outputs": [],
   "source": [
    "# NetworKit-based PageRank Simulation - Real Web Network Version\n",
    "# This uses your 500K real web page network instead of synthetic data\n",
    "\n",
    "# Installation (run first)\n",
    "# !pip install networkit pandas numpy matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import networkit as nk\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- User Input for Graph Files ---\n",
    "old_graph_filename = \"link_graph_edges.csv\"\n",
    "new_graph_filename = \"240_best_updated_link_graph_1.csv\"\n",
    "\n",
    "# REAL WEB NETWORK FILE - Your downloaded FineWeb network\n",
    "real_web_network_file = \"fineweb_500k_pages.csv\"\n",
    "\n",
    "# Number of simulations to run\n",
    "NUM_SIMULATIONS = 100\n",
    "\n",
    "# Connection range for WWW-Kalicube interconnections\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "\n",
    "# Simulation parameters\n",
    "TOTAL_NODES_WWW = None  # Will be set from real network\n",
    "EDGES_PER_NEW_NODE = 2  # Not used with real network\n",
    "PAGERANK_TOLERANCE = 1e-6\n",
    "\n",
    "# Global WWW graph cache\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def load_real_web_network(network_file, sample_size=100000):\n",
    "    \"\"\"\n",
    "    Load REAL web network from your downloaded FineWeb data\n",
    "    Replaces synthetic Barabasi-Albert network\n",
    "    \"\"\"\n",
    "    print(f\"Loading REAL web network from: {network_file}\")\n",
    "\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(network_file):\n",
    "            print(f\"Error: Real web network file not found: {network_file}\")\n",
    "            print(\"Available CSV files:\")\n",
    "            csv_files = [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
    "            for f in csv_files:\n",
    "                print(f\"   - {f}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Load the real web links\n",
    "        print(\"Loading page-to-page links...\")\n",
    "        df = pd.read_csv(network_file, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "        print(f\"Loaded {len(df):,} real web page links\")\n",
    "\n",
    "        # Sample if too large for memory\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"Sampled down to {sample_size:,} links for simulation\")\n",
    "\n",
    "        # Create URL to node mapping\n",
    "        from_pages = df[\"FROM\"].values\n",
    "        to_pages = df[\"TO\"].values\n",
    "\n",
    "        # Get unique pages\n",
    "        all_pages = np.unique(np.concatenate([from_pages, to_pages]))\n",
    "        page_to_idx = {page: i for i, page in enumerate(all_pages)}\n",
    "\n",
    "        # Create NetworKit graph\n",
    "        n_nodes = len(all_pages)\n",
    "        g = nk.Graph(n=n_nodes, weighted=False, directed=True)\n",
    "\n",
    "        # Add edges efficiently\n",
    "        for src_page, tgt_page in zip(from_pages, to_pages):\n",
    "            src_idx = page_to_idx[src_page]\n",
    "            tgt_idx = page_to_idx[tgt_page]\n",
    "            g.addEdge(src_idx, tgt_idx)\n",
    "\n",
    "        print(f\"Created REAL web network:\")\n",
    "        print(f\"   Nodes (web pages): {n_nodes:,}\")\n",
    "        print(f\"   Edges (hyperlinks): {g.numberOfEdges():,}\")\n",
    "        print(f\"   Network type: REAL web pages\")\n",
    "\n",
    "        return g, all_pages, page_to_idx\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading real web network: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def create_www_graph_networkit_real(n_nodes=None, m_edges=None, seed=42):\n",
    "    \"\"\"\n",
    "    Create WWW graph using REAL web network data instead of synthetic\n",
    "    FIXED: Proper caching to avoid reloading network every simulation\n",
    "    \"\"\"\n",
    "    global _www_graph_cache\n",
    "    global TOTAL_NODES_WWW\n",
    "\n",
    "    # FIXED: Use consistent cache key (ignore seed for real network)\n",
    "    cache_key = \"real_web_network\"\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
    "        # Silent cache usage - no print statement\n",
    "        cached_graph = _www_graph_cache[1]\n",
    "        new_graph = nk.Graph(\n",
    "            n=cached_graph.numberOfNodes(), weighted=False, directed=True\n",
    "        )\n",
    "        for u, v in cached_graph.iterEdges():\n",
    "            new_graph.addEdge(u, v)\n",
    "        return new_graph\n",
    "\n",
    "    # Load real web network ONLY ONCE\n",
    "    print(\"Loading real web network (ONCE)...\")\n",
    "    real_graph, pages, page_mapping = load_real_web_network(\n",
    "        real_web_network_file, sample_size=100000\n",
    "    )\n",
    "\n",
    "    if real_graph is None:\n",
    "        print(\"Falling back to synthetic network...\")\n",
    "        nk.setSeed(seed, False)\n",
    "        generator = nk.generators.BarabasiAlbertGenerator(k=2, nMax=100000, n0=2)\n",
    "        synthetic_graph = generator.generate()\n",
    "        TOTAL_NODES_WWW = synthetic_graph.numberOfNodes()\n",
    "        return synthetic_graph\n",
    "\n",
    "    # Update global parameter\n",
    "    TOTAL_NODES_WWW = real_graph.numberOfNodes()\n",
    "    print(f\"Set TOTAL_NODES_WWW to {TOTAL_NODES_WWW:,} nodes\")\n",
    "\n",
    "    # Cache the result with consistent key\n",
    "    cached_graph = nk.Graph(n=real_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    for u, v in real_graph.iterEdges():\n",
    "        cached_graph.addEdge(u, v)\n",
    "    _www_graph_cache = (cache_key, cached_graph)\n",
    "    print(\"Network cached for future simulations\")\n",
    "\n",
    "    return real_graph\n",
    "\n",
    "\n",
    "def load_graph_from_csv_networkit(file_name):\n",
    "    \"\"\"\n",
    "    Ultra-fast graph loading with NetworKit.\n",
    "    NetworKit is significantly faster than graph-tool for large networks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read only what we need\n",
    "        df = pd.read_csv(file_name, usecols=[\"FROM\", \"TO\"])\n",
    "        df = df.dropna()\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_name} not found. Please ensure the file exists.\")\n",
    "        return None, None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Required column {e} not found in {file_name}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Extract URLs and create mapping\n",
    "    from_urls = df[\"FROM\"].values\n",
    "    to_urls = df[\"TO\"].values\n",
    "\n",
    "    if len(from_urls) == 0 or len(to_urls) == 0:\n",
    "        print(f\"Error: No valid edges found in {file_name}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Get unique URLs\n",
    "    all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "    url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "    # Create NetworKit graph\n",
    "    n_nodes = len(all_urls)\n",
    "    g = nk.Graph(n=n_nodes, weighted=False, directed=True)\n",
    "\n",
    "    # Add edges efficiently\n",
    "    for src_url, tgt_url in zip(from_urls, to_urls):\n",
    "        src_idx = url_to_idx[src_url]\n",
    "        tgt_idx = url_to_idx[tgt_url]\n",
    "        g.addEdge(src_idx, tgt_idx)\n",
    "\n",
    "    return g, all_urls, url_to_idx\n",
    "\n",
    "\n",
    "def process_configuration_networkit(\n",
    "    www_graph, kalicube_edges, kalicube_nodes, kalicube_url_mapping\n",
    "):\n",
    "    \"\"\"\n",
    "    Process configuration using NetworKit's high-performance algorithms.\n",
    "    \"\"\"\n",
    "    kalicube_offset = www_graph.numberOfNodes()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "\n",
    "    # Create merged graph\n",
    "    merged_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
    "    # Copy all edges from www_graph\n",
    "    for u, v in www_graph.iterEdges():\n",
    "        merged_graph.addEdge(u, v)\n",
    "\n",
    "    # Add Kalicube nodes\n",
    "    for _ in range(n_kalicube):\n",
    "        merged_graph.addNode()\n",
    "\n",
    "    # Add Kalicube edges\n",
    "    if kalicube_edges:\n",
    "        for src, tgt in kalicube_edges:\n",
    "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
    "\n",
    "    # Add interconnections between WWW and Kalicube\n",
    "    n_www_sample = min(MIN_CONNECTIONS, TOTAL_NODES_WWW if TOTAL_NODES_WWW else 100000)\n",
    "    n_kalicube_sample = min(MIN_CONNECTIONS, len(kalicube_nodes))\n",
    "\n",
    "    # Sample nodes for interconnection\n",
    "    www_nodes_sample = np.random.choice(\n",
    "        TOTAL_NODES_WWW if TOTAL_NODES_WWW else 100000, size=n_www_sample, replace=False\n",
    "    )\n",
    "    kalicube_indices = np.random.choice(\n",
    "        len(kalicube_nodes), size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    # Add interconnection edges\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
    "\n",
    "    # Run PageRank using NetworKit's optimized implementation\n",
    "    pagerank_algo = nk.centrality.PageRank(\n",
    "        merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE\n",
    "    )\n",
    "    pagerank_algo.run()\n",
    "    pagerank_scores = pagerank_algo.scores()\n",
    "\n",
    "    # Extract results for Kalicube nodes\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def create_comparison_dataframe_networkit(\n",
    "    pagerank_old_dict, pagerank_new_dict, simulation\n",
    "):\n",
    "    \"\"\"\n",
    "    Create comparison dataframe with optimized numpy operations.\n",
    "    \"\"\"\n",
    "    # Find common URLs\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to arrays for fast processing\n",
    "    urls = list(common_urls)\n",
    "    n_urls = len(urls)\n",
    "\n",
    "    pagerank_before = np.array([pagerank_old_dict[url] for url in urls])\n",
    "    pagerank_after = np.array([pagerank_new_dict[url] for url in urls])\n",
    "\n",
    "    # Fast ranking using argsort\n",
    "    rank_before = np.empty(n_urls)\n",
    "    rank_after = np.empty(n_urls)\n",
    "\n",
    "    rank_before[np.argsort(-pagerank_before)] = np.arange(1, n_urls + 1)\n",
    "    rank_after[np.argsort(-pagerank_after)] = np.arange(1, n_urls + 1)\n",
    "\n",
    "    # Calculate changes\n",
    "    pagerank_delta = pagerank_after - pagerank_before\n",
    "    pagerank_delta_pct = (pagerank_delta / np.maximum(pagerank_before, 1e-10)) * 100\n",
    "    rank_change = rank_after - rank_before\n",
    "    rank_change_pct = (rank_change / np.maximum(rank_before, 1e-10)) * 100\n",
    "\n",
    "    # Create DataFrame\n",
    "    comparison_df = pd.DataFrame(\n",
    "        {\n",
    "            \"URL\": urls,\n",
    "            \"PageRank_Before\": pagerank_before,\n",
    "            \"PageRank_After\": pagerank_after,\n",
    "            \"Rank_Before\": rank_before,\n",
    "            \"Rank_After\": rank_after,\n",
    "            \"PageRank_Delta\": pagerank_delta,\n",
    "            \"PageRank_Delta_%\": pagerank_delta_pct,\n",
    "            \"Rank_Change\": rank_change,\n",
    "            \"Rank_Change_%\": rank_change_pct,\n",
    "            \"Simulation\": simulation + 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def run_single_simulation_networkit(\n",
    "    simulation_id,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run single simulation with NetworKit's high-performance algorithms.\n",
    "    Modified to use real web network with FIXED caching.\n",
    "    \"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    # Create REAL WWW graph instead of synthetic (now properly cached)\n",
    "    www_graph = create_www_graph_networkit_real(seed=sim_seed)\n",
    "\n",
    "    # Process both configurations\n",
    "    pagerank_old_dict = process_configuration_networkit(\n",
    "        www_graph, kalicube_old_edges, kalicube_nodes_old, kalicube_url_mapping_old\n",
    "    )\n",
    "\n",
    "    pagerank_new_dict = process_configuration_networkit(\n",
    "        www_graph, kalicube_new_edges, kalicube_nodes_new, kalicube_url_mapping_new\n",
    "    )\n",
    "\n",
    "    # Create comparison\n",
    "    comparison_df = create_comparison_dataframe_networkit(\n",
    "        pagerank_old_dict, pagerank_new_dict, simulation_id\n",
    "    )\n",
    "\n",
    "    if comparison_df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Calculate summary metrics\n",
    "    total_before = comparison_df[\"PageRank_Before\"].sum()\n",
    "    total_after = comparison_df[\"PageRank_After\"].sum()\n",
    "    total_delta = total_after - total_before\n",
    "    delta_pct = (total_delta / total_before) * 100 if total_before > 0 else 0\n",
    "\n",
    "    rank_changes = comparison_df[\"Rank_Change\"].values\n",
    "    rank_improvements = np.sum(rank_changes < 0)\n",
    "    rank_drops = np.sum(rank_changes > 0)\n",
    "    rank_unchanged = np.sum(rank_changes == 0)\n",
    "    avg_rank_change = np.mean(rank_changes)\n",
    "\n",
    "    result = {\n",
    "        \"Simulation\": simulation_id + 1,\n",
    "        \"Total_Before\": total_before,\n",
    "        \"Total_After\": total_after,\n",
    "        \"Total_Delta\": total_delta,\n",
    "        \"Delta_Percent\": delta_pct,\n",
    "        \"Rank_Improvements\": rank_improvements,\n",
    "        \"Rank_Drops\": rank_drops,\n",
    "        \"Rank_Unchanged\": rank_unchanged,\n",
    "        \"Avg_Rank_Change\": avg_rank_change,\n",
    "    }\n",
    "\n",
    "    return result, comparison_df\n",
    "\n",
    "\n",
    "def run_batch_simulations_networkit(\n",
    "    start_idx,\n",
    "    end_idx,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run batch of simulations with NetworKit optimization.\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    batch_comparisons = []\n",
    "\n",
    "    for sim_id in range(start_idx, end_idx):\n",
    "        result, comparison_df = run_single_simulation_networkit(\n",
    "            sim_id,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "            batch_comparisons.append(comparison_df)\n",
    "\n",
    "        # Progress indicator\n",
    "        if (sim_id - start_idx + 1) % 5 == 0:\n",
    "            print(f\"    Completed {sim_id - start_idx + 1} simulations in batch\")\n",
    "\n",
    "    return batch_results, batch_comparisons\n",
    "\n",
    "\n",
    "def check_available_files():\n",
    "    \"\"\"Helper function to check available CSV files\"\"\"\n",
    "    csv_files = [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
    "    print(\"Available CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {f}\")\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "def plot_results_networkit(results_df):\n",
    "    \"\"\"\n",
    "    Create visualization of results using matplotlib/seaborn.\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Distribution of delta percentages\n",
    "    ax1.hist(results_df[\"Delta_Percent\"], bins=20, alpha=0.7, color=\"skyblue\")\n",
    "    ax1.set_xlabel(\"Delta Percentage (%)\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_title(\"Distribution of PageRank Changes\")\n",
    "    ax1.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "    # Rank improvements vs drops\n",
    "    improvements = results_df[\"Rank_Improvements\"].mean()\n",
    "    drops = results_df[\"Rank_Drops\"].mean()\n",
    "    unchanged = results_df[\"Rank_Unchanged\"].mean()\n",
    "\n",
    "    ax2.bar(\n",
    "        [\"Improvements\", \"Drops\", \"Unchanged\"],\n",
    "        [improvements, drops, unchanged],\n",
    "        color=[\"green\", \"red\", \"gray\"],\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax2.set_ylabel(\"Average Count\")\n",
    "    ax2.set_title(\"Average Rank Changes per Simulation\")\n",
    "\n",
    "    # Time series of delta percentages\n",
    "    ax3.plot(results_df[\"Simulation\"], results_df[\"Delta_Percent\"], \"o-\", alpha=0.7)\n",
    "    ax3.set_xlabel(\"Simulation Number\")\n",
    "    ax3.set_ylabel(\"Delta Percentage (%)\")\n",
    "    ax3.set_title(\"PageRank Changes Over Simulations\")\n",
    "    ax3.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Box plot of key metrics\n",
    "    metrics_data = [results_df[\"Delta_Percent\"], results_df[\"Avg_Rank_Change\"]]\n",
    "    ax4.boxplot(metrics_data, labels=[\"Delta %\", \"Avg Rank Change\"])\n",
    "    ax4.set_title(\"Distribution of Key Metrics\")\n",
    "    ax4.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"networkit_real_web_simulation_results.png\", dpi=300, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting NetworKit PageRank simulation with REAL WEB NETWORK!\")\n",
    "    print(f\"NetworKit version: {nk.__version__}\")\n",
    "\n",
    "    # Check available files\n",
    "    print(\"\\nChecking available files...\")\n",
    "    available_files = check_available_files()\n",
    "\n",
    "    # Validate Kalicube files\n",
    "    if new_graph_filename not in available_files:\n",
    "        print(f\"\\nWarning: {new_graph_filename} not found!\")\n",
    "        print(\"Available options:\")\n",
    "        for f in available_files:\n",
    "            print(f\"  - {f}\")\n",
    "        print(\n",
    "            \"\\nPlease update the 'new_graph_filename' variable or ensure the file exists.\"\n",
    "        )\n",
    "        exit(1)\n",
    "\n",
    "    # Check for real web network file\n",
    "    if real_web_network_file not in available_files:\n",
    "        print(f\"\\nReal web network file not found: {real_web_network_file}\")\n",
    "        print(\"Looking for FineWeb network files...\")\n",
    "        fineweb_files = [\n",
    "            f\n",
    "            for f in available_files\n",
    "            if \"fineweb\" in f.lower() and \"pages\" in f.lower()\n",
    "        ]\n",
    "        if fineweb_files:\n",
    "            real_web_network_file = fineweb_files[0]\n",
    "            print(f\"Found: {real_web_network_file}\")\n",
    "        else:\n",
    "            print(\n",
    "                \"No FineWeb network files found. Using synthetic network as fallback.\"\n",
    "            )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\nLoading graphs with NetworKit optimization...\")\n",
    "\n",
    "    # Load old Kalicube graph\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_networkit(old_graph_filename)\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        print(f\"Failed to load old graph from {old_graph_filename}. Exiting.\")\n",
    "        exit(1)\n",
    "    print(f\"Loaded OLD Kalicube graph from: {old_graph_filename}\")\n",
    "\n",
    "    # Load new Kalicube graph\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_networkit(new_graph_filename)\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        print(f\"Failed to load new graph from {new_graph_filename}. Exiting.\")\n",
    "        exit(1)\n",
    "    print(f\"Loaded NEW Kalicube graph from: {new_graph_filename}\")\n",
    "\n",
    "    print(\"Pre-processing graph data...\")\n",
    "    # Convert NetworKit edges to list format\n",
    "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
    "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
    "\n",
    "    print(\"\\nNetwork Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    num_nodes_old = kalicube_graph_old.numberOfNodes()\n",
    "    num_edges_old = kalicube_graph_old.numberOfEdges()\n",
    "    print(f\"OLD Kalicube Graph: {num_nodes_old:,} nodes, {num_edges_old:,} edges\")\n",
    "\n",
    "    num_nodes_new = kalicube_graph_new.numberOfNodes()\n",
    "    num_edges_new = kalicube_graph_new.numberOfEdges()\n",
    "    print(f\"NEW Kalicube Graph: {num_nodes_new:,} nodes, {num_edges_new:,} edges\")\n",
    "\n",
    "    print(f\"WWW Graph: REAL web network from {real_web_network_file}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Clean up\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Running {NUM_SIMULATIONS} NetworKit simulations with REAL web data...\")\n",
    "\n",
    "    # Run simulations in batches\n",
    "    BATCH_SIZE = 5\n",
    "    all_results = []\n",
    "    all_comparison_dfs = []\n",
    "\n",
    "    for batch_start in range(0, NUM_SIMULATIONS, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, NUM_SIMULATIONS)\n",
    "        print(\n",
    "            f\"Processing batch {batch_start // BATCH_SIZE + 1}: simulations {batch_start + 1}-{batch_end}\"\n",
    "        )\n",
    "\n",
    "        batch_results, batch_comparisons = run_batch_simulations_networkit(\n",
    "            batch_start,\n",
    "            batch_end,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "        all_comparison_dfs.extend(batch_comparisons)\n",
    "\n",
    "        # Show batch results\n",
    "        for result in batch_results:\n",
    "            effect_symbol = (\n",
    "                \"+\"\n",
    "                if result[\"Total_Delta\"] > 0\n",
    "                else \"-\"\n",
    "                if result[\"Total_Delta\"] < 0\n",
    "                else \"=\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Sim {result['Simulation']:3d}: {effect_symbol} PageRank:{result['Total_Delta']:+.6f} \"\n",
    "                f\"({result['Delta_Percent']:+.2f}%) | Ranks: \"\n",
    "                f\"{result['Rank_Improvements']}up {result['Rank_Drops']}down {result['Rank_Unchanged']}same\"\n",
    "            )\n",
    "\n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"All REAL WEB simulations completed in {end_time - start_time:.2f} seconds!\")\n",
    "    print(\n",
    "        f\"Average time per simulation: {(end_time - start_time) / NUM_SIMULATIONS:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Process and save results\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        all_comparisons_df = pd.concat(all_comparison_dfs, ignore_index=True)\n",
    "\n",
    "        results_df.to_csv(\"simulation_summary_real_web.csv\", index=False)\n",
    "        all_comparisons_df.to_csv(\"all_simulations_detailed_real_web.csv\", index=False)\n",
    "\n",
    "        print(\"Saved REAL WEB simulation results:\")\n",
    "        print(\" - simulation_summary_real_web.csv: Overall metrics\")\n",
    "        print(\" - all_simulations_detailed_real_web.csv: Detailed results\")\n",
    "\n",
    "        # Generate visualization\n",
    "        print(\"\\nGenerating result visualizations...\")\n",
    "        plot_results_networkit(results_df)\n",
    "\n",
    "        print(\"\\nREAL WEB Network Statistical Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Mean overall delta: {results_df['Total_Delta'].mean():.6f}\")\n",
    "        print(f\"Std dev overall delta: {results_df['Total_Delta'].std():.6f}\")\n",
    "        print(f\"Mean delta percentage: {results_df['Delta_Percent'].mean():.2f}%\")\n",
    "        print(f\"Std dev delta percentage: {results_df['Delta_Percent'].std():.2f}%\")\n",
    "\n",
    "        positive_outcomes = (results_df[\"Total_Delta\"] > 0).sum()\n",
    "        negative_outcomes = (results_df[\"Total_Delta\"] < 0).sum()\n",
    "        neutral_outcomes = (results_df[\"Total_Delta\"] == 0).sum()\n",
    "\n",
    "        print(f\"\\nREAL WEB Outcome Distribution:\")\n",
    "        print(\"=\" * 35)\n",
    "        print(\n",
    "            f\" - Positive outcomes: {positive_outcomes}/{NUM_SIMULATIONS} \"\n",
    "            f\"({positive_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Negative outcomes: {negative_outcomes}/{NUM_SIMULATIONS} \"\n",
    "            f\"({negative_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Neutral outcomes: {neutral_outcomes}/{NUM_SIMULATIONS} \"\n",
    "            f\"({neutral_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nREAL WEB NETWORK SIMULATION COMPLETE!\")\n",
    "        print(f\"Total simulation time: {end_time - start_time:.1f} seconds\")\n",
    "        print(f\"Analyzed PageRank on real web pages from FineWeb dataset\")\n",
    "        print(f\"Results are based on actual web topology, not synthetic!\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid simulation results generated.\")\n",
    "\n",
    "    # Clear cache and cleanup\n",
    "    _www_graph_cache = None\n",
    "    gc.collect()\n",
    "    print(\"Memory cleaned up. REAL WEB simulation complete!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
