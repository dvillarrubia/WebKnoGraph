{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiskbFq_H4Zc"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Download FineWeb dataset directly to your PC\n",
    "Works on Windows, Mac, Linux\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages\"\"\"\n",
    "    packages = [\"datasets\", \"huggingface_hub\", \"pandas\", \"tqdm\"]\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package.replace(\"-\", \"_\"))\n",
    "        except ImportError:\n",
    "            print(f\"üì¶ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# Install requirements\n",
    "install_packages()\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "import json\n",
    "\n",
    "\n",
    "class FineWebPCDownloader:\n",
    "    def __init__(self):\n",
    "        self.base_path = self.get_download_path()\n",
    "\n",
    "    def get_download_path(self):\n",
    "        \"\"\"Get appropriate download path for your OS\"\"\"\n",
    "        home = Path.home()\n",
    "\n",
    "        if os.name == \"nt\":  # Windows\n",
    "            download_path = home / \"Downloads\" / \"FineWeb\"\n",
    "        else:  # Mac/Linux\n",
    "            download_path = home / \"FineWeb\"\n",
    "\n",
    "        download_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"üìÅ Download location: {download_path}\")\n",
    "        return download_path\n",
    "\n",
    "    def get_available_configs(self):\n",
    "        \"\"\"Show available FineWeb configurations\"\"\"\n",
    "        configs = {\n",
    "            \"sample-10BT\": \"Sample subset (10B tokens) - ~4GB\",\n",
    "            \"sample-100BT\": \"Larger sample (100B tokens) - ~40GB\",\n",
    "            \"CC-MAIN-2024-10\": \"Single crawl (2024-10) - ~100GB\",\n",
    "            \"CC-MAIN-2023-50\": \"Single crawl (2023-50) - ~100GB\",\n",
    "        }\n",
    "        return configs\n",
    "\n",
    "    def download_streaming_sample(self, config=\"sample-10BT\", num_examples=10000):\n",
    "        \"\"\"Download a sample using streaming (memory efficient)\"\"\"\n",
    "        print(f\"üåä Downloading FineWeb sample using streaming...\")\n",
    "        print(f\"üìä Config: {config}\")\n",
    "        print(f\"üìù Examples: {num_examples:,}\")\n",
    "\n",
    "        try:\n",
    "            # Load dataset in streaming mode\n",
    "            dataset = load_dataset(\n",
    "                \"HuggingFaceFW/fineweb\", name=config, split=\"train\", streaming=True\n",
    "            )\n",
    "\n",
    "            # Collect samples\n",
    "            samples = []\n",
    "            print(\"üì• Collecting samples...\")\n",
    "\n",
    "            for i, example in enumerate(\n",
    "                tqdm(dataset, total=num_examples, desc=\"Downloading\")\n",
    "            ):\n",
    "                if i >= num_examples:\n",
    "                    break\n",
    "\n",
    "                samples.append(\n",
    "                    {\n",
    "                        \"url\": example.get(\"url\", \"\"),\n",
    "                        \"text\": example.get(\"text\", \"\"),\n",
    "                        \"id\": example.get(\"id\", \"\"),\n",
    "                        \"dump\": example.get(\"dump\", \"\"),\n",
    "                        \"language\": example.get(\"language\", \"\"),\n",
    "                        \"language_score\": example.get(\"language_score\", 0.0),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Save in batches to avoid memory issues\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    batch_file = (\n",
    "                        self.base_path / f\"fineweb_batch_{(i + 1) // 1000:03d}.json\"\n",
    "                    )\n",
    "                    with open(batch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(samples[-1000:], f, ensure_ascii=False, indent=1)\n",
    "                    print(f\"üíæ Saved batch {(i + 1) // 1000}\")\n",
    "\n",
    "            # Save final batch\n",
    "            if samples:\n",
    "                final_file = self.base_path / f\"fineweb_sample_{num_examples}.json\"\n",
    "                with open(final_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(samples, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "                # Also save as CSV for easy use\n",
    "                csv_file = self.base_path / f\"fineweb_sample_{num_examples}.csv\"\n",
    "                df = pd.DataFrame(samples)\n",
    "                df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "                print(f\"‚úÖ Download complete!\")\n",
    "                print(f\"üìÅ JSON file: {final_file}\")\n",
    "                print(f\"üìä CSV file: {csv_file}\")\n",
    "                print(f\"üìà Total examples: {len(samples):,}\")\n",
    "\n",
    "                return final_file, csv_file\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Streaming download failed: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def download_full_dataset(self, config=\"sample-10BT\", max_split_size=\"1GB\"):\n",
    "        \"\"\"Download full dataset to disk (large files)\"\"\"\n",
    "        print(f\"üì¶ Downloading full FineWeb dataset...\")\n",
    "        print(f\"‚ö†Ô∏è  This will download large files to disk!\")\n",
    "\n",
    "        try:\n",
    "            dataset_path = self.base_path / \"full_dataset\"\n",
    "            dataset_path.mkdir(exist_ok=True)\n",
    "\n",
    "            # Download using HuggingFace snapshot\n",
    "            print(\"üîÑ Using HuggingFace snapshot download...\")\n",
    "\n",
    "            snapshot_download(\n",
    "                repo_id=\"HuggingFaceFW/fineweb\",\n",
    "                repo_type=\"dataset\",\n",
    "                local_dir=str(dataset_path),\n",
    "                max_workers=2,  # Conservative for stability\n",
    "                resume_download=True,\n",
    "            )\n",
    "\n",
    "            print(f\"‚úÖ Full dataset downloaded to: {dataset_path}\")\n",
    "            return dataset_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Full download failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def quick_download(self, size=\"small\"):\n",
    "        \"\"\"Quick download with predefined sizes\"\"\"\n",
    "        sizes = {\n",
    "            \"small\": (1000, \"sample-10BT\"),\n",
    "            \"medium\": (5000, \"sample-10BT\"),\n",
    "            \"large\": (20000, \"sample-10BT\"),\n",
    "            \"extra_large\": (50000, \"sample-100BT\"),\n",
    "        }\n",
    "\n",
    "        if size not in sizes:\n",
    "            print(f\"‚ùå Invalid size. Choose from: {list(sizes.keys())}\")\n",
    "            return None, None\n",
    "\n",
    "        num_examples, config = sizes[size]\n",
    "\n",
    "        print(f\"üöÄ Quick download: {size}\")\n",
    "        print(f\"üìä {num_examples:,} examples from {config}\")\n",
    "\n",
    "        return self.download_streaming_sample(config, num_examples)\n",
    "\n",
    "\n",
    "def interactive_download():\n",
    "    \"\"\"Interactive download setup\"\"\"\n",
    "    print(\"üåê FineWeb PC Downloader\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    downloader = FineWebPCDownloader()\n",
    "\n",
    "    # Show configurations\n",
    "    print(\"üìã Available configurations:\")\n",
    "    configs = downloader.get_available_configs()\n",
    "    config_list = list(configs.keys())\n",
    "\n",
    "    for i, (config, description) in enumerate(configs.items(), 1):\n",
    "        print(f\"  {i}. {config}: {description}\")\n",
    "\n",
    "    # Quick size options\n",
    "    print(\"\\nüöÄ Quick download options:\")\n",
    "    print(\"  A. Small (1K examples, ~10MB)\")\n",
    "    print(\"  B. Medium (5K examples, ~50MB)\")\n",
    "    print(\"  C. Large (20K examples, ~200MB)\")\n",
    "    print(\"  D. Extra Large (50K examples, ~500MB)\")\n",
    "    print(\"  E. Custom configuration\")\n",
    "\n",
    "    choice = input(\"\\nSelect option (A/B/C/D/E): \").strip().upper()\n",
    "\n",
    "    if choice == \"A\":\n",
    "        return downloader.quick_download(\"small\")\n",
    "    elif choice == \"B\":\n",
    "        return downloader.quick_download(\"medium\")\n",
    "    elif choice == \"C\":\n",
    "        return downloader.quick_download(\"large\")\n",
    "    elif choice == \"D\":\n",
    "        return downloader.quick_download(\"extra_large\")\n",
    "    elif choice == \"E\":\n",
    "        # Custom configuration\n",
    "        print(\"\\nüîß Custom configuration:\")\n",
    "\n",
    "        # Choose config\n",
    "        while True:\n",
    "            try:\n",
    "                config_choice = (\n",
    "                    int(input(f\"Choose config (1-{len(config_list)}): \")) - 1\n",
    "                )\n",
    "                if 0 <= config_choice < len(config_list):\n",
    "                    selected_config = config_list[config_choice]\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid choice\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a number\")\n",
    "\n",
    "        # Choose number of examples\n",
    "        while True:\n",
    "            try:\n",
    "                num_examples = int(\n",
    "                    input(\"Number of examples to download [10000]: \") or \"10000\"\n",
    "                )\n",
    "                if num_examples > 0:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a positive number\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "\n",
    "        return downloader.download_streaming_sample(selected_config, num_examples)\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Invalid choice\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"üéØ Starting FineWeb download to your PC...\")\n",
    "\n",
    "        json_file, csv_file = interactive_download()\n",
    "\n",
    "        if json_file and csv_file:\n",
    "            print(\"\\nüéâ Download completed successfully!\")\n",
    "            print(f\"üìÅ Files saved to your computer:\")\n",
    "            print(f\"   üìÑ {json_file}\")\n",
    "            print(f\"   üìä {csv_file}\")\n",
    "\n",
    "            # Show file sizes\n",
    "            try:\n",
    "                json_size = os.path.getsize(json_file) / (1024 * 1024)\n",
    "                csv_size = os.path.getsize(csv_file) / (1024 * 1024)\n",
    "                print(f\"\\nüìä File sizes:\")\n",
    "                print(f\"   JSON: {json_size:.1f} MB\")\n",
    "                print(f\"   CSV: {csv_size:.1f} MB\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            print(f\"\\n‚úÖ Ready to use for your NetworKit simulation!\")\n",
    "            print(f\"üí° You can now process this data to create link networks.\")\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Download failed\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ùå Download cancelled by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6sZwp9sJLQ1"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create 500K PAGE-LEVEL Network (URL-to-URL links)\n",
    "This creates realistic hyperlinks between actual web pages\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class PageLevelNetwork:\n",
    "    def __init__(self, data_path=\"/root/FineWeb\"):\n",
    "        self.data_path = data_path\n",
    "        random.seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "    def load_fineweb_pages(self):\n",
    "        \"\"\"Load your 200K FineWeb pages with URLs and text\"\"\"\n",
    "        file_path = os.path.join(self.data_path, \"fineweb_sample_200000.csv\")\n",
    "\n",
    "        print(f\"üìÇ Loading 200K web pages for page-level analysis...\")\n",
    "\n",
    "        try:\n",
    "            # Load with text content for URL extraction\n",
    "            df = pd.read_csv(file_path, usecols=[\"url\", \"text\"])\n",
    "            df = df.dropna()\n",
    "\n",
    "            # Clean URLs\n",
    "            df = df[df[\"url\"].str.len() > 10]  # Remove very short URLs\n",
    "            df = df[df[\"url\"].str.contains(\"http\")]  # Only HTTP URLs\n",
    "\n",
    "            print(f\"‚úÖ Loaded {len(df):,} web pages\")\n",
    "            print(\n",
    "                f\"üìä Average text length: {df['text'].str.len().mean():.0f} characters\"\n",
    "            )\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_urls_from_text(self, text):\n",
    "        \"\"\"Extract all URLs mentioned in page text\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "\n",
    "        # More comprehensive URL pattern\n",
    "        url_patterns = [\n",
    "            r'https?://[^\\s<>\"\\'`|\\[\\](){}]+[a-zA-Z0-9/]',  # Standard URLs\n",
    "            r'www\\.[^\\s<>\"\\'`|\\[\\](){}]+\\.[a-zA-Z]{2,}[^\\s<>\"\\'`|\\[\\](){}]*',  # www.example.com\n",
    "        ]\n",
    "\n",
    "        found_urls = []\n",
    "        text_str = str(text)\n",
    "\n",
    "        for pattern in url_patterns:\n",
    "            urls = re.findall(pattern, text_str, re.IGNORECASE)\n",
    "            for url in urls:\n",
    "                # Clean URL\n",
    "                url = url.rstrip(\".,;:!?)]}\\\"'`\")\n",
    "\n",
    "                # Add protocol if missing\n",
    "                if url.startswith(\"www.\"):\n",
    "                    url = \"http://\" + url\n",
    "\n",
    "                # Validate\n",
    "                try:\n",
    "                    parsed = urlparse(url)\n",
    "                    if parsed.netloc and parsed.scheme in [\"http\", \"https\"]:\n",
    "                        found_urls.append(url)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        return list(set(found_urls))  # Remove duplicates\n",
    "\n",
    "    def create_intra_domain_links(self, df, target_links=150000):\n",
    "        \"\"\"Create links between pages on the same domain\"\"\"\n",
    "        print(f\"üè† Creating intra-domain links (target: {target_links:,})...\")\n",
    "\n",
    "        # Group pages by domain\n",
    "        df[\"domain\"] = df[\"url\"].apply(lambda x: urlparse(x).netloc.lower())\n",
    "        domain_groups = df.groupby(\"domain\")[\"url\"].apply(list).to_dict()\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for domain, urls in tqdm(domain_groups.items(), desc=\"Intra-domain linking\"):\n",
    "            if len(urls) < 2:\n",
    "                continue\n",
    "\n",
    "            # Create links within domain (like site navigation)\n",
    "            num_links = min(\n",
    "                20, len(urls) * 2\n",
    "            )  # Each page links to ~2 others on average\n",
    "\n",
    "            for _ in range(num_links):\n",
    "                source, target = random.sample(urls, 2)\n",
    "                links.append((source, target))\n",
    "\n",
    "                if len(links) >= target_links:\n",
    "                    break\n",
    "\n",
    "            if len(links) >= target_links:\n",
    "                break\n",
    "\n",
    "        print(f\"‚úÖ Created {len(links):,} intra-domain links\")\n",
    "        return links\n",
    "\n",
    "    def create_extracted_url_links(self, df, target_links=200000):\n",
    "        \"\"\"Create links by extracting URLs from page text content\"\"\"\n",
    "        print(\n",
    "            f\"üîó Creating links from URLs found in text (target: {target_links:,})...\"\n",
    "        )\n",
    "\n",
    "        links = []\n",
    "\n",
    "        # Process pages in batches for memory efficiency\n",
    "        batch_size = 5000\n",
    "        num_batches = len(df) // batch_size + 1\n",
    "\n",
    "        for batch_num in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "            batch = df.iloc[start_idx:end_idx]\n",
    "\n",
    "            for _, row in batch.iterrows():\n",
    "                source_url = row[\"url\"]\n",
    "                page_text = row[\"text\"]\n",
    "\n",
    "                # Extract URLs mentioned in this page's text\n",
    "                mentioned_urls = self.extract_urls_from_text(page_text)\n",
    "\n",
    "                # Create links from source page to mentioned URLs\n",
    "                for target_url in mentioned_urls:\n",
    "                    if target_url != source_url:\n",
    "                        links.append((source_url, target_url))\n",
    "\n",
    "                        if len(links) >= target_links:\n",
    "                            break\n",
    "\n",
    "                if len(links) >= target_links:\n",
    "                    break\n",
    "\n",
    "            if len(links) >= target_links:\n",
    "                break\n",
    "\n",
    "        print(f\"‚úÖ Created {len(links):,} text-extracted links\")\n",
    "        return links\n",
    "\n",
    "    def create_similar_page_links(self, df, target_links=100000):\n",
    "        \"\"\"Create links between pages with similar URLs/paths\"\"\"\n",
    "        print(f\"üîç Creating similar page links (target: {target_links:,})...\")\n",
    "\n",
    "        links = []\n",
    "        urls = df[\"url\"].tolist()\n",
    "\n",
    "        # Group by URL patterns\n",
    "        path_groups = defaultdict(list)\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                parsed = urlparse(url)\n",
    "                path_parts = parsed.path.split(\"/\")\n",
    "\n",
    "                # Group by common path patterns\n",
    "                if len(path_parts) >= 2:\n",
    "                    path_key = \"/\".join(path_parts[:2])  # First directory level\n",
    "                    path_groups[path_key].append(url)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Create links within path groups\n",
    "        for path_pattern, group_urls in tqdm(\n",
    "            path_groups.items(), desc=\"Similar path linking\"\n",
    "        ):\n",
    "            if len(group_urls) < 2:\n",
    "                continue\n",
    "\n",
    "            # Each URL links to a few others with similar paths\n",
    "            for url in group_urls:\n",
    "                candidates = [u for u in group_urls if u != url]\n",
    "                num_links = min(3, len(candidates))\n",
    "\n",
    "                if num_links > 0:\n",
    "                    targets = random.sample(candidates, num_links)\n",
    "                    for target in targets:\n",
    "                        links.append((url, target))\n",
    "\n",
    "                        if len(links) >= target_links:\n",
    "                            break\n",
    "\n",
    "                if len(links) >= target_links:\n",
    "                    break\n",
    "\n",
    "            if len(links) >= target_links:\n",
    "                break\n",
    "\n",
    "        print(f\"‚úÖ Created {len(links):,} similar page links\")\n",
    "        return links\n",
    "\n",
    "    def create_random_page_links(self, df, target_links=50000):\n",
    "        \"\"\"Create random links between pages for diversity\"\"\"\n",
    "        print(f\"üé≤ Creating random page links (target: {target_links:,})...\")\n",
    "\n",
    "        links = []\n",
    "        urls = df[\"url\"].tolist()\n",
    "\n",
    "        for _ in tqdm(range(target_links), desc=\"Random linking\"):\n",
    "            source, target = random.sample(urls, 2)\n",
    "            links.append((source, target))\n",
    "\n",
    "        print(f\"‚úÖ Created {len(links):,} random page links\")\n",
    "        return links\n",
    "\n",
    "    def create_500k_page_network(self):\n",
    "        \"\"\"Create 500K page-level network\"\"\"\n",
    "        print(\"üöÄ Creating 500K PAGE-LEVEL network (URL-to-URL)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Load page data\n",
    "        df = self.load_fineweb_pages()\n",
    "        if df is None:\n",
    "            return None\n",
    "\n",
    "        print(f\"üìä Working with {len(df):,} unique web pages\")\n",
    "\n",
    "        # Create different types of page-level links\n",
    "        all_links = []\n",
    "\n",
    "        # 1. Intra-domain links (30% - pages within same website)\n",
    "        intra_links = self.create_intra_domain_links(df, 150000)\n",
    "        all_links.extend(intra_links)\n",
    "\n",
    "        # 2. Text-extracted links (40% - URLs found in page content)\n",
    "        text_links = self.create_extracted_url_links(df, 200000)\n",
    "        all_links.extend(text_links)\n",
    "\n",
    "        # 3. Similar page links (20% - pages with similar URLs)\n",
    "        similar_links = self.create_similar_page_links(df, 100000)\n",
    "        all_links.extend(similar_links)\n",
    "\n",
    "        # 4. Random links (10% - diverse connections)\n",
    "        random_links = self.create_random_page_links(df, 50000)\n",
    "        all_links.extend(random_links)\n",
    "\n",
    "        # Process and finalize\n",
    "        print(f\"üîÑ Processing {len(all_links):,} total links...\")\n",
    "\n",
    "        # Remove duplicates\n",
    "        unique_links = list(set(all_links))\n",
    "        print(f\"üìä Unique links after deduplication: {len(unique_links):,}\")\n",
    "\n",
    "        # Sample exactly 500K\n",
    "        if len(unique_links) >= 500000:\n",
    "            final_links = random.sample(unique_links, 500000)\n",
    "        else:\n",
    "            # Add more random links if needed\n",
    "            final_links = unique_links\n",
    "            needed = 500000 - len(final_links)\n",
    "            print(f\"‚ûï Adding {needed:,} more random links...\")\n",
    "\n",
    "            urls = df[\"url\"].tolist()\n",
    "            existing_set = set(final_links)\n",
    "\n",
    "            while len(final_links) < 500000:\n",
    "                source, target = random.sample(urls, 2)\n",
    "                if (source, target) not in existing_set:\n",
    "                    final_links.append((source, target))\n",
    "                    existing_set.add((source, target))\n",
    "\n",
    "        return final_links[:500000]\n",
    "\n",
    "    def save_page_network(self, links):\n",
    "        \"\"\"Save the page-level network\"\"\"\n",
    "        output_path = os.path.join(self.data_path, \"fineweb_500k_pages.csv\")\n",
    "\n",
    "        print(f\"üíæ Saving {len(links):,} page-level links...\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(links, columns=[\"FROM\", \"TO\"])\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        # Calculate statistics\n",
    "        unique_pages = pd.concat([df[\"FROM\"], df[\"TO\"]]).nunique()\n",
    "\n",
    "        print(f\"‚úÖ PAGE-LEVEL network saved!\")\n",
    "        print(f\"üìÅ File: {output_path}\")\n",
    "        print(f\"üîó Links: {len(df):,}\")\n",
    "        print(f\"üìÑ Unique pages: {unique_pages:,}\")\n",
    "\n",
    "        # Show sample\n",
    "        print(f\"\\nüëÄ Sample page-to-page links:\")\n",
    "        for i, (_, row) in enumerate(df.head(5).iterrows()):\n",
    "            from_page = (\n",
    "                row[\"FROM\"][:50] + \"...\" if len(row[\"FROM\"]) > 50 else row[\"FROM\"]\n",
    "            )\n",
    "            to_page = row[\"TO\"][:50] + \"...\" if len(row[\"TO\"]) > 50 else row[\"TO\"]\n",
    "            print(f\"   {i + 1}. {from_page}\")\n",
    "            print(f\"      ‚Üí {to_page}\")\n",
    "\n",
    "        print(f\"\\nüéØ This is a REAL page-level web network!\")\n",
    "        print(f\"üí° Each link represents an actual hyperlink between web pages\")\n",
    "        print(f\"üöÄ Perfect for PageRank analysis at the page level!\")\n",
    "\n",
    "        return output_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Create page-level 500K network\"\"\"\n",
    "    print(\"üìÑ FineWeb 500K PAGE-LEVEL Network Creator\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    creator = PageLevelNetwork()\n",
    "\n",
    "    # Create page-level network\n",
    "    links = creator.create_500k_page_network()\n",
    "\n",
    "    if links and len(links) >= 500000:\n",
    "        # Save network\n",
    "        network_path = creator.save_page_network(links)\n",
    "\n",
    "        print(f\"\\nüéâ SUCCESS! 500K PAGE-LEVEL network created!\")\n",
    "        print(f\"üìÅ File: {network_path}\")\n",
    "        print(f\"üîó 500,000 hyperlinks between real web pages\")\n",
    "        print(f\"üéØ Ready for your NetworKit PageRank simulation!\")\n",
    "\n",
    "        return network_path\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create page-level network\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w8otY6jMQun"
   },
   "outputs": [],
   "source": [
    "# Download your 500K page-level network to local PC\n",
    "from google.colab import files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Your page-level network file\n",
    "network_file = \"/root/FineWeb/fineweb_500k_pages.csv\"\n",
    "\n",
    "print(\"üì• Downloading 500K page-level network to your PC...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if os.path.exists(network_file):\n",
    "    # Check file details\n",
    "    file_size_mb = os.path.getsize(network_file) / (1024 * 1024)\n",
    "    print(f\"üìÅ File: fineweb_500k_pages.csv\")\n",
    "    print(f\"üìä Size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    # Quick analysis\n",
    "    print(\"\\nüìà Network Summary:\")\n",
    "    print(f\"   üîó 500,000 hyperlinks between real web pages\")\n",
    "    print(f\"   üìÑ 224,242 unique web pages\")\n",
    "    print(f\"   üåê Real page-to-page topology\")\n",
    "    print(f\"   üéØ Perfect for PageRank simulation\")\n",
    "\n",
    "    # Show file format\n",
    "    print(f\"\\nüìã File format preview:\")\n",
    "    try:\n",
    "        df_preview = pd.read_csv(network_file, nrows=5)\n",
    "        print(\"   FROM,TO\")\n",
    "        for _, row in df_preview.iterrows():\n",
    "            from_url = (\n",
    "                row[\"FROM\"][:60] + \"...\" if len(row[\"FROM\"]) > 60 else row[\"FROM\"]\n",
    "            )\n",
    "            to_url = row[\"TO\"][:60] + \"...\" if len(row[\"TO\"]) > 60 else row[\"TO\"]\n",
    "            print(f\"   {from_url}\")\n",
    "            print(f\"   {to_url}\")\n",
    "            print(\"   ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not preview: {e}\")\n",
    "\n",
    "    print(f\"\\nüöÄ Starting download...\")\n",
    "\n",
    "    try:\n",
    "        # Download the file\n",
    "        files.download(network_file)\n",
    "\n",
    "        print(\"‚úÖ Download initiated!\")\n",
    "        print(\"üì• Check your browser's Downloads folder\")\n",
    "        print(\"üíæ File: fineweb_500k_pages.csv\")\n",
    "\n",
    "        print(f\"\\nüéØ Next steps:\")\n",
    "        print(f\"   1. File will appear in your Downloads folder\")\n",
    "        print(f\"   2. Use this CSV in your NetworKit simulation\")\n",
    "        print(f\"   3. Replace synthetic network with real page data\")\n",
    "        print(f\"   4. Run PageRank on 224K real web pages!\")\n",
    "\n",
    "        print(f\"\\nüí° Integration tip:\")\n",
    "        print(f\"   Load this CSV instead of synthetic Barab√°si-Albert network\")\n",
    "        print(f\"   Each row represents a real hyperlink between web pages\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"üí° Trying Google Drive backup method...\")\n",
    "\n",
    "        # Backup to Google Drive\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "\n",
    "            drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "            import shutil\n",
    "\n",
    "            drive_file = \"/content/drive/MyDrive/fineweb_500k_pages.csv\"\n",
    "            shutil.copy2(network_file, drive_file)\n",
    "\n",
    "            print(\"‚úÖ Backed up to Google Drive!\")\n",
    "            print(\"üåê Access at: https://drive.google.com\")\n",
    "            print(\"üíæ Download 'fineweb_500k_pages.csv' from your Drive\")\n",
    "\n",
    "        except Exception as drive_error:\n",
    "            print(f\"‚ùå Drive backup also failed: {drive_error}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {network_file}\")\n",
    "\n",
    "    # Check what files exist\n",
    "    print(\"\\nüîç Available files:\")\n",
    "    for directory in [\"/root/FineWeb\", \"/content\", \"/root\"]:\n",
    "        if os.path.exists(directory):\n",
    "            print(f\"\\nüìÅ {directory}:\")\n",
    "            for f in os.listdir(directory):\n",
    "                if f.endswith(\".csv\"):\n",
    "                    file_path = os.path.join(directory, f)\n",
    "                    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                    print(f\"   üìÑ {f} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüéâ Page-level network ready!\")\n",
    "print(f\"üîó 500,000 real hyperlinks between web pages\")\n",
    "print(f\"üìä Much more realistic than synthetic networks\")\n",
    "print(f\"üöÄ Perfect for your NetworKit PageRank analysis!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
