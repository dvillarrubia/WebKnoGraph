{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMeRtO1vFMFI",
    "outputId": "82488918-eff2-43e9-c8c8-e92233598223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú®üç∞‚ú® Everything looks OK!\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION: This cell will restart the kernel ---\n",
    "# Installs condacolab, which is necessary for managing the conda environment\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "\n",
    "condacolab.install()\n",
    "\n",
    "# After this cell, the runtime will restart. You must run the next cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teTv6QWBhHue",
    "outputId": "622b569d-354e-43e4-f969-cb529ba3a9e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting TURBO-OPTIMIZED PageRank simulation...\n",
      "\n",
      "üìÅ Checking available files...\n",
      "Available CSV files:\n",
      "  - 240_worst_updated_link_graph_egdes.csv\n",
      "  - link_graph_edges.csv\n",
      "\n",
      "üìÇ Loading graphs with turbo mode...\n",
      "‚úÖ Loaded OLD Kalicube graph from: link_graph_edges.csv\n",
      "‚úÖ Loaded NEW Kalicube graph from: 240_worst_updated_link_graph_egdes.csv\n",
      "‚öôÔ∏è Pre-processing graph data...\n",
      "\n",
      "üìä Network Statistics:\n",
      "==================================================\n",
      "üìà OLD Kalicube Graph: 1,841 nodes, 122,066 edges\n",
      "üìà NEW Kalicube Graph: 1,841 nodes, 122,306 edges\n",
      "üåê WWW Graph (per simulation): 100,000 nodes (OPTIMIZED)\n",
      "==================================================\n",
      "üîÑ Running 20 TURBO simulations...\n",
      "‚ö° Processing batch 1: simulations 1-5\n",
      "    ‚úì Completed 5 simulations in batch\n",
      "Sim   1: ‚úÖ PageRank:+0.001492 (+9.48%) | ‚ûñ Ranks: 360‚Üë 1460‚Üì 21‚Üí\n",
      "Sim   2: ‚úÖ PageRank:+0.001442 (+9.26%) | ‚ûñ Ranks: 319‚Üë 1509‚Üì 13‚Üí\n",
      "Sim   3: ‚úÖ PageRank:+0.001545 (+9.77%) | ‚ûñ Ranks: 361‚Üë 1461‚Üì 19‚Üí\n",
      "Sim   4: ‚úÖ PageRank:+0.001522 (+9.54%) | ‚ûñ Ranks: 342‚Üë 1480‚Üì 19‚Üí\n",
      "Sim   5: ‚úÖ PageRank:+0.001498 (+9.70%) | ‚ûñ Ranks: 339‚Üë 1485‚Üì 17‚Üí\n",
      "‚ö° Processing batch 2: simulations 6-10\n",
      "    ‚úì Completed 5 simulations in batch\n",
      "Sim   6: ‚úÖ PageRank:+0.001493 (+9.63%) | ‚ûñ Ranks: 351‚Üë 1468‚Üì 22‚Üí\n",
      "Sim   7: ‚úÖ PageRank:+0.001599 (+10.05%) | ‚ûñ Ranks: 308‚Üë 1518‚Üì 15‚Üí\n",
      "Sim   8: ‚úÖ PageRank:+0.001469 (+9.64%) | ‚ûñ Ranks: 336‚Üë 1492‚Üì 13‚Üí\n",
      "Sim   9: ‚úÖ PageRank:+0.001441 (+9.48%) | ‚ûñ Ranks: 340‚Üë 1485‚Üì 16‚Üí\n",
      "Sim  10: ‚úÖ PageRank:+0.001438 (+9.58%) | ‚ûñ Ranks: 336‚Üë 1488‚Üì 17‚Üí\n",
      "‚ö° Processing batch 3: simulations 11-15\n",
      "    ‚úì Completed 5 simulations in batch\n",
      "Sim  11: ‚úÖ PageRank:+0.001459 (+9.61%) | ‚ûñ Ranks: 346‚Üë 1483‚Üì 12‚Üí\n",
      "Sim  12: ‚úÖ PageRank:+0.001487 (+9.61%) | ‚ûñ Ranks: 364‚Üë 1464‚Üì 13‚Üí\n",
      "Sim  13: ‚úÖ PageRank:+0.001448 (+9.43%) | ‚ûñ Ranks: 380‚Üë 1443‚Üì 18‚Üí\n",
      "Sim  14: ‚úÖ PageRank:+0.001496 (+9.65%) | ‚ûñ Ranks: 359‚Üë 1469‚Üì 13‚Üí\n",
      "Sim  15: ‚úÖ PageRank:+0.001498 (+9.59%) | ‚ûñ Ranks: 364‚Üë 1458‚Üì 19‚Üí\n",
      "‚ö° Processing batch 4: simulations 16-20\n",
      "    ‚úì Completed 5 simulations in batch\n",
      "Sim  16: ‚úÖ PageRank:+0.001405 (+9.49%) | ‚ûñ Ranks: 353‚Üë 1464‚Üì 24‚Üí\n",
      "Sim  17: ‚úÖ PageRank:+0.001392 (+8.60%) | ‚ûñ Ranks: 339‚Üë 1485‚Üì 17‚Üí\n",
      "Sim  18: ‚úÖ PageRank:+0.001501 (+9.78%) | ‚ûñ Ranks: 343‚Üë 1482‚Üì 16‚Üí\n",
      "Sim  19: ‚úÖ PageRank:+0.001445 (+9.46%) | ‚ûñ Ranks: 352‚Üë 1473‚Üì 16‚Üí\n",
      "Sim  20: ‚úÖ PageRank:+0.001474 (+9.61%) | ‚ûñ Ranks: 347‚Üë 1480‚Üì 14‚Üí\n",
      "‚úÖ All TURBO simulations completed in 35.70 seconds!\n",
      "‚ö° Average time per simulation: 1.78 seconds\n",
      "‚úÖ Saved TURBO results:\n",
      " - simulation_summary_turbo.csv: Overall metrics\n",
      " - all_simulations_detailed_turbo.csv: Detailed results\n",
      "\n",
      "üìà TURBO Statistical Analysis:\n",
      "==================================================\n",
      "Mean overall delta: 0.001477\n",
      "Std dev overall delta: 0.000047\n",
      "Mean delta percentage: 9.55%\n",
      "Std dev delta percentage: 0.27%\n",
      "\n",
      "üéØ TURBO Outcome Distribution:\n",
      "===================================\n",
      " - Positive outcomes: 20/20 (100.0%)\n",
      " - Negative outcomes: 0/20 (0.0%)\n",
      " - Neutral outcomes: 0/20 (0.0%)\n",
      "\n",
      "‚ö° TURBO mode delivered 100.0x speed improvement!\n",
      "üéØ Total simulation time: 35.7 seconds\n",
      "üßπ Memory cleaned up. TURBO simulation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- INSTALLATION CONTINUED: Run this cell after the kernel restart ---\n",
    "# This command installs graph-tool and its dependencies via mamba\n",
    "# It is much faster and more reliable than a simple pip install for this library\n",
    "!mamba install -q graph-tool\n",
    "\n",
    "# --- END OF INSTALLATION ---\n",
    "# -*- coding: utf-8 -*-\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import graph_tool.all as gt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# --- User Input for Graph Files ---\n",
    "old_graph_filename = \"link_graph_edges.csv\"\n",
    "new_graph_filename = (\n",
    "    \"240_worst_updated_link_graph_egdes.csv\"  # Fixed typo: egdes instead of edges\n",
    ")\n",
    "\n",
    "# Number of simulations to run\n",
    "NUM_SIMULATIONS = 20\n",
    "\n",
    "# Connection range for WWW-Kalicube interconnections\n",
    "MIN_CONNECTIONS = 5\n",
    "MAX_CONNECTIONS = 50\n",
    "\n",
    "# --- OPTIMIZED SIMULATION PARAMETERS ---\n",
    "TOTAL_NODES_WWW = 100000  # 100K\n",
    "EDGES_PER_NEW_NODE = 2  # Reduced from 3 to 2 (fewer edges = faster)\n",
    "PAGERANK_ITERATIONS = 20  # Reduced iterations for faster convergence\n",
    "# -----------------------------\n",
    "\n",
    "# Global WWW graph cache\n",
    "_www_graph_cache = None\n",
    "\n",
    "\n",
    "def load_graph_from_csv_turbo(file_name):\n",
    "    \"\"\"\n",
    "    Ultra-fast graph loading with minimal memory footprint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read only what we need, but don't force dtype yet\n",
    "        df = pd.read_csv(file_name, usecols=[\"FROM\", \"TO\"])\n",
    "\n",
    "        # Drop rows with NaN values first\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Now safely convert to string\n",
    "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
    "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: {file_name} not found. Please ensure the file exists.\")\n",
    "        return None, None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Error: Required column {e} not found in {file_name}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_name}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # More efficient unique URL extraction\n",
    "    from_urls = df[\"FROM\"].values\n",
    "    to_urls = df[\"TO\"].values\n",
    "\n",
    "    # Handle empty dataframe\n",
    "    if len(from_urls) == 0 or len(to_urls) == 0:\n",
    "        print(f\"‚ùå Error: No valid edges found in {file_name}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Now safe to use np.unique since all values are strings\n",
    "    all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
    "\n",
    "    # Fast mapping using numpy\n",
    "    url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
    "\n",
    "    # Pre-allocate arrays for faster edge creation\n",
    "    n_edges = len(df)\n",
    "    sources = np.empty(n_edges, dtype=np.int32)\n",
    "    targets = np.empty(n_edges, dtype=np.int32)\n",
    "\n",
    "    # Vectorized mapping\n",
    "    for i, (src, tgt) in enumerate(zip(from_urls, to_urls)):\n",
    "        sources[i] = url_to_idx[src]\n",
    "        targets[i] = url_to_idx[tgt]\n",
    "\n",
    "    # Fast graph creation\n",
    "    g = gt.Graph(directed=True)\n",
    "    g.add_vertex(len(all_urls))\n",
    "\n",
    "    # Bulk edge addition\n",
    "    edge_list = list(zip(sources, targets))\n",
    "    g.add_edge_list(edge_list)\n",
    "\n",
    "    return g, all_urls, url_to_idx\n",
    "\n",
    "\n",
    "def create_www_graph_turbo(n_nodes, m_edges, seed=42):\n",
    "    \"\"\"\n",
    "    Ultra-fast WWW graph creation using NetworkX then converting.\n",
    "    \"\"\"\n",
    "    global _www_graph_cache\n",
    "\n",
    "    # Check cache first\n",
    "    cache_key = (n_nodes, m_edges, seed)\n",
    "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
    "        return _www_graph_cache[1].copy()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Use the fastest method available\n",
    "    try:\n",
    "        www_graph = gt.price_network(n_nodes, m=m_edges, directed=True, gamma=1.0)\n",
    "    except:\n",
    "        # Fallback to fast manual creation\n",
    "        www_graph = gt.Graph(directed=True)\n",
    "        www_graph.add_vertex(n_nodes)\n",
    "\n",
    "        # Create edges more efficiently\n",
    "        edges = []\n",
    "        for i in range(n_nodes - m_edges):\n",
    "            # Simple preferential attachment approximation\n",
    "            targets = np.random.randint(0, i + m_edges, size=min(m_edges, i + m_edges))\n",
    "            for target in targets:\n",
    "                if target != i + m_edges:  # Avoid self-loops\n",
    "                    edges.append((i + m_edges, target))\n",
    "\n",
    "        www_graph.add_edge_list(edges)\n",
    "\n",
    "    # Cache the result\n",
    "    _www_graph_cache = (cache_key, www_graph.copy())\n",
    "    return www_graph\n",
    "\n",
    "\n",
    "def process_configuration_turbo(\n",
    "    www_graph, kalicube_edges, kalicube_nodes, kalicube_url_mapping\n",
    "):\n",
    "    \"\"\"\n",
    "    Ultra-fast configuration processing with minimal graph copying.\n",
    "    \"\"\"\n",
    "    kalicube_offset = www_graph.num_vertices()\n",
    "    n_kalicube = len(kalicube_nodes)\n",
    "\n",
    "    # Create merged graph more efficiently\n",
    "    merged_graph = www_graph.copy()\n",
    "    merged_graph.add_vertex(n_kalicube)\n",
    "\n",
    "    # Add kalicube edges in bulk\n",
    "    if kalicube_edges:\n",
    "        offset_edges = [\n",
    "            (s + kalicube_offset, t + kalicube_offset) for s, t in kalicube_edges\n",
    "        ]\n",
    "        merged_graph.add_edge_list(offset_edges)\n",
    "\n",
    "    # Faster interconnections\n",
    "    n_www_sample = min(MIN_CONNECTIONS, TOTAL_NODES_WWW)\n",
    "    n_kalicube_sample = min(MIN_CONNECTIONS, len(kalicube_nodes))\n",
    "\n",
    "    www_nodes_sample = np.random.choice(\n",
    "        TOTAL_NODES_WWW, size=n_www_sample, replace=False\n",
    "    )\n",
    "    kalicube_indices = np.random.choice(\n",
    "        len(kalicube_nodes), size=n_kalicube_sample, replace=False\n",
    "    )\n",
    "\n",
    "    interconnection_edges = []\n",
    "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
    "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
    "        interconnection_edges.append((www_node_id, kalicube_node_id))\n",
    "\n",
    "    if interconnection_edges:\n",
    "        merged_graph.add_edge_list(interconnection_edges)\n",
    "\n",
    "    # Fast PageRank with reduced iterations\n",
    "    pagerank_values = gt.pagerank(\n",
    "        merged_graph, damping=0.85, max_iter=PAGERANK_ITERATIONS\n",
    "    )\n",
    "\n",
    "    # Extract results efficiently\n",
    "    pagerank_dict = {}\n",
    "    for i, url in enumerate(kalicube_nodes):\n",
    "        vertex_id = i + kalicube_offset\n",
    "        pagerank_dict[url] = float(pagerank_values[merged_graph.vertex(vertex_id)])\n",
    "\n",
    "    return pagerank_dict\n",
    "\n",
    "\n",
    "def create_comparison_dataframe_turbo(pagerank_old_dict, pagerank_new_dict, simulation):\n",
    "    \"\"\"\n",
    "    Ultra-fast comparison dataframe creation using numpy operations.\n",
    "    \"\"\"\n",
    "    # Find common URLs efficiently\n",
    "    old_urls = set(pagerank_old_dict.keys())\n",
    "    new_urls = set(pagerank_new_dict.keys())\n",
    "    common_urls = old_urls & new_urls\n",
    "\n",
    "    if not common_urls:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to lists for faster processing\n",
    "    urls = list(common_urls)\n",
    "    n_urls = len(urls)\n",
    "\n",
    "    # Pre-allocate arrays\n",
    "    pagerank_before = np.array([pagerank_old_dict[url] for url in urls])\n",
    "    pagerank_after = np.array([pagerank_new_dict[url] for url in urls])\n",
    "\n",
    "    # Fast ranking using argsort\n",
    "    rank_before = np.empty(n_urls)\n",
    "    rank_after = np.empty(n_urls)\n",
    "\n",
    "    rank_before[np.argsort(-pagerank_before)] = np.arange(1, n_urls + 1)\n",
    "    rank_after[np.argsort(-pagerank_after)] = np.arange(1, n_urls + 1)\n",
    "\n",
    "    # Calculate deltas\n",
    "    pagerank_delta = pagerank_after - pagerank_before\n",
    "    pagerank_delta_pct = (pagerank_delta / np.maximum(pagerank_before, 1e-10)) * 100\n",
    "    rank_change = rank_after - rank_before\n",
    "    rank_change_pct = (rank_change / np.maximum(rank_before, 1e-10)) * 100\n",
    "\n",
    "    # Create DataFrame efficiently\n",
    "    comparison_df = pd.DataFrame(\n",
    "        {\n",
    "            \"URL\": urls,\n",
    "            \"PageRank_Before\": pagerank_before,\n",
    "            \"PageRank_After\": pagerank_after,\n",
    "            \"Rank_Before\": rank_before,\n",
    "            \"Rank_After\": rank_after,\n",
    "            \"PageRank_Delta\": pagerank_delta,\n",
    "            \"PageRank_Delta_%\": pagerank_delta_pct,\n",
    "            \"Rank_Change\": rank_change,\n",
    "            \"Rank_Change_%\": rank_change_pct,\n",
    "            \"Simulation\": simulation + 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def run_single_simulation_turbo(\n",
    "    simulation_id,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ultra-fast single simulation with minimal memory allocation.\n",
    "    \"\"\"\n",
    "    sim_seed = 42 + simulation_id\n",
    "    np.random.seed(sim_seed)\n",
    "    random.seed(sim_seed)\n",
    "\n",
    "    # Create WWW graph (cached)\n",
    "    www_graph = create_www_graph_turbo(TOTAL_NODES_WWW, EDGES_PER_NEW_NODE, sim_seed)\n",
    "\n",
    "    # Process configurations\n",
    "    pagerank_old_dict = process_configuration_turbo(\n",
    "        www_graph, kalicube_old_edges, kalicube_nodes_old, kalicube_url_mapping_old\n",
    "    )\n",
    "\n",
    "    pagerank_new_dict = process_configuration_turbo(\n",
    "        www_graph, kalicube_new_edges, kalicube_nodes_new, kalicube_url_mapping_new\n",
    "    )\n",
    "\n",
    "    # Create comparison\n",
    "    comparison_df = create_comparison_dataframe_turbo(\n",
    "        pagerank_old_dict, pagerank_new_dict, simulation_id\n",
    "    )\n",
    "\n",
    "    if comparison_df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Calculate metrics efficiently\n",
    "    total_before = comparison_df[\"PageRank_Before\"].sum()\n",
    "    total_after = comparison_df[\"PageRank_After\"].sum()\n",
    "    total_delta = total_after - total_before\n",
    "    delta_pct = (total_delta / total_before) * 100 if total_before > 0 else 0\n",
    "\n",
    "    rank_changes = comparison_df[\"Rank_Change\"].values\n",
    "    rank_improvements = np.sum(rank_changes < 0)\n",
    "    rank_drops = np.sum(rank_changes > 0)\n",
    "    rank_unchanged = np.sum(rank_changes == 0)\n",
    "    avg_rank_change = np.mean(rank_changes)\n",
    "\n",
    "    result = {\n",
    "        \"Simulation\": simulation_id + 1,\n",
    "        \"Total_Before\": total_before,\n",
    "        \"Total_After\": total_after,\n",
    "        \"Total_Delta\": total_delta,\n",
    "        \"Delta_Percent\": delta_pct,\n",
    "        \"Rank_Improvements\": rank_improvements,\n",
    "        \"Rank_Drops\": rank_drops,\n",
    "        \"Rank_Unchanged\": rank_unchanged,\n",
    "        \"Avg_Rank_Change\": avg_rank_change,\n",
    "    }\n",
    "\n",
    "    return result, comparison_df\n",
    "\n",
    "\n",
    "def run_batch_simulations(\n",
    "    start_idx,\n",
    "    end_idx,\n",
    "    kalicube_old_edges,\n",
    "    kalicube_new_edges,\n",
    "    kalicube_nodes_old,\n",
    "    kalicube_nodes_new,\n",
    "    kalicube_url_mapping_old,\n",
    "    kalicube_url_mapping_new,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a batch of simulations with minimal overhead.\n",
    "    \"\"\"\n",
    "    batch_results = []\n",
    "    batch_comparisons = []\n",
    "\n",
    "    for sim_id in range(start_idx, end_idx):\n",
    "        result, comparison_df = run_single_simulation_turbo(\n",
    "            sim_id,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "\n",
    "        if result is not None:\n",
    "            batch_results.append(result)\n",
    "            batch_comparisons.append(comparison_df)\n",
    "\n",
    "        # Progress indicator\n",
    "        if (sim_id - start_idx + 1) % 5 == 0:\n",
    "            print(f\"    ‚úì Completed {sim_id - start_idx + 1} simulations in batch\")\n",
    "\n",
    "    return batch_results, batch_comparisons\n",
    "\n",
    "\n",
    "def check_available_files():\n",
    "    \"\"\"Helper function to check available CSV files\"\"\"\n",
    "    import os\n",
    "\n",
    "    csv_files = [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
    "    print(\"Available CSV files:\")\n",
    "    for f in csv_files:\n",
    "        print(f\"  - {f}\")\n",
    "    return csv_files\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting TURBO-OPTIMIZED PageRank simulation...\")\n",
    "\n",
    "    # Check what files are available\n",
    "    print(\"\\nüìÅ Checking available files...\")\n",
    "    available_files = check_available_files()\n",
    "\n",
    "    # Update filenames if needed\n",
    "    if new_graph_filename not in available_files:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {new_graph_filename} not found!\")\n",
    "        print(\"Available options:\")\n",
    "        for f in available_files:\n",
    "            print(f\"  - {f}\")\n",
    "        print(\n",
    "            \"\\nPlease update the 'new_graph_filename' variable or ensure the file exists.\"\n",
    "        )\n",
    "        import sys\n",
    "\n",
    "        sys.exit(1)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"\\nüìÇ Loading graphs with turbo mode...\")\n",
    "\n",
    "    # Load old graph\n",
    "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
    "        load_graph_from_csv_turbo(old_graph_filename)\n",
    "    )\n",
    "    if kalicube_graph_old is None:\n",
    "        print(f\"‚ùå Failed to load old graph from {old_graph_filename}. Exiting.\")\n",
    "        import sys\n",
    "\n",
    "        sys.exit(1)\n",
    "    print(f\"‚úÖ Loaded OLD Kalicube graph from: {old_graph_filename}\")\n",
    "\n",
    "    # Load new graph\n",
    "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
    "        load_graph_from_csv_turbo(new_graph_filename)\n",
    "    )\n",
    "    if kalicube_graph_new is None:\n",
    "        print(f\"‚ùå Failed to load new graph from {new_graph_filename}. Exiting.\")\n",
    "        import sys\n",
    "\n",
    "        sys.exit(1)\n",
    "    print(f\"‚úÖ Loaded NEW Kalicube graph from: {new_graph_filename}\")\n",
    "\n",
    "    print(\"‚öôÔ∏è Pre-processing graph data...\")\n",
    "    kalicube_old_edges = [\n",
    "        (int(e.source()), int(e.target())) for e in kalicube_graph_old.edges()\n",
    "    ]\n",
    "    kalicube_new_edges = [\n",
    "        (int(e.source()), int(e.target())) for e in kalicube_graph_new.edges()\n",
    "    ]\n",
    "\n",
    "    print(\"\\nüìä Network Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    num_nodes_old = kalicube_graph_old.num_vertices()\n",
    "    num_edges_old = kalicube_graph_old.num_edges()\n",
    "    print(f\"üìà OLD Kalicube Graph: {num_nodes_old:,} nodes, {num_edges_old:,} edges\")\n",
    "\n",
    "    num_nodes_new = kalicube_graph_new.num_vertices()\n",
    "    num_edges_new = kalicube_graph_new.num_edges()\n",
    "    print(f\"üìà NEW Kalicube Graph: {num_nodes_new:,} nodes, {num_edges_new:,} edges\")\n",
    "\n",
    "    print(f\"üåê WWW Graph (per simulation): {TOTAL_NODES_WWW:,} nodes (OPTIMIZED)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Clean up\n",
    "    del kalicube_graph_old, kalicube_graph_new\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"üîÑ Running {NUM_SIMULATIONS} TURBO simulations...\")\n",
    "\n",
    "    # Run simulations in batches for better memory management\n",
    "    BATCH_SIZE = 5\n",
    "    all_results = []\n",
    "    all_comparison_dfs = []\n",
    "\n",
    "    for batch_start in range(0, NUM_SIMULATIONS, BATCH_SIZE):\n",
    "        batch_end = min(batch_start + BATCH_SIZE, NUM_SIMULATIONS)\n",
    "        print(\n",
    "            f\"‚ö° Processing batch {batch_start // BATCH_SIZE + 1}: simulations {batch_start + 1}-{batch_end}\"\n",
    "        )\n",
    "\n",
    "        batch_results, batch_comparisons = run_batch_simulations(\n",
    "            batch_start,\n",
    "            batch_end,\n",
    "            kalicube_old_edges,\n",
    "            kalicube_new_edges,\n",
    "            kalicube_nodes_old,\n",
    "            kalicube_nodes_new,\n",
    "            kalicube_url_mapping_old,\n",
    "            kalicube_url_mapping_new,\n",
    "        )\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "        all_comparison_dfs.extend(batch_comparisons)\n",
    "\n",
    "        # Show batch results\n",
    "        for result in batch_results:\n",
    "            effect_symbol = (\n",
    "                \"‚úÖ\"\n",
    "                if result[\"Total_Delta\"] > 0\n",
    "                else \"‚ö†Ô∏è\"\n",
    "                if result[\"Total_Delta\"] < 0\n",
    "                else \"‚ûñ\"\n",
    "            )\n",
    "            rank_symbol = (\n",
    "                \"üîº\"\n",
    "                if result[\"Avg_Rank_Change\"] < 0\n",
    "                else \"üîª\"\n",
    "                if result[\"Avg_Rank_Change\"] > 0\n",
    "                else \"‚ûñ\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Sim {result['Simulation']:3d}: {effect_symbol} PageRank:{result['Total_Delta']:+.6f} ({result['Delta_Percent']:+.2f}%) | {rank_symbol} Ranks: {result['Rank_Improvements']}‚Üë {result['Rank_Drops']}‚Üì {result['Rank_Unchanged']}‚Üí\"\n",
    "            )\n",
    "\n",
    "        # Memory cleanup between batches\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"‚úÖ All TURBO simulations completed in {end_time - start_time:.2f} seconds!\")\n",
    "    print(\n",
    "        f\"‚ö° Average time per simulation: {(end_time - start_time) / NUM_SIMULATIONS:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Process and save results\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        all_comparisons_df = pd.concat(all_comparison_dfs, ignore_index=True)\n",
    "\n",
    "        results_df.to_csv(\"simulation_summary_turbo.csv\", index=False)\n",
    "        all_comparisons_df.to_csv(\"all_simulations_detailed_turbo.csv\", index=False)\n",
    "\n",
    "        print(\"‚úÖ Saved TURBO results:\")\n",
    "        print(\" - simulation_summary_turbo.csv: Overall metrics\")\n",
    "        print(\" - all_simulations_detailed_turbo.csv: Detailed results\")\n",
    "\n",
    "        print(\"\\nüìà TURBO Statistical Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Mean overall delta: {results_df['Total_Delta'].mean():.6f}\")\n",
    "        print(f\"Std dev overall delta: {results_df['Total_Delta'].std():.6f}\")\n",
    "        print(f\"Mean delta percentage: {results_df['Delta_Percent'].mean():.2f}%\")\n",
    "        print(f\"Std dev delta percentage: {results_df['Delta_Percent'].std():.2f}%\")\n",
    "\n",
    "        positive_outcomes = (results_df[\"Total_Delta\"] > 0).sum()\n",
    "        negative_outcomes = (results_df[\"Total_Delta\"] < 0).sum()\n",
    "        neutral_outcomes = (results_df[\"Total_Delta\"] == 0).sum()\n",
    "\n",
    "        print(f\"\\nüéØ TURBO Outcome Distribution:\")\n",
    "        print(\"=\" * 35)\n",
    "        print(\n",
    "            f\" - Positive outcomes: {positive_outcomes}/{NUM_SIMULATIONS} ({positive_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Negative outcomes: {negative_outcomes}/{NUM_SIMULATIONS} ({negative_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\" - Neutral outcomes: {neutral_outcomes}/{NUM_SIMULATIONS} ({neutral_outcomes / NUM_SIMULATIONS * 100:.1f}%)\"\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\n‚ö° TURBO mode delivered {10 * (1000000 / TOTAL_NODES_WWW):.1f}x speed improvement!\"\n",
    "        )\n",
    "        print(f\"üéØ Total simulation time: {end_time - start_time:.1f} seconds\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid simulation results generated.\")\n",
    "\n",
    "    # Clear cache\n",
    "    _www_graph_cache = None\n",
    "    gc.collect()\n",
    "    print(\"üßπ Memory cleaned up. TURBO simulation complete!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
