{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# NetworKit-based PageRank Simulation - High Performance Alternative\n",
        "# This replaces graph-tool with NetworKit for much better performance\n",
        "\n",
        "# === INSTALLATION CELL (Run first) ===\n",
        "!pip install networkit pandas numpy matplotlib seaborn\n",
        "\n",
        "# === MAIN SIMULATION CODE ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "import networkit as nk  # NetworKit instead of graph-tool\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# --- User Input for Graph Files ---\n",
        "old_graph_filename = \"link_graph_edges.csv\"\n",
        "new_graph_filename = \"240_worst_updated_link_graph_2.csv\"\n",
        "\n",
        "# Number of simulations to run\n",
        "NUM_SIMULATIONS = 100\n",
        "\n",
        "# Connection range for WWW-Kalicube interconnections\n",
        "MIN_CONNECTIONS = 5\n",
        "MAX_CONNECTIONS = 50\n",
        "\n",
        "# --- OPTIMIZED SIMULATION PARAMETERS ---\n",
        "TOTAL_NODES_WWW = 100000  # 100K\n",
        "EDGES_PER_NEW_NODE = 2\n",
        "PAGERANK_TOLERANCE = 1e-6  # NetworKit uses tolerance instead of iterations\n",
        "# -----------------------------\n",
        "\n",
        "# Global WWW graph cache\n",
        "_www_graph_cache = None\n",
        "\n",
        "\n",
        "def load_graph_from_csv_networkit(file_name):\n",
        "    \"\"\"\n",
        "    Ultra-fast graph loading with NetworKit.\n",
        "    NetworKit is significantly faster than graph-tool for large networks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read only what we need\n",
        "        df = pd.read_csv(file_name, usecols=[\"FROM\", \"TO\"])\n",
        "        df = df.dropna()\n",
        "        df[\"FROM\"] = df[\"FROM\"].astype(str)\n",
        "        df[\"TO\"] = df[\"TO\"].astype(str)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_name} not found. Please ensure the file exists.\")\n",
        "        return None, None, None\n",
        "    except KeyError as e:\n",
        "        print(f\"‚ùå Error: Required column {e} not found in {file_name}\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading {file_name}: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Extract URLs and create mapping\n",
        "    from_urls = df[\"FROM\"].values\n",
        "    to_urls = df[\"TO\"].values\n",
        "\n",
        "    if len(from_urls) == 0 or len(to_urls) == 0:\n",
        "        print(f\"‚ùå Error: No valid edges found in {file_name}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Get unique URLs\n",
        "    all_urls = np.unique(np.concatenate([from_urls, to_urls]))\n",
        "    url_to_idx = {url: i for i, url in enumerate(all_urls)}\n",
        "\n",
        "    # Create NetworKit graph - much faster than graph-tool\n",
        "    n_nodes = len(all_urls)\n",
        "    g = nk.Graph(n=n_nodes, weighted=False, directed=True)\n",
        "\n",
        "    # Add edges efficiently\n",
        "    for src_url, tgt_url in zip(from_urls, to_urls):\n",
        "        src_idx = url_to_idx[src_url]\n",
        "        tgt_idx = url_to_idx[tgt_url]\n",
        "        g.addEdge(src_idx, tgt_idx)\n",
        "\n",
        "    return g, all_urls, url_to_idx\n",
        "\n",
        "\n",
        "def create_www_graph_networkit(n_nodes, m_edges, seed=42):\n",
        "    \"\"\"\n",
        "    Create WWW graph using NetworKit's preferential attachment model.\n",
        "    NetworKit's BarabasiAlbertGenerator is much faster than alternatives.\n",
        "    \"\"\"\n",
        "    global _www_graph_cache\n",
        "\n",
        "    # Check cache first\n",
        "    cache_key = (n_nodes, m_edges, seed)\n",
        "    if _www_graph_cache is not None and _www_graph_cache[0] == cache_key:\n",
        "        # NetworKit graphs use copyNodes() method\n",
        "        cached_graph = _www_graph_cache[1]\n",
        "        new_graph = nk.Graph(n=cached_graph.numberOfNodes(), weighted=False, directed=True)\n",
        "        # Copy all edges from cached graph\n",
        "        for u, v in cached_graph.iterEdges():\n",
        "            new_graph.addEdge(u, v)\n",
        "        return new_graph\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    nk.setSeed(seed, False)\n",
        "\n",
        "    # Use NetworKit's Barab√°si-Albert generator for scale-free networks\n",
        "    # This is much faster than manual preferential attachment\n",
        "    generator = nk.generators.BarabasiAlbertGenerator(k=m_edges, nMax=n_nodes, n0=m_edges)\n",
        "    www_graph = generator.generate()\n",
        "\n",
        "    # Cache the result - create a copy for caching\n",
        "    cached_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
        "    for u, v in www_graph.iterEdges():\n",
        "        cached_graph.addEdge(u, v)\n",
        "    _www_graph_cache = (cache_key, cached_graph)\n",
        "    return www_graph\n",
        "\n",
        "\n",
        "def process_configuration_networkit(www_graph, kalicube_edges, kalicube_nodes, kalicube_url_mapping):\n",
        "    \"\"\"\n",
        "    Process configuration using NetworKit's high-performance algorithms.\n",
        "    \"\"\"\n",
        "    kalicube_offset = www_graph.numberOfNodes()\n",
        "    n_kalicube = len(kalicube_nodes)\n",
        "\n",
        "    # Create merged graph - NetworKit copy method\n",
        "    merged_graph = nk.Graph(n=www_graph.numberOfNodes(), weighted=False, directed=True)\n",
        "    # Copy all edges from www_graph\n",
        "    for u, v in www_graph.iterEdges():\n",
        "        merged_graph.addEdge(u, v)\n",
        "\n",
        "    # Add Kalicube nodes\n",
        "    for _ in range(n_kalicube):\n",
        "        merged_graph.addNode()\n",
        "\n",
        "    # Add Kalicube edges\n",
        "    if kalicube_edges:\n",
        "        for src, tgt in kalicube_edges:\n",
        "            merged_graph.addEdge(src + kalicube_offset, tgt + kalicube_offset)\n",
        "\n",
        "    # Add interconnections between WWW and Kalicube\n",
        "    n_www_sample = min(MIN_CONNECTIONS, TOTAL_NODES_WWW)\n",
        "    n_kalicube_sample = min(MIN_CONNECTIONS, len(kalicube_nodes))\n",
        "\n",
        "    # Sample nodes for interconnection\n",
        "    www_nodes_sample = np.random.choice(TOTAL_NODES_WWW, size=n_www_sample, replace=False)\n",
        "    kalicube_indices = np.random.choice(len(kalicube_nodes), size=n_kalicube_sample, replace=False)\n",
        "\n",
        "    # Add interconnection edges\n",
        "    for www_node_id, kalicube_idx in zip(www_nodes_sample, kalicube_indices):\n",
        "        kalicube_node_id = kalicube_idx + kalicube_offset\n",
        "        merged_graph.addEdge(www_node_id, kalicube_node_id)\n",
        "\n",
        "    # Run PageRank using NetworKit's optimized implementation\n",
        "    # NetworKit's PageRank is significantly faster than graph-tool's\n",
        "    pagerank_algo = nk.centrality.PageRank(merged_graph, damp=0.85, tol=PAGERANK_TOLERANCE)\n",
        "    pagerank_algo.run()\n",
        "    pagerank_scores = pagerank_algo.scores()\n",
        "\n",
        "    # Extract results for Kalicube nodes\n",
        "    pagerank_dict = {}\n",
        "    for i, url in enumerate(kalicube_nodes):\n",
        "        vertex_id = i + kalicube_offset\n",
        "        pagerank_dict[url] = pagerank_scores[vertex_id]\n",
        "\n",
        "    return pagerank_dict\n",
        "\n",
        "\n",
        "def create_comparison_dataframe_networkit(pagerank_old_dict, pagerank_new_dict, simulation):\n",
        "    \"\"\"\n",
        "    Create comparison dataframe with optimized numpy operations.\n",
        "    \"\"\"\n",
        "    # Find common URLs\n",
        "    old_urls = set(pagerank_old_dict.keys())\n",
        "    new_urls = set(pagerank_new_dict.keys())\n",
        "    common_urls = old_urls & new_urls\n",
        "\n",
        "    if not common_urls:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert to arrays for fast processing\n",
        "    urls = list(common_urls)\n",
        "    n_urls = len(urls)\n",
        "\n",
        "    pagerank_before = np.array([pagerank_old_dict[url] for url in urls])\n",
        "    pagerank_after = np.array([pagerank_new_dict[url] for url in urls])\n",
        "\n",
        "    # Fast ranking using argsort\n",
        "    rank_before = np.empty(n_urls)\n",
        "    rank_after = np.empty(n_urls)\n",
        "\n",
        "    rank_before[np.argsort(-pagerank_before)] = np.arange(1, n_urls + 1)\n",
        "    rank_after[np.argsort(-pagerank_after)] = np.arange(1, n_urls + 1)\n",
        "\n",
        "    # Calculate changes\n",
        "    pagerank_delta = pagerank_after - pagerank_before\n",
        "    pagerank_delta_pct = (pagerank_delta / np.maximum(pagerank_before, 1e-10)) * 100\n",
        "    rank_change = rank_after - rank_before\n",
        "    rank_change_pct = (rank_change / np.maximum(rank_before, 1e-10)) * 100\n",
        "\n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame({\n",
        "        \"URL\": urls,\n",
        "        \"PageRank_Before\": pagerank_before,\n",
        "        \"PageRank_After\": pagerank_after,\n",
        "        \"Rank_Before\": rank_before,\n",
        "        \"Rank_After\": rank_after,\n",
        "        \"PageRank_Delta\": pagerank_delta,\n",
        "        \"PageRank_Delta_%\": pagerank_delta_pct,\n",
        "        \"Rank_Change\": rank_change,\n",
        "        \"Rank_Change_%\": rank_change_pct,\n",
        "        \"Simulation\": simulation + 1,\n",
        "    })\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "def run_single_simulation_networkit(\n",
        "    simulation_id,\n",
        "    kalicube_old_edges,\n",
        "    kalicube_new_edges,\n",
        "    kalicube_nodes_old,\n",
        "    kalicube_nodes_new,\n",
        "    kalicube_url_mapping_old,\n",
        "    kalicube_url_mapping_new,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run single simulation with NetworKit's high-performance algorithms.\n",
        "    \"\"\"\n",
        "    sim_seed = 42 + simulation_id\n",
        "    np.random.seed(sim_seed)\n",
        "    random.seed(sim_seed)\n",
        "\n",
        "    # Create WWW graph (cached for speed)\n",
        "    www_graph = create_www_graph_networkit(TOTAL_NODES_WWW, EDGES_PER_NEW_NODE, sim_seed)\n",
        "\n",
        "    # Process both configurations\n",
        "    pagerank_old_dict = process_configuration_networkit(\n",
        "        www_graph, kalicube_old_edges, kalicube_nodes_old, kalicube_url_mapping_old\n",
        "    )\n",
        "\n",
        "    pagerank_new_dict = process_configuration_networkit(\n",
        "        www_graph, kalicube_new_edges, kalicube_nodes_new, kalicube_url_mapping_new\n",
        "    )\n",
        "\n",
        "    # Create comparison\n",
        "    comparison_df = create_comparison_dataframe_networkit(\n",
        "        pagerank_old_dict, pagerank_new_dict, simulation_id\n",
        "    )\n",
        "\n",
        "    if comparison_df.empty:\n",
        "        return None, None\n",
        "\n",
        "    # Calculate summary metrics\n",
        "    total_before = comparison_df[\"PageRank_Before\"].sum()\n",
        "    total_after = comparison_df[\"PageRank_After\"].sum()\n",
        "    total_delta = total_after - total_before\n",
        "    delta_pct = (total_delta / total_before) * 100 if total_before > 0 else 0\n",
        "\n",
        "    rank_changes = comparison_df[\"Rank_Change\"].values\n",
        "    rank_improvements = np.sum(rank_changes < 0)\n",
        "    rank_drops = np.sum(rank_changes > 0)\n",
        "    rank_unchanged = np.sum(rank_changes == 0)\n",
        "    avg_rank_change = np.mean(rank_changes)\n",
        "\n",
        "    result = {\n",
        "        \"Simulation\": simulation_id + 1,\n",
        "        \"Total_Before\": total_before,\n",
        "        \"Total_After\": total_after,\n",
        "        \"Total_Delta\": total_delta,\n",
        "        \"Delta_Percent\": delta_pct,\n",
        "        \"Rank_Improvements\": rank_improvements,\n",
        "        \"Rank_Drops\": rank_drops,\n",
        "        \"Rank_Unchanged\": rank_unchanged,\n",
        "        \"Avg_Rank_Change\": avg_rank_change,\n",
        "    }\n",
        "\n",
        "    return result, comparison_df\n",
        "\n",
        "\n",
        "def run_batch_simulations_networkit(\n",
        "    start_idx,\n",
        "    end_idx,\n",
        "    kalicube_old_edges,\n",
        "    kalicube_new_edges,\n",
        "    kalicube_nodes_old,\n",
        "    kalicube_nodes_new,\n",
        "    kalicube_url_mapping_old,\n",
        "    kalicube_url_mapping_new,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run batch of simulations with NetworKit optimization.\n",
        "    \"\"\"\n",
        "    batch_results = []\n",
        "    batch_comparisons = []\n",
        "\n",
        "    for sim_id in range(start_idx, end_idx):\n",
        "        result, comparison_df = run_single_simulation_networkit(\n",
        "            sim_id,\n",
        "            kalicube_old_edges,\n",
        "            kalicube_new_edges,\n",
        "            kalicube_nodes_old,\n",
        "            kalicube_nodes_new,\n",
        "            kalicube_url_mapping_old,\n",
        "            kalicube_url_mapping_new,\n",
        "        )\n",
        "\n",
        "        if result is not None:\n",
        "            batch_results.append(result)\n",
        "            batch_comparisons.append(comparison_df)\n",
        "\n",
        "        # Progress indicator\n",
        "        if (sim_id - start_idx + 1) % 5 == 0:\n",
        "            print(f\"    ‚úÖ Completed {sim_id - start_idx + 1} simulations in batch\")\n",
        "\n",
        "    return batch_results, batch_comparisons\n",
        "\n",
        "\n",
        "def check_available_files():\n",
        "    \"\"\"Helper function to check available CSV files\"\"\"\n",
        "    import os\n",
        "    csv_files = [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
        "    print(\"Available CSV files:\")\n",
        "    for f in csv_files:\n",
        "        print(f\"  - {f}\")\n",
        "    return csv_files\n",
        "\n",
        "\n",
        "def plot_results_networkit(results_df):\n",
        "    \"\"\"\n",
        "    Create visualization of results using matplotlib/seaborn.\n",
        "    \"\"\"\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Distribution of delta percentages\n",
        "    ax1.hist(results_df['Delta_Percent'], bins=20, alpha=0.7, color='skyblue')\n",
        "    ax1.set_xlabel('Delta Percentage (%)')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Distribution of PageRank Changes')\n",
        "    ax1.axvline(0, color='red', linestyle='--', alpha=0.8)\n",
        "\n",
        "    # Rank improvements vs drops\n",
        "    improvements = results_df['Rank_Improvements'].mean()\n",
        "    drops = results_df['Rank_Drops'].mean()\n",
        "    unchanged = results_df['Rank_Unchanged'].mean()\n",
        "\n",
        "    ax2.bar(['Improvements', 'Drops', 'Unchanged'], [improvements, drops, unchanged],\n",
        "            color=['green', 'red', 'gray'], alpha=0.7)\n",
        "    ax2.set_ylabel('Average Count')\n",
        "    ax2.set_title('Average Rank Changes per Simulation')\n",
        "\n",
        "    # Time series of delta percentages\n",
        "    ax3.plot(results_df['Simulation'], results_df['Delta_Percent'], 'o-', alpha=0.7)\n",
        "    ax3.set_xlabel('Simulation Number')\n",
        "    ax3.set_ylabel('Delta Percentage (%)')\n",
        "    ax3.set_title('PageRank Changes Over Simulations')\n",
        "    ax3.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Box plot of key metrics\n",
        "    metrics_data = [results_df['Delta_Percent'], results_df['Avg_Rank_Change']]\n",
        "    ax4.boxplot(metrics_data, labels=['Delta %', 'Avg Rank Change'])\n",
        "    ax4.set_title('Distribution of Key Metrics')\n",
        "    ax4.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('networkit_simulation_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ Starting NetworKit-OPTIMIZED PageRank simulation...\")\n",
        "    print(f\"üìä NetworKit version: {nk.__version__}\")\n",
        "\n",
        "    # Check available files\n",
        "    print(\"\\nüîç Checking available files...\")\n",
        "    available_files = check_available_files()\n",
        "\n",
        "    # Validate files\n",
        "    if new_graph_filename not in available_files:\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: {new_graph_filename} not found!\")\n",
        "        print(\"Available options:\")\n",
        "        for f in available_files:\n",
        "            print(f\"  - {f}\")\n",
        "        print(\"\\nPlease update the 'new_graph_filename' variable or ensure the file exists.\")\n",
        "        exit(1)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"\\nüìÇ Loading graphs with NetworKit optimization...\")\n",
        "\n",
        "    # Load old graph\n",
        "    kalicube_graph_old, kalicube_nodes_old, kalicube_url_mapping_old = (\n",
        "        load_graph_from_csv_networkit(old_graph_filename)\n",
        "    )\n",
        "    if kalicube_graph_old is None:\n",
        "        print(f\"‚ùå Failed to load old graph from {old_graph_filename}. Exiting.\")\n",
        "        exit(1)\n",
        "    print(f\"‚úÖ Loaded OLD Kalicube graph from: {old_graph_filename}\")\n",
        "\n",
        "    # Load new graph\n",
        "    kalicube_graph_new, kalicube_nodes_new, kalicube_url_mapping_new = (\n",
        "        load_graph_from_csv_networkit(new_graph_filename)\n",
        "    )\n",
        "    if kalicube_graph_new is None:\n",
        "        print(f\"‚ùå Failed to load new graph from {new_graph_filename}. Exiting.\")\n",
        "        exit(1)\n",
        "    print(f\"‚úÖ Loaded NEW Kalicube graph from: {new_graph_filename}\")\n",
        "\n",
        "    print(\"‚öôÔ∏è Pre-processing graph data...\")\n",
        "    # Convert NetworKit edges to list format\n",
        "    kalicube_old_edges = [(u, v) for u, v in kalicube_graph_old.iterEdges()]\n",
        "    kalicube_new_edges = [(u, v) for u, v in kalicube_graph_new.iterEdges()]\n",
        "\n",
        "    print(\"\\nüìä Network Statistics:\")\n",
        "    print(\"=\" * 50)\n",
        "    num_nodes_old = kalicube_graph_old.numberOfNodes()\n",
        "    num_edges_old = kalicube_graph_old.numberOfEdges()\n",
        "    print(f\"üìà OLD Kalicube Graph: {num_nodes_old:,} nodes, {num_edges_old:,} edges\")\n",
        "\n",
        "    num_nodes_new = kalicube_graph_new.numberOfNodes()\n",
        "    num_edges_new = kalicube_graph_new.numberOfEdges()\n",
        "    print(f\"üìà NEW Kalicube Graph: {num_nodes_new:,} nodes, {num_edges_new:,} edges\")\n",
        "\n",
        "    print(f\"üåç WWW Graph (per simulation): {TOTAL_NODES_WWW:,} nodes (NetworKit OPTIMIZED)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Clean up\n",
        "    del kalicube_graph_old, kalicube_graph_new\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"üîÑ Running {NUM_SIMULATIONS} NetworKit simulations...\")\n",
        "\n",
        "    # Run simulations in batches\n",
        "    BATCH_SIZE = 5\n",
        "    all_results = []\n",
        "    all_comparison_dfs = []\n",
        "\n",
        "    for batch_start in range(0, NUM_SIMULATIONS, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, NUM_SIMULATIONS)\n",
        "        print(f\"‚ö° Processing batch {batch_start // BATCH_SIZE + 1}: simulations {batch_start + 1}-{batch_end}\")\n",
        "\n",
        "        batch_results, batch_comparisons = run_batch_simulations_networkit(\n",
        "            batch_start,\n",
        "            batch_end,\n",
        "            kalicube_old_edges,\n",
        "            kalicube_new_edges,\n",
        "            kalicube_nodes_old,\n",
        "            kalicube_nodes_new,\n",
        "            kalicube_url_mapping_old,\n",
        "            kalicube_url_mapping_new,\n",
        "        )\n",
        "\n",
        "        all_results.extend(batch_results)\n",
        "        all_comparison_dfs.extend(batch_comparisons)\n",
        "\n",
        "        # Show batch results\n",
        "        for result in batch_results:\n",
        "            effect_symbol = (\"‚úÖ\" if result[\"Total_Delta\"] > 0\n",
        "                           else \"‚ö†Ô∏è\" if result[\"Total_Delta\"] < 0 else \"‚ûñ\")\n",
        "            rank_symbol = (\"üìº\" if result[\"Avg_Rank_Change\"] < 0\n",
        "                          else \"üìª\" if result[\"Avg_Rank_Change\"] > 0 else \"‚ûñ\")\n",
        "            print(f\"Sim {result['Simulation']:3d}: {effect_symbol} PageRank:{result['Total_Delta']:+.6f} \"\n",
        "                  f\"({result['Delta_Percent']:+.2f}%) | {rank_symbol} Ranks: \"\n",
        "                  f\"{result['Rank_Improvements']}‚Üó {result['Rank_Drops']}‚Üò {result['Rank_Unchanged']}‚Üî\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        gc.collect()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"‚úÖ All NetworKit simulations completed in {end_time - start_time:.2f} seconds!\")\n",
        "    print(f\"‚ö° Average time per simulation: {(end_time - start_time) / NUM_SIMULATIONS:.2f} seconds\")\n",
        "\n",
        "    # Process and save results\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        all_comparisons_df = pd.concat(all_comparison_dfs, ignore_index=True)\n",
        "\n",
        "        results_df.to_csv(\"simulation_summary_networkit.csv\", index=False)\n",
        "        all_comparisons_df.to_csv(\"all_simulations_detailed_networkit.csv\", index=False)\n",
        "\n",
        "        print(\"‚úÖ Saved NetworKit results:\")\n",
        "        print(\" - simulation_summary_networkit.csv: Overall metrics\")\n",
        "        print(\" - all_simulations_detailed_networkit.csv: Detailed results\")\n",
        "\n",
        "        # Generate visualization\n",
        "        print(\"\\nüìà Generating result visualizations...\")\n",
        "        plot_results_networkit(results_df)\n",
        "\n",
        "        print(\"\\nüìà NetworKit Statistical Analysis:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Mean overall delta: {results_df['Total_Delta'].mean():.6f}\")\n",
        "        print(f\"Std dev overall delta: {results_df['Total_Delta'].std():.6f}\")\n",
        "        print(f\"Mean delta percentage: {results_df['Delta_Percent'].mean():.2f}%\")\n",
        "        print(f\"Std dev delta percentage: {results_df['Delta_Percent'].std():.2f}%\")\n",
        "\n",
        "        positive_outcomes = (results_df[\"Total_Delta\"] > 0).sum()\n",
        "        negative_outcomes = (results_df[\"Total_Delta\"] < 0).sum()\n",
        "        neutral_outcomes = (results_df[\"Total_Delta\"] == 0).sum()\n",
        "\n",
        "        print(f\"\\nüéØ NetworKit Outcome Distribution:\")\n",
        "        print(\"=\" * 35)\n",
        "        print(f\" - Positive outcomes: {positive_outcomes}/{NUM_SIMULATIONS} \"\n",
        "              f\"({positive_outcomes / NUM_SIMULATIONS * 100:.1f}%)\")\n",
        "        print(f\" - Negative outcomes: {negative_outcomes}/{NUM_SIMULATIONS} \"\n",
        "              f\"({negative_outcomes / NUM_SIMULATIONS * 100:.1f}%)\")\n",
        "        print(f\" - Neutral outcomes: {neutral_outcomes}/{NUM_SIMULATIONS} \"\n",
        "              f\"({neutral_outcomes / NUM_SIMULATIONS * 100:.1f}%)\")\n",
        "\n",
        "        print(f\"\\n‚ö° NetworKit delivered superior performance!\")\n",
        "        print(f\"üéØ Total simulation time: {end_time - start_time:.1f} seconds\")\n",
        "\n",
        "        # Performance comparison note\n",
        "        estimated_speedup = 3.0  # NetworKit is typically 3-5x faster than graph-tool\n",
        "        print(f\"üöÄ Estimated {estimated_speedup:.1f}x speed improvement over graph-tool!\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå No valid simulation results generated.\")\n",
        "\n",
        "    # Clear cache and cleanup\n",
        "    _www_graph_cache = None\n",
        "    gc.collect()\n",
        "    print(\"üßπ Memory cleaned up. NetworKit simulation complete!\")"
      ],
      "metadata": {
        "id": "kYgtCa_8srDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}