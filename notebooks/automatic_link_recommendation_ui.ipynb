{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYL7OLjWanv_",
    "outputId": "6bd1a16b-837f-4ed5-b81f-813cce70112b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m436.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cAanZXMGaSGF",
    "outputId": "c46b3eb4-99b2-4595-f508-b2f5a92bdfb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "Mounted at /content/drive\n",
      "INFO: Loading trained artifacts for recommendations...\n",
      "INFO: Artifacts loaded successfully.\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "INFO: Artifacts already loaded.\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "* Running on public URL: https://cfd0b3dd0ce74ffbf0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n",
      "INFO: Artifacts already loaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cfd0b3dd0ce74ffbf0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Artifacts already loaded.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import tempfile  # Import tempfile for creating temporary files\n",
    "import sys  # Import sys to modify Python path\n",
    "import abc  # Import abc for abstract base classes\n",
    "from urllib.parse import urlparse  # Added for URLProcessor's concrete implementation\n",
    "import logging  # Added for URLProcessor's concrete implementation\n",
    "\n",
    "# --- Google Colab Drive Mounting ---\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = \"/content/drive/My Drive/WebKnoGraph\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Import Real Classes from WebKnoGraph Project ---\n",
    "# This block will now strictly import your actual classes.\n",
    "# If any of these imports fail, the script will stop.\n",
    "import torch\n",
    "import json\n",
    "from src.backend.config.link_prediction_config import LinkPredictionConfig\n",
    "from src.backend.models.graph_models import GraphSAGEModel\n",
    "from src.shared.interfaces import (\n",
    "    ILogger as OriginalILogger,\n",
    ")  # Alias to avoid name conflict\n",
    "\n",
    "\n",
    "# Define a concrete ConsoleLogger that implements OriginalILogger\n",
    "class ConsoleLogger(OriginalILogger):\n",
    "    def info(self, message: str):\n",
    "        print(f\"INFO: {message}\")\n",
    "\n",
    "    def error(self, message: str):\n",
    "        print(f\"ERROR: {message}\")\n",
    "\n",
    "    def debug(self, message: str):\n",
    "        print(f\"DEBUG: {message}\")\n",
    "\n",
    "    def warning(self, message: str):\n",
    "        print(f\"WARNING: {message}\")\n",
    "\n",
    "    def exception(self, message: str):\n",
    "        print(f\"EXCEPTION: {message}\")\n",
    "\n",
    "\n",
    "# Use this concrete logger as the ILogger for the application\n",
    "ILogger = ConsoleLogger\n",
    "\n",
    "# Import the real URLProcessor\n",
    "from src.backend.utils.url_processing import URLProcessor\n",
    "\n",
    "\n",
    "# --- Real RecommendationEngine Class (as provided by user) ---\n",
    "class RecommendationEngine:\n",
    "    \"\"\"Loads trained artifacts and provides link recommendations using a Top-K strategy.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: LinkPredictionConfig, logger: ILogger, url_processor: URLProcessor\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.url_processor = url_processor\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.node_embeddings = None\n",
    "        self.url_to_idx = None\n",
    "        self.idx_to_url = None\n",
    "        self.existing_edges = None\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        \"\"\"Loads the trained model, embeddings, and mappings into memory.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.logger.info(\"Artifacts already loaded.\")\n",
    "            return True\n",
    "\n",
    "        self.logger.info(\"Loading trained artifacts for recommendations...\")\n",
    "        try:\n",
    "            # Ensure the directory exists before attempting to open files\n",
    "            model_dir = os.path.dirname(self.config.node_mapping_path)\n",
    "            if not os.path.exists(model_dir):\n",
    "                raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "\n",
    "            with open(self.config.node_mapping_path, \"r\") as f:\n",
    "                model_metadata = json.load(f)\n",
    "\n",
    "            self.url_to_idx = model_metadata[\"url_to_idx\"]\n",
    "            in_channels = model_metadata[\"in_channels\"]\n",
    "            hidden_channels = model_metadata[\"hidden_channels\"]\n",
    "            out_channels = model_metadata[\"out_channels\"]\n",
    "\n",
    "            self.idx_to_url = {v: k for k, v in self.url_to_idx.items()}\n",
    "\n",
    "            self.node_embeddings = torch.load(\n",
    "                self.config.node_embeddings_path, map_location=self.device\n",
    "            ).to(self.device)\n",
    "            edge_index = torch.load(\n",
    "                self.config.edge_index_path, map_location=self.device\n",
    "            )\n",
    "            self.existing_edges = set(\n",
    "                zip(edge_index[0].tolist(), edge_index[1].tolist())\n",
    "            )\n",
    "\n",
    "            self.model = GraphSAGEModel(in_channels, hidden_channels, out_channels)\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.model_state_path, map_location=self.device)\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            self.logger.info(\"Artifacts loaded successfully.\")\n",
    "            return True\n",
    "        except FileNotFoundError as fnf_e:\n",
    "            self.logger.error(\n",
    "                f\"Could not find trained model artifacts. Please run the training pipeline first. Error: {fnf_e}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"DEBUG: FileNotFoundError during artifact loading: {fnf_e}\"\n",
    "            )  # Added for debugging\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while loading artifacts: {e}\")\n",
    "            print(\n",
    "                f\"DEBUG: General Exception during artifact loading: {e}\"\n",
    "            )  # Added for debugging\n",
    "            # Re-raise for debugging if needed, but for Gradio, returning False is often better\n",
    "            # raise\n",
    "            return False\n",
    "\n",
    "    def get_recommendations(\n",
    "        self,\n",
    "        source_url: str,\n",
    "        top_n: int = 20,\n",
    "        min_folder_depth: int = 0,\n",
    "        max_folder_depth: int = 10,\n",
    "        folder_path_filter: str = None,\n",
    "    ):\n",
    "        # The load_artifacts call is crucial here. If it returns False, we return None.\n",
    "        if not self.load_artifacts():\n",
    "            return (\n",
    "                None,\n",
    "                \"Error: Trained model artifacts not found. Please run the training pipeline first.\",\n",
    "            )\n",
    "        if source_url not in self.url_to_idx:\n",
    "            return (\n",
    "                None,\n",
    "                f\"Error: Source URL '{source_url}' not found in the graph's training data.\",\n",
    "            )\n",
    "\n",
    "        source_idx = self.url_to_idx[source_url]\n",
    "        num_nodes = len(self.url_to_idx)\n",
    "\n",
    "        # 1. Generate scores for all possible links from the source node\n",
    "        candidate_dest_indices = torch.arange(num_nodes, device=self.device)\n",
    "        candidate_source_indices = torch.full_like(\n",
    "            candidate_dest_indices, fill_value=source_idx\n",
    "        )\n",
    "        candidate_edge_index = torch.stack(\n",
    "            [candidate_source_indices, candidate_dest_indices]\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = self.model.predict_link(self.node_embeddings, candidate_edge_index)\n",
    "\n",
    "        # 2. Create a DataFrame from all possible candidates\n",
    "        all_candidates_df = pd.DataFrame(\n",
    "            {\n",
    "                \"DEST_IDX\": candidate_dest_indices.cpu().numpy(),\n",
    "                \"SCORE\": torch.sigmoid(scores).cpu().numpy(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 3. Add URL and FOLDER_DEPTH columns\n",
    "        # Use .get() with a default value to handle missing keys and prevent KeyError\n",
    "        all_candidates_df[\"RECOMMENDED_URL\"] = all_candidates_df[\"DEST_IDX\"].apply(\n",
    "            lambda idx: self.idx_to_url.get(idx, None)\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid URLs (where index was not found in mapping)\n",
    "        all_candidates_df.dropna(subset=[\"RECOMMENDED_URL\"], inplace=True)\n",
    "\n",
    "        all_candidates_df[\"FOLDER_DEPTH\"] = all_candidates_df[\"RECOMMENDED_URL\"].apply(\n",
    "            lambda url: self.url_processor.get_folder_depth(url)\n",
    "        )\n",
    "\n",
    "        # 4. Filter the DataFrame based on all criteria\n",
    "        filtered_df = all_candidates_df.copy()\n",
    "\n",
    "        # Filter out self-links\n",
    "        filtered_df = filtered_df[filtered_df[\"DEST_IDX\"] != source_idx]\n",
    "\n",
    "        # Filter out existing links\n",
    "        # Create a tuple column for easy set membership check\n",
    "        filtered_df[\"SOURCE_IDX\"] = source_idx\n",
    "        filtered_df[\"EDGE_TUPLE\"] = list(\n",
    "            zip(filtered_df[\"SOURCE_IDX\"], filtered_df[\"DEST_IDX\"])\n",
    "        )\n",
    "        filtered_df = filtered_df[~filtered_df[\"EDGE_TUPLE\"].isin(self.existing_edges)]\n",
    "\n",
    "        # Apply the folder depth filter\n",
    "        filtered_df = filtered_df[\n",
    "            (filtered_df[\"FOLDER_DEPTH\"] >= min_folder_depth)\n",
    "            & (filtered_df[\"FOLDER_DEPTH\"] <= max_folder_depth)\n",
    "        ]\n",
    "\n",
    "        # Apply the folder path filter if provided\n",
    "        if folder_path_filter:\n",
    "            self.logger.info(f\"Applying folder path filter: {folder_path_filter}\")\n",
    "            filtered_df = filtered_df[\n",
    "                filtered_df[\"RECOMMENDED_URL\"].str.startswith(folder_path_filter)\n",
    "            ]\n",
    "\n",
    "        # 5. Sort the filtered DataFrame by score and take the top N\n",
    "        final_recommendations_df = filtered_df.sort_values(\n",
    "            by=\"SCORE\", ascending=False\n",
    "        ).head(top_n)\n",
    "\n",
    "        # 6. Select the final columns and return\n",
    "        final_recommendations_df = final_recommendations_df[\n",
    "            [\"RECOMMENDED_URL\", \"SCORE\", \"FOLDER_DEPTH\"]\n",
    "        ]\n",
    "\n",
    "        if final_recommendations_df.empty:\n",
    "            return (\n",
    "                pd.DataFrame(),  # Return empty DataFrame for consistency\n",
    "                \"No recommendations found matching the criteria (filters, existing links, etc.). Try adjusting filters or source URL.\",\n",
    "            )\n",
    "\n",
    "        return final_recommendations_df, None\n",
    "\n",
    "\n",
    "# --- Gradio Application Logic ---\n",
    "\n",
    "# Instantiate real classes\n",
    "logger = ILogger()  # Now instantiates the concrete ConsoleLogger\n",
    "url_processor = URLProcessor()\n",
    "config = LinkPredictionConfig()  # This will now use the updated paths\n",
    "recommendation_engine = RecommendationEngine(config, logger, url_processor)\n",
    "\n",
    "\n",
    "def process_csv_for_recommendations(csv_file, min_depth: int, max_depth: int):\n",
    "    \"\"\"\n",
    "    Gradio function to process the uploaded CSV and generate recommendations.\n",
    "    Returns the DataFrame for display and the path to the saved CSV for download.\n",
    "    \"\"\"\n",
    "    # Define default empty DataFrame and file path for error cases\n",
    "    empty_df = pd.DataFrame(\n",
    "        [[\"\", \"\", \"\", \"\", \"\", \"Please upload a CSV file.\"]],\n",
    "        columns=[\n",
    "            \"NEW_FROM\",\n",
    "            \"NEW_FROM_DEPTH\",\n",
    "            \"NEW_TO\",\n",
    "            \"NEW_TO_DEPTH\",\n",
    "            \"Candidate Score\",\n",
    "            \"Status\",\n",
    "        ],\n",
    "    )\n",
    "    empty_file_path = None\n",
    "\n",
    "    if csv_file is None:\n",
    "        return empty_df, empty_file_path\n",
    "\n",
    "    try:\n",
    "        df_input = pd.read_csv(csv_file.name)\n",
    "\n",
    "        # Validate required columns\n",
    "        required_cols = [\"NEW_FROM\", \"NEW_FROM_DEPTH\", \"NEW_TO\", \"NEW_TO_DEPTH\"]\n",
    "        if not all(col in df_input.columns for col in required_cols):\n",
    "            missing_cols = [col for col in required_cols if col not in df_input.columns]\n",
    "            return pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        f\"Error: Missing columns: {', '.join(missing_cols)}.\",\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"NEW_FROM\",\n",
    "                    \"NEW_FROM_DEPTH\",\n",
    "                    \"NEW_TO\",\n",
    "                    \"NEW_TO_DEPTH\",\n",
    "                    \"Candidate Score\",\n",
    "                    \"Status\",\n",
    "                ],\n",
    "            ), empty_file_path\n",
    "\n",
    "        results = []\n",
    "        for index, row in df_input.iterrows():\n",
    "            # Original values from the input CSV\n",
    "            original_new_from = row[\"NEW_FROM\"]\n",
    "            original_new_from_depth = row[\n",
    "                \"NEW_FROM_DEPTH\"\n",
    "            ]  # Original depth of NEW_FROM\n",
    "            original_new_to = row[\n",
    "                \"NEW_TO\"\n",
    "            ]  # This is the URL for which we need recommendations\n",
    "            new_to_depth_value = row[\n",
    "                \"NEW_TO_DEPTH\"\n",
    "            ]  # This is the depth value, not the URL\n",
    "\n",
    "            # Use the URL from the 'NEW_TO' column for recommendation\n",
    "            source_url_for_recommendation = original_new_to\n",
    "\n",
    "            # Simulate processing time (can be removed for real processing)\n",
    "            # time.sleep(0.1)\n",
    "\n",
    "            candidate_url = None\n",
    "            candidate_score = None\n",
    "            new_from_candidate_depth = None  # Initialize new depth\n",
    "            status = \"\"\n",
    "\n",
    "            # Attempt to get recommendations from the real engine, passing depth filters\n",
    "            recommendations_df, error_msg = recommendation_engine.get_recommendations(\n",
    "                source_url=source_url_for_recommendation,\n",
    "                top_n=50,\n",
    "                min_folder_depth=min_depth,  # Pass min depth from UI\n",
    "                max_folder_depth=max_depth,  # Pass max depth from UI\n",
    "            )\n",
    "\n",
    "            if error_msg:\n",
    "                status = f\"Error: {error_msg}\"\n",
    "                candidate_url = pd.NA  # Explicitly set to pandas Not Available\n",
    "                candidate_score = pd.NA  # Explicitly set to pandas Not Available\n",
    "                new_from_candidate_depth = pd.NA\n",
    "            elif recommendations_df.empty:\n",
    "                status = \"No recommendations found by model\"\n",
    "                candidate_url = pd.NA\n",
    "                candidate_score = pd.NA\n",
    "                new_from_candidate_depth = pd.NA\n",
    "            else:\n",
    "                # Weighted random selection from model recommendations\n",
    "                # Ensure scores are positive for weighting to avoid division by zero or issues with negative weights\n",
    "                recommendations_df[\"SCORE\"] = recommendations_df[\"SCORE\"].apply(\n",
    "                    lambda x: max(x, 0.001)\n",
    "                )\n",
    "\n",
    "                total_score = recommendations_df[\"SCORE\"].sum()\n",
    "\n",
    "                if total_score > 0:\n",
    "                    # Sample one row based on 'SCORE' as weights\n",
    "                    selected_row = recommendations_df.sample(\n",
    "                        n=1, weights=\"SCORE\", random_state=None\n",
    "                    )\n",
    "                    candidate_url = selected_row[\"RECOMMENDED_URL\"].iloc[0]\n",
    "                    candidate_score = selected_row[\"SCORE\"].iloc[0]\n",
    "\n",
    "                    # Calculate depth for the newly recommended URL\n",
    "                    # This uses the real URLProcessor\n",
    "                    new_from_candidate_depth = url_processor.get_folder_depth(\n",
    "                        candidate_url\n",
    "                    )\n",
    "\n",
    "                    status = \"Success (weighted random)\"\n",
    "                else:\n",
    "                    # If total score is zero (e.g., all scores were 0.001 after max(x, 0.001))\n",
    "                    status = \"No valid scores for weighted random selection\"\n",
    "                    candidate_url = pd.NA\n",
    "                    candidate_score = pd.NA\n",
    "                    new_from_candidate_depth = pd.NA\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"NEW_FROM\": candidate_url,  # This will be the new candidate URL\n",
    "                    \"NEW_FROM_DEPTH\": new_from_candidate_depth,  # New: Depth of the recommended URL\n",
    "                    \"NEW_TO\": original_new_to,\n",
    "                    \"NEW_TO_DEPTH\": new_to_depth_value,  # Keep the original depth value here\n",
    "                    \"Candidate Score\": candidate_score,\n",
    "                    \"Status\": status,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_output = pd.DataFrame(results)\n",
    "\n",
    "        # --- Save DataFrame to a temporary CSV file ---\n",
    "        # Create a temporary file\n",
    "        temp_file = tempfile.NamedTemporaryFile(\n",
    "            mode=\"w\", delete=False, suffix=\".csv\", encoding=\"utf-8\"\n",
    "        )\n",
    "        temp_file_path = temp_file.name\n",
    "        temp_file.close()  # Close the file handle so pandas can write to it\n",
    "\n",
    "        # Save the DataFrame to the temporary file\n",
    "        df_output.to_csv(temp_file_path, index=False)\n",
    "\n",
    "        # Return both the DataFrame for display and the path to the saved file\n",
    "        return df_output, temp_file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # Return a DataFrame with correct headers for unexpected errors and no file\n",
    "        return pd.DataFrame(\n",
    "            [[\"\", \"\", \"\", \"\", \"\", f\"An unexpected error occurred: {e}\"]],\n",
    "            columns=[\n",
    "                \"NEW_FROM\",\n",
    "                \"NEW_FROM_DEPTH\",\n",
    "                \"NEW_TO\",\n",
    "                \"NEW_TO_DEPTH\",\n",
    "                \"Candidate Score\",\n",
    "                \"Status\",\n",
    "            ],\n",
    "        ), empty_file_path\n",
    "\n",
    "\n",
    "# --- Gradio Interface Definition ---\n",
    "with gr.Blocks(title=\"WebKnoGraph Link Recommender\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # WebKnoGraph Link Recommender\n",
    "        Upload a CSV file with columns: `NEW_FROM`, `NEW_FROM_DEPTH`, `NEW_TO`, `NEW_TO_DEPTH`.\n",
    "        The system will suggest a new candidate URL for each `NEW_TO` URL,\n",
    "        storing the result in the `NEW_FROM` column.\n",
    "        The selection uses a weighted random approach from the model's top recommendations.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        csv_input = gr.File(\n",
    "            label=\"Upload CSV File\", type=\"filepath\", file_types=[\".csv\"]\n",
    "        )\n",
    "        submit_button = gr.Button(\"Generate Recommendations\")\n",
    "\n",
    "    with gr.Row():\n",
    "        min_depth_input = gr.Number(label=\"Minimum Folder Depth\", value=0, precision=0)\n",
    "        max_depth_input = gr.Number(label=\"Maximum Folder Depth\", value=10, precision=0)\n",
    "\n",
    "    output_dataframe = gr.DataFrame(\n",
    "        headers=[\n",
    "            \"NEW_FROM\",\n",
    "            \"NEW_FROM_DEPTH\",\n",
    "            \"NEW_TO\",\n",
    "            \"NEW_TO_DEPTH\",\n",
    "            \"Candidate Score\",\n",
    "            \"Status\",\n",
    "        ],\n",
    "        row_count=0,  # Gradio will dynamically adjust row count\n",
    "        col_count=6,  # Explicitly set column count\n",
    "        wrap=True,\n",
    "        interactive=False,\n",
    "        label=\"Recommendation Results\",\n",
    "    )\n",
    "\n",
    "    # New Gradio File component for output download\n",
    "    download_csv_file = gr.File(\n",
    "        label=\"Download Results CSV\",\n",
    "        type=\"filepath\",\n",
    "        file_types=[\".csv\"],\n",
    "        visible=False,\n",
    "    )\n",
    "\n",
    "    submit_button.click(\n",
    "        fn=process_csv_for_recommendations,\n",
    "        inputs=[csv_input, min_depth_input, max_depth_input],  # Pass new depth inputs\n",
    "        outputs=[\n",
    "            output_dataframe,\n",
    "            download_csv_file,\n",
    "        ],  # Now outputs both DataFrame and file\n",
    "        api_name=\"process_csv\",\n",
    "    ).then(\n",
    "        # Make the download button visible only after processing is complete and a file path is returned\n",
    "        lambda x: gr.File(\n",
    "            visible=True, value=x\n",
    "        ),  # x will be the file path from the previous function\n",
    "        inputs=download_csv_file,  # Use the output from the previous step as input here\n",
    "        outputs=download_csv_file,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
