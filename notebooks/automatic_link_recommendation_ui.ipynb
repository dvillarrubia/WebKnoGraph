{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYL7OLjWanv_"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torch-geometric pandas duckdb pyarrow networkx gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAanZXMGaSGF"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import tempfile  # Import tempfile for creating temporary files\n",
    "import sys  # Import sys to modify Python path\n",
    "import abc  # Import abc for abstract base classes\n",
    "from urllib.parse import urlparse  # Added for URLProcessor's concrete implementation\n",
    "import logging  # Added for URLProcessor's concrete implementation\n",
    "\n",
    "# --- Google Colab Drive Mounting ---\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = \"/content/drive/My Drive/WebKnoGraph\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  # Corrected: removed extra 'sys.'\n",
    "\n",
    "# --- Import Real Classes from WebKnoGraph Project ---\n",
    "try:\n",
    "    import torch\n",
    "    import json\n",
    "\n",
    "    # Import the real LinkPredictionConfig\n",
    "    from src.backend.config.link_prediction_config import LinkPredictionConfig\n",
    "    from src.backend.models.graph_models import GraphSAGEModel\n",
    "\n",
    "    # Use the exact ILogger definition provided by the user\n",
    "    from src.shared.interfaces import (\n",
    "        ILogger as OriginalILogger,\n",
    "    )  # Alias to avoid name conflict\n",
    "\n",
    "    # Import the real URLProcessor\n",
    "    from src.backend.utils.url_processing import URLProcessor as OriginalURLProcessor\n",
    "\n",
    "    # Define a concrete ConsoleLogger that implements OriginalILogger\n",
    "    class ConsoleLogger(OriginalILogger):\n",
    "        def info(self, message: str):\n",
    "            print(f\"INFO: {message}\")\n",
    "\n",
    "        def error(self, message: str):\n",
    "            print(f\"ERROR: {message}\")\n",
    "\n",
    "        def debug(self, message: str):\n",
    "            print(f\"DEBUG: {message}\")\n",
    "\n",
    "        def warning(self, message: str):\n",
    "            print(f\"WARNING: {message}\")\n",
    "\n",
    "        def exception(self, message: str):\n",
    "            print(f\"EXCEPTION: {message}\")\n",
    "\n",
    "    # Use this concrete logger as the ILogger for the application\n",
    "    ILogger = ConsoleLogger\n",
    "    # Assign the imported URLProcessor to the expected name\n",
    "    URLProcessor = OriginalURLProcessor\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing WebKnoGraph modules: {e}\")\n",
    "    print(\n",
    "        \"Please ensure your 'WebKnoGraph' project is correctly structured in Google Drive at:\"\n",
    "    )\n",
    "    print(f\"'{project_root}' and all dependencies are installed.\")\n",
    "\n",
    "    # Define dummy classes if imports fail, to allow the Gradio app to at least launch\n",
    "    # This is a fallback for development/debugging, for real use, imports must succeed.\n",
    "    class LinkPredictionConfig:  # Dummy LinkPredictionConfig\n",
    "        def __init__(self):\n",
    "            print(\"Using dummy LinkPredictionConfig due to import error.\")\n",
    "            # Fallback paths for dummy config\n",
    "            base_path = \"/tmp\"  # Use a temporary directory for dummy files\n",
    "            self.model_state_path = os.path.join(\n",
    "                base_path, \"graphsage_link_predictor.pth\"\n",
    "            )\n",
    "            self.node_embeddings_path = os.path.join(\n",
    "                base_path, \"final_node_embeddings.pt\"\n",
    "            )\n",
    "            self.node_mapping_path = os.path.join(base_path, \"model_metadata.json\")\n",
    "            self.edge_index_path = os.path.join(base_path, \"edge_index.pt\")\n",
    "            self.edge_csv_path = os.path.join(\n",
    "                base_path, \"link_graph_edges.csv\"\n",
    "            )  # Corrected dummy path\n",
    "\n",
    "    # Dummy ILogger for fallback scenario\n",
    "    class ILogger:\n",
    "        def info(self, message: str):\n",
    "            print(f\"INFO (Dummy Logger): {message}\")\n",
    "\n",
    "        def error(self, message: str):\n",
    "            print(f\"ERROR (Dummy Logger): {message}\")\n",
    "\n",
    "        def debug(self, message: str):\n",
    "            print(f\"DEBUG (Dummy Logger): {message}\")\n",
    "\n",
    "        def warning(self, message: str):\n",
    "            print(f\"WARNING (Dummy Logger): {message}\")\n",
    "\n",
    "        def exception(self, message: str):\n",
    "            print(f\"EXCEPTION (Dummy Logger): {message}\")\n",
    "\n",
    "    # Dummy URLProcessor for fallback scenario\n",
    "    class URLProcessor:\n",
    "        def get_folder_depth(self, url: str) -> int:\n",
    "            try:\n",
    "                # Basic dummy depth calculation\n",
    "                return len(\n",
    "                    url.split(\"://\", 1)[-1].split(\"/\", 1)[-1].strip(\"/\").split(\"/\")\n",
    "                )\n",
    "            except:\n",
    "                return -1\n",
    "\n",
    "    class GraphSAGEModel:  # A very basic mock if the real one can't be imported\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            pass\n",
    "\n",
    "        def predict_link(self, node_embeddings, candidate_edge_index):\n",
    "            # Ensure candidate_edge_index has a shape attribute for the mock\n",
    "            if not hasattr(candidate_edge_index, \"shape\"):\n",
    "                # Create a dummy shape if not provided, assuming 2 rows\n",
    "                num_cols = 10  # Default number of candidates for mock\n",
    "                candidate_edge_index = type(\n",
    "                    \"obj\", (object,), {\"shape\": (2, num_cols)}\n",
    "                )()\n",
    "            return [\n",
    "                random.uniform(0.1, 0.9) for _ in range(candidate_edge_index.shape[1])\n",
    "            ]\n",
    "\n",
    "        def load_state_dict(self, state_dict):\n",
    "            pass\n",
    "\n",
    "        def eval(self):\n",
    "            pass\n",
    "\n",
    "        def to(self, device):\n",
    "            return self\n",
    "\n",
    "\n",
    "# --- Real RecommendationEngine Class (as provided by user) ---\n",
    "class RecommendationEngine:\n",
    "    \"\"\"Loads trained artifacts and provides link recommendations using a Top-K strategy.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: LinkPredictionConfig, logger: ILogger, url_processor: URLProcessor\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.url_processor = url_processor\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.node_embeddings = None\n",
    "        self.url_to_idx = None\n",
    "        self.idx_to_url = None\n",
    "        self.existing_edges = None  # This will still be used by the model for filtering\n",
    "\n",
    "    def load_artifacts(self):\n",
    "        \"\"\"Loads the trained model, embeddings, and mappings into memory.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.logger.info(\"Artifacts already loaded.\")\n",
    "            return True\n",
    "\n",
    "        self.logger.info(\"Loading trained artifacts for recommendations...\")\n",
    "        try:\n",
    "            # Ensure the directory exists before attempting to open files\n",
    "            model_dir = os.path.dirname(self.config.node_mapping_path)\n",
    "            if not os.path.exists(model_dir):\n",
    "                raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "\n",
    "            with open(self.config.node_mapping_path, \"r\") as f:\n",
    "                model_metadata = json.load(f)\n",
    "\n",
    "            self.url_to_idx = model_metadata[\"url_to_idx\"]\n",
    "            in_channels = model_metadata[\"in_channels\"]\n",
    "            hidden_channels = model_metadata[\"hidden_channels\"]\n",
    "            out_channels = model_metadata[\"out_channels\"]\n",
    "\n",
    "            self.idx_to_url = {v: k for k, v in self.url_to_idx.items()}\n",
    "\n",
    "            self.node_embeddings = torch.load(\n",
    "                self.config.node_embeddings_path, map_location=self.device\n",
    "            ).to(self.device)\n",
    "            edge_index = torch.load(\n",
    "                self.config.edge_index_path, map_location=self.device\n",
    "            )\n",
    "            self.existing_edges = set(\n",
    "                zip(edge_index[0].tolist(), edge_index[1].tolist())\n",
    "            )\n",
    "\n",
    "            self.model = GraphSAGEModel(in_channels, hidden_channels, out_channels)\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.model_state_path, map_location=self.device)\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "\n",
    "            self.logger.info(\"Artifacts loaded successfully.\")\n",
    "            return True\n",
    "        except FileNotFoundError as fnf_e:\n",
    "            self.logger.error(\n",
    "                f\"Could not find trained model artifacts. Please run the training pipeline first. Error: {fnf_e}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"DEBUG: FileNotFoundError during artifact loading: {fnf_e}\"\n",
    "            )  # Added for debugging\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while loading artifacts: {e}\")\n",
    "            print(\n",
    "                f\"DEBUG: General Exception during artifact loading: {e}\"\n",
    "            )  # Added for debugging\n",
    "            # Re-raise for debugging if needed, but for Gradio, returning False is often better\n",
    "            # raise\n",
    "            return False\n",
    "\n",
    "    def get_recommendations(\n",
    "        self,\n",
    "        source_url: str,\n",
    "        top_n: int = 20,\n",
    "        min_folder_depth: int = 0,\n",
    "        max_folder_depth: int = 10,\n",
    "        folder_path_filter: str = None,\n",
    "    ):\n",
    "        # The load_artifacts call is crucial here. If it returns False, we return None.\n",
    "        if not self.load_artifacts():\n",
    "            return (\n",
    "                None,\n",
    "                \"Error: Trained model artifacts not found. Please run the training pipeline first.\",\n",
    "            )\n",
    "        if source_url not in self.url_to_idx:\n",
    "            return (\n",
    "                None,\n",
    "                f\"Error: Source URL '{source_url}' not found in the graph's training data.\",\n",
    "            )\n",
    "\n",
    "        source_idx = self.url_to_idx[source_url]\n",
    "        num_nodes = len(self.url_to_idx)\n",
    "\n",
    "        # 1. Generate scores for all possible links from the source node\n",
    "        candidate_dest_indices = torch.arange(num_nodes, device=self.device)\n",
    "        candidate_source_indices = torch.full_like(\n",
    "            candidate_dest_indices, fill_value=source_idx\n",
    "        )\n",
    "        candidate_edge_index = torch.stack(\n",
    "            [candidate_source_indices, candidate_dest_indices]\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = self.model.predict_link(self.node_embeddings, candidate_edge_index)\n",
    "\n",
    "        # 2. Create a DataFrame from all possible candidates\n",
    "        all_candidates_df = pd.DataFrame(\n",
    "            {\n",
    "                \"DEST_IDX\": candidate_dest_indices.cpu().numpy(),\n",
    "                \"SCORE\": torch.sigmoid(scores).cpu().numpy(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 3. Add URL and FOLDER_DEPTH columns\n",
    "        # Use .get() with a default value to handle missing keys and prevent KeyError\n",
    "        all_candidates_df[\"RECOMMENDED_URL\"] = all_candidates_df[\"DEST_IDX\"].apply(\n",
    "            lambda idx: self.idx_to_url.get(idx, None)\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid URLs (where index was not found in mapping)\n",
    "        all_candidates_df.dropna(subset=[\"RECOMMENDED_URL\"], inplace=True)\n",
    "\n",
    "        all_candidates_df[\"FOLDER_DEPTH\"] = all_candidates_df[\"RECOMMENDED_URL\"].apply(\n",
    "            lambda url: self.url_processor.get_folder_depth(url)\n",
    "        )\n",
    "\n",
    "        # 4. Filter the DataFrame based on all criteria\n",
    "        filtered_df = all_candidates_df.copy()\n",
    "\n",
    "        # Filter out self-links\n",
    "        filtered_df = filtered_df[filtered_df[\"DEST_IDX\"] != source_idx]\n",
    "\n",
    "        # Filter out existing links\n",
    "        # Create a tuple column for easy set membership check\n",
    "        filtered_df[\"SOURCE_IDX\"] = source_idx\n",
    "        filtered_df[\"EDGE_TUPLE\"] = list(\n",
    "            zip(filtered_df[\"SOURCE_IDX\"], filtered_df[\"DEST_IDX\"])\n",
    "        )\n",
    "        filtered_df = filtered_df[~filtered_df[\"EDGE_TUPLE\"].isin(self.existing_edges)]\n",
    "\n",
    "        # Apply the folder depth filter\n",
    "        filtered_df = filtered_df[\n",
    "            (filtered_df[\"FOLDER_DEPTH\"] >= min_folder_depth)\n",
    "            & (filtered_df[\"FOLDER_DEPTH\"] <= max_folder_depth)\n",
    "        ]\n",
    "\n",
    "        # Apply the folder path filter if provided\n",
    "        if folder_path_filter:\n",
    "            self.logger.info(f\"Applying folder path filter: {folder_path_filter}\")\n",
    "            filtered_df = filtered_df[\n",
    "                filtered_df[\"RECOMMENDED_URL\"].str.startswith(folder_path_filter)\n",
    "            ]\n",
    "\n",
    "        # 5. Sort the filtered DataFrame by score and take the top N\n",
    "        final_recommendations_df = filtered_df.sort_values(\n",
    "            by=\"SCORE\", ascending=False\n",
    "        ).head(top_n)\n",
    "\n",
    "        # 6. Select the final columns and return\n",
    "        final_recommendations_df = final_recommendations_df[\n",
    "            [\"RECOMMENDED_URL\", \"SCORE\", \"FOLDER_DEPTH\"]\n",
    "        ]\n",
    "\n",
    "        if final_recommendations_df.empty:\n",
    "            return (\n",
    "                pd.DataFrame(),  # Return empty DataFrame for consistency\n",
    "                \"No recommendations found matching the criteria (filters, existing links, etc.). Try adjusting filters or source URL.\",\n",
    "            )\n",
    "\n",
    "        return final_recommendations_df, None\n",
    "\n",
    "\n",
    "# --- Gradio Application Logic ---\n",
    "\n",
    "# Instantiate real classes\n",
    "logger = ILogger()  # Now instantiates the concrete ConsoleLogger\n",
    "url_processor = URLProcessor()\n",
    "config = LinkPredictionConfig()  # This will now use the updated paths\n",
    "recommendation_engine = RecommendationEngine(config, logger, url_processor)\n",
    "\n",
    "\n",
    "def process_csv_for_recommendations(csv_file, min_depth: int, max_depth: int):\n",
    "    \"\"\"\n",
    "    Gradio function to process the uploaded CSV and generate recommendations.\n",
    "    Returns the DataFrame for display and the path to the saved CSV for download.\n",
    "    \"\"\"\n",
    "    # Define default empty DataFrame and file paths for error cases\n",
    "    empty_df_display = pd.DataFrame(\n",
    "        [[\"\", \"\", \"\", \"\", \"\", \"Please upload a CSV file.\"]],\n",
    "        columns=[\n",
    "            \"NEW_FROM\",\n",
    "            \"NEW_FROM_DEPTH\",\n",
    "            \"NEW_TO\",\n",
    "            \"NEW_TO_DEPTH\",\n",
    "            \"Candidate Score\",\n",
    "            \"Status\",\n",
    "        ],\n",
    "    )\n",
    "    empty_file_path = None\n",
    "\n",
    "    # Initial counts for display\n",
    "    initial_graph_nodes = 0\n",
    "    final_graph_nodes = 0\n",
    "    new_nodes_added = 0\n",
    "    total_rows_processed = 0\n",
    "    successful_recommendations_count = 0\n",
    "    original_edges_count = 0  # New: Count of edges in original graph\n",
    "    upgraded_edges_count = 0  # New: Count of edges in upgraded graph\n",
    "    edge_difference = 0  # New: Difference in edges\n",
    "\n",
    "    # Output placeholders for Markdown\n",
    "    md_total_rows = \"Total Rows Processed: **0**\"\n",
    "    md_successful_recs = \"Successful Recommendations: **0**\"\n",
    "    md_initial_graph = \"Initial Graph Nodes: **0**\"\n",
    "    md_final_graph = \"Final Graph Nodes (after processing input): **0**\"\n",
    "    md_new_nodes_added = \"New Nodes from Input (not in graph): **0**\"\n",
    "    md_original_edges = \"Original Graph Edges: **0**\"  # New Markdown output\n",
    "    md_upgraded_edges = \"Upgraded Graph Edges: **0**\"  # New Markdown output\n",
    "    md_edge_difference = (\n",
    "        \"Difference in Edges (New - Original): **0**\"  # New Markdown output\n",
    "    )\n",
    "\n",
    "    if csv_file is None:\n",
    "        return (\n",
    "            empty_df_display,\n",
    "            empty_file_path,\n",
    "            empty_file_path,\n",
    "            empty_file_path,\n",
    "            md_total_rows,\n",
    "            md_successful_recs,\n",
    "            md_initial_graph,\n",
    "            md_final_graph,\n",
    "            md_new_nodes_added,\n",
    "            md_original_edges,\n",
    "            md_upgraded_edges,\n",
    "            md_edge_difference,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Load artifacts once to get initial graph size\n",
    "        if not recommendation_engine.load_artifacts():\n",
    "            error_msg = (\n",
    "                \"Error: Model artifacts could not be loaded. Check paths and files.\"\n",
    "            )\n",
    "            return (\n",
    "                pd.DataFrame(\n",
    "                    [[\"\", \"\", \"\", \"\", \"\", error_msg]],\n",
    "                    columns=[\n",
    "                        \"NEW_FROM\",\n",
    "                        \"NEW_FROM_DEPTH\",\n",
    "                        \"NEW_TO\",\n",
    "                        \"NEW_TO_DEPTH\",\n",
    "                        \"Candidate Score\",\n",
    "                        \"Status\",\n",
    "                    ],\n",
    "                ),\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                md_total_rows,\n",
    "                md_successful_recs,\n",
    "                md_initial_graph,\n",
    "                md_final_graph,\n",
    "                md_new_nodes_added,\n",
    "                md_original_edges,\n",
    "                md_upgraded_edges,\n",
    "                md_edge_difference,\n",
    "            )\n",
    "\n",
    "        initial_graph_nodes = len(recommendation_engine.url_to_idx)\n",
    "        md_initial_graph = f\"Initial Graph Nodes: **{initial_graph_nodes}**\"\n",
    "\n",
    "        df_input = pd.read_csv(csv_file.name)\n",
    "        total_rows_processed = len(df_input)\n",
    "        md_total_rows = f\"Total Rows Processed: **{total_rows_processed}**\"\n",
    "\n",
    "        # Validate required columns\n",
    "        required_cols = [\"NEW_FROM\", \"NEW_FROM_DEPTH\", \"NEW_TO\", \"NEW_TO_DEPTH\"]\n",
    "        if not all(col in df_input.columns for col in required_cols):\n",
    "            missing_cols = [col for col in required_cols if col not in df_input.columns]\n",
    "            error_msg = f\"Error: Missing columns: {', '.join(missing_cols)}.\"\n",
    "            return (\n",
    "                pd.DataFrame(\n",
    "                    [[\"\", \"\", \"\", \"\", \"\", error_msg]],\n",
    "                    columns=[\n",
    "                        \"NEW_FROM\",\n",
    "                        \"NEW_FROM_DEPTH\",\n",
    "                        \"NEW_TO\",\n",
    "                        \"NEW_TO_DEPTH\",\n",
    "                        \"Candidate Score\",\n",
    "                        \"Status\",\n",
    "                    ],\n",
    "                ),\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                md_total_rows,\n",
    "                md_successful_recs,\n",
    "                md_initial_graph,\n",
    "                md_initial_graph,\n",
    "                \"New Nodes from Input (not in graph): **0**\",\n",
    "                md_original_edges,\n",
    "                md_upgraded_edges,\n",
    "                md_edge_difference,\n",
    "            )\n",
    "\n",
    "        results = []\n",
    "        successful_recommendations_count = 0\n",
    "\n",
    "        # --- Generate Original Graph Edges CSV by loading from config.edge_csv_path ---\n",
    "        try:\n",
    "            df_original_edges = pd.read_csv(\n",
    "                config.edge_csv_path\n",
    "            )  # Load from the specified path\n",
    "            # Ensure 'FROM' and 'TO' columns exist in the loaded CSV\n",
    "            if (\n",
    "                \"FROM\" not in df_original_edges.columns\n",
    "                or \"TO\" not in df_original_edges.columns\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Original graph CSV '{config.edge_csv_path}' must contain 'FROM' and 'TO' columns.\"\n",
    "                )\n",
    "            original_edges_count = len(df_original_edges)\n",
    "            md_original_edges = f\"Original Graph Edges: **{original_edges_count}**\"\n",
    "        except FileNotFoundError:\n",
    "            error_msg = f\"Error: Original graph CSV not found at {config.edge_csv_path}. Please check the path in LinkPredictionConfig.\"\n",
    "            return (\n",
    "                pd.DataFrame(\n",
    "                    [[\"\", \"\", \"\", \"\", \"\", error_msg]],\n",
    "                    columns=[\n",
    "                        \"NEW_FROM\",\n",
    "                        \"NEW_FROM_DEPTH\",\n",
    "                        \"NEW_TO\",\n",
    "                        \"NEW_TO_DEPTH\",\n",
    "                        \"Candidate Score\",\n",
    "                        \"Status\",\n",
    "                    ],\n",
    "                ),\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                md_total_rows,\n",
    "                md_successful_recs,\n",
    "                md_initial_graph,\n",
    "                md_final_graph,\n",
    "                md_new_nodes_added,\n",
    "                md_original_edges,\n",
    "                md_upgraded_edges,\n",
    "                md_edge_difference,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading original graph CSV: {e}\"\n",
    "            return (\n",
    "                pd.DataFrame(\n",
    "                    [[\"\", \"\", \"\", \"\", \"\", error_msg]],\n",
    "                    columns=[\n",
    "                        \"NEW_FROM\",\n",
    "                        \"NEW_FROM_DEPTH\",\n",
    "                        \"NEW_TO\",\n",
    "                        \"NEW_TO_DEPTH\",\n",
    "                        \"Candidate Score\",\n",
    "                        \"Status\",\n",
    "                    ],\n",
    "                ),\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                empty_file_path,\n",
    "                md_total_rows,\n",
    "                md_successful_recs,\n",
    "                md_initial_graph,\n",
    "                md_final_graph,\n",
    "                md_new_nodes_added,\n",
    "                md_original_edges,\n",
    "                md_upgraded_edges,\n",
    "                md_edge_difference,\n",
    "            )\n",
    "\n",
    "        temp_file_original_graph = tempfile.NamedTemporaryFile(\n",
    "            mode=\"w\", delete=False, suffix=\"_original_graph.csv\", encoding=\"utf-8\"\n",
    "        )\n",
    "        original_graph_csv_path = temp_file_original_graph.name\n",
    "        temp_file_original_graph.close()\n",
    "        df_original_edges.to_csv(original_graph_csv_path, index=False)\n",
    "\n",
    "        # --- Process Input CSV for Recommendations ---\n",
    "        new_recommended_edges_data = []  # To store new links for the upgraded graph\n",
    "\n",
    "        for index, row in df_input.iterrows():\n",
    "            original_new_from = row[\"NEW_FROM\"]\n",
    "            original_new_from_depth = row[\"NEW_FROM_DEPTH\"]\n",
    "            original_new_to = row[\"NEW_TO\"]\n",
    "            new_to_depth_value = row[\"NEW_TO_DEPTH\"]\n",
    "\n",
    "            source_url_for_recommendation = original_new_to\n",
    "\n",
    "            candidate_url = pd.NA\n",
    "            candidate_score = pd.NA\n",
    "            new_from_candidate_depth = pd.NA\n",
    "            status = \"\"\n",
    "\n",
    "            recommendations_df, error_msg = recommendation_engine.get_recommendations(\n",
    "                source_url=source_url_for_recommendation,\n",
    "                top_n=50,\n",
    "                min_folder_depth=min_depth,\n",
    "                max_folder_depth=max_depth,\n",
    "            )\n",
    "\n",
    "            if error_msg:\n",
    "                status = f\"Error: {error_msg}\"\n",
    "            elif recommendations_df.empty:\n",
    "                status = \"No recommendations found by model\"\n",
    "            else:\n",
    "                recommendations_df[\"SCORE\"] = recommendations_df[\"SCORE\"].apply(\n",
    "                    lambda x: max(x, 0.001)\n",
    "                )\n",
    "                total_score = recommendations_df[\"SCORE\"].sum()\n",
    "\n",
    "                if total_score > 0:\n",
    "                    selected_row = recommendations_df.sample(\n",
    "                        n=1, weights=\"SCORE\", random_state=None\n",
    "                    )\n",
    "                    candidate_url = selected_row[\"RECOMMENDED_URL\"].iloc[0]\n",
    "                    candidate_score = selected_row[\"SCORE\"].iloc[0]\n",
    "                    new_from_candidate_depth = url_processor.get_folder_depth(\n",
    "                        candidate_url\n",
    "                    )\n",
    "                    status = \"Success (weighted random)\"\n",
    "                    successful_recommendations_count += 1\n",
    "\n",
    "                    # Add this new recommended link to the list for the upgraded graph\n",
    "                    new_recommended_edges_data.append(\n",
    "                        {\"FROM\": source_url_for_recommendation, \"TO\": candidate_url}\n",
    "                    )\n",
    "                else:\n",
    "                    status = \"No valid scores for weighted random selection\"\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"NEW_FROM\": candidate_url,\n",
    "                    \"NEW_FROM_DEPTH\": new_from_candidate_depth,\n",
    "                    \"NEW_TO\": original_new_to,\n",
    "                    \"NEW_TO_DEPTH\": new_to_depth_value,\n",
    "                    \"Candidate Score\": candidate_score,\n",
    "                    \"Status\": status,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_output_display = pd.DataFrame(results)\n",
    "        md_successful_recs = (\n",
    "            f\"Successful Recommendations: **{successful_recommendations_count}**\"\n",
    "        )\n",
    "\n",
    "        # --- Generate Upgraded Graph Edges CSV ---\n",
    "        df_new_recommended_edges = pd.DataFrame(new_recommended_edges_data)\n",
    "\n",
    "        # Combine original edges with new recommended edges\n",
    "        df_upgraded_edges = (\n",
    "            pd.concat([df_original_edges, df_new_recommended_edges])\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        upgraded_edges_count = len(df_upgraded_edges)  # Count upgraded edges\n",
    "        md_upgraded_edges = f\"Upgraded Graph Edges: **{upgraded_edges_count}**\"\n",
    "\n",
    "        edge_difference = (\n",
    "            upgraded_edges_count - original_edges_count\n",
    "        )  # Calculate difference\n",
    "        md_edge_difference = f\"Difference in Edges (New - Original): **{edge_difference}**\"  # Display difference\n",
    "\n",
    "        temp_file_upgraded_graph = tempfile.NamedTemporaryFile(\n",
    "            mode=\"w\", delete=False, suffix=\"_upgraded_graph.csv\", encoding=\"utf-8\"\n",
    "        )\n",
    "        upgraded_graph_csv_path = temp_file_upgraded_graph.name\n",
    "        temp_file_upgraded_graph.close()\n",
    "        df_upgraded_edges.to_csv(upgraded_graph_csv_path, index=False)\n",
    "\n",
    "        # --- Calculate Node Counts for Display ---\n",
    "        # Initial graph nodes (already calculated)\n",
    "        # Final graph nodes: unique nodes from original edges + unique nodes from new recommended edges\n",
    "        all_nodes_in_upgraded_graph = set(\n",
    "            df_upgraded_edges[\"FROM\"].dropna().unique()\n",
    "        ) | set(df_upgraded_edges[\"TO\"].dropna().unique())\n",
    "        final_graph_nodes = len(all_nodes_in_upgraded_graph)\n",
    "        md_final_graph = (\n",
    "            f\"Final Graph Nodes (after processing input): **{final_graph_nodes}**\"\n",
    "        )\n",
    "\n",
    "        # New nodes added: nodes in the upgraded graph that were not in the initial graph\n",
    "        new_nodes_from_input = len(\n",
    "            set(df_input[\"NEW_TO\"].dropna().unique())\n",
    "            - set(recommendation_engine.url_to_idx.keys())\n",
    "        )\n",
    "        new_nodes_added = (\n",
    "            final_graph_nodes - initial_graph_nodes\n",
    "        )  # This is more accurate for total new nodes in graph\n",
    "        md_new_nodes_added = f\"New Nodes Added: **{new_nodes_added}**\"\n",
    "\n",
    "        # --- Save Display DataFrame to a temporary CSV file (for the primary download button) ---\n",
    "        temp_file_display_output = tempfile.NamedTemporaryFile(\n",
    "            mode=\"w\",\n",
    "            delete=False,\n",
    "            suffix=\"_recommendations_output.csv\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        display_output_csv_path = temp_file_display_output.name\n",
    "        temp_file_display_output.close()\n",
    "        df_output_display.to_csv(display_output_csv_path, index=False)\n",
    "\n",
    "        # Return all outputs\n",
    "        return (\n",
    "            df_output_display,\n",
    "            display_output_csv_path,  # Primary recommendations output\n",
    "            original_graph_csv_path,  # Original graph edges\n",
    "            upgraded_graph_csv_path,  # Upgraded graph edges\n",
    "            md_total_rows,\n",
    "            md_successful_recs,\n",
    "            md_initial_graph,\n",
    "            md_final_graph,\n",
    "            md_new_nodes_added,\n",
    "            md_original_edges,  # New output\n",
    "            md_upgraded_edges,  # New output\n",
    "            md_edge_difference,  # New output\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An unexpected error occurred: {e}\"\n",
    "        # Return error state for all outputs\n",
    "        return (\n",
    "            pd.DataFrame(\n",
    "                [[\"\", \"\", \"\", \"\", \"\", error_message]],\n",
    "                columns=[\n",
    "                    \"NEW_FROM\",\n",
    "                    \"NEW_FROM_DEPTH\",\n",
    "                    \"NEW_TO\",\n",
    "                    \"NEW_TO_DEPTH\",\n",
    "                    \"Candidate Score\",\n",
    "                    \"Status\",\n",
    "                ],\n",
    "            ),\n",
    "            empty_file_path,\n",
    "            empty_file_path,\n",
    "            empty_file_path,\n",
    "            md_total_rows,\n",
    "            md_successful_recs,\n",
    "            md_initial_graph,\n",
    "            md_final_graph,\n",
    "            md_new_nodes_added,\n",
    "            md_original_edges,\n",
    "            md_upgraded_edges,\n",
    "            md_edge_difference,\n",
    "        )\n",
    "\n",
    "\n",
    "# --- Gradio Interface Definition ---\n",
    "with gr.Blocks(title=\"WebKnoGraph Link Recommender\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # WebKnoGraph Link Recommender\n",
    "        Upload a CSV file with columns: `NEW_FROM`, `NEW_FROM_DEPTH`, `NEW_TO`, `NEW_TO_DEPTH`.\n",
    "        The system will suggest a new candidate URL for each `NEW_TO` URL,\n",
    "        storing the result in the `NEW_FROM` column.\n",
    "        The selection uses a weighted random approach from the model's top recommendations.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        csv_input = gr.File(\n",
    "            label=\"Upload CSV File\", type=\"filepath\", file_types=[\".csv\"]\n",
    "        )\n",
    "        submit_button = gr.Button(\"Generate Recommendations\")\n",
    "\n",
    "    with gr.Row():\n",
    "        min_depth_input = gr.Number(label=\"Minimum Folder Depth\", value=0, precision=0)\n",
    "        max_depth_input = gr.Number(label=\"Maximum Folder Depth\", value=10, precision=0)\n",
    "\n",
    "    # New components for displaying counts\n",
    "    with gr.Row():\n",
    "        total_rows_output = gr.Markdown(\"Total Rows Processed: **0**\")\n",
    "        successful_recs_output = gr.Markdown(\"Successful Recommendations: **0**\")\n",
    "    with gr.Row():  # Separate row for graph node counts\n",
    "        initial_graph_nodes_output = gr.Markdown(\"Initial Graph Nodes: **0**\")\n",
    "        final_graph_nodes_output = gr.Markdown(\n",
    "            \"Final Graph Nodes (after processing input): **0**\"\n",
    "        )\n",
    "        new_nodes_added_output = gr.Markdown(\"New Nodes Added: **0**\")\n",
    "    with gr.Row():  # New row for edge counts\n",
    "        original_edges_output = gr.Markdown(\"Original Graph Edges: **0**\")\n",
    "        upgraded_edges_output = gr.Markdown(\"Upgraded Graph Edges: **0**\")\n",
    "        edge_difference_output = gr.Markdown(\n",
    "            \"Difference in Edges (New - Original): **0**\"\n",
    "        )\n",
    "\n",
    "    output_dataframe = gr.DataFrame(\n",
    "        headers=[\n",
    "            \"NEW_FROM\",\n",
    "            \"NEW_FROM_DEPTH\",\n",
    "            \"NEW_TO\",\n",
    "            \"NEW_TO_DEPTH\",\n",
    "            \"Candidate Score\",\n",
    "            \"Status\",\n",
    "        ],\n",
    "        row_count=0,  # Gradio will dynamically adjust row count\n",
    "        col_count=6,  # Explicitly set column count\n",
    "        wrap=True,\n",
    "        interactive=False,\n",
    "        label=\"Recommendation Results\",\n",
    "    )\n",
    "\n",
    "    # New Gradio File components for output downloads\n",
    "    download_recommendations_csv = gr.File(\n",
    "        label=\"Download Recommendations Output CSV\",\n",
    "        type=\"filepath\",\n",
    "        file_types=[\".csv\"],\n",
    "        visible=False,\n",
    "    )\n",
    "    download_original_graph_csv = gr.File(\n",
    "        label=\"Download Original Graph Edges CSV\",\n",
    "        type=\"filepath\",\n",
    "        file_types=[\".csv\"],\n",
    "        visible=False,\n",
    "    )\n",
    "    download_upgraded_graph_csv = gr.File(\n",
    "        label=\"Download Upgraded Graph Edges CSV\",\n",
    "        type=\"filepath\",\n",
    "        file_types=[\".csv\"],\n",
    "        visible=False,\n",
    "    )\n",
    "\n",
    "    submit_button.click(\n",
    "        fn=process_csv_for_recommendations,\n",
    "        inputs=[csv_input, min_depth_input, max_depth_input],  # Pass new depth inputs\n",
    "        outputs=[\n",
    "            output_dataframe,\n",
    "            download_recommendations_csv,  # Primary recommendations output\n",
    "            download_original_graph_csv,  # Original graph edges\n",
    "            download_upgraded_graph_csv,  # Upgraded graph edges\n",
    "            total_rows_output,\n",
    "            successful_recs_output,\n",
    "            initial_graph_nodes_output,\n",
    "            final_graph_nodes_output,\n",
    "            new_nodes_added_output,\n",
    "            original_edges_output,  # New output\n",
    "            upgraded_edges_output,  # New output\n",
    "            edge_difference_output,  # New output\n",
    "        ],  # Now outputs all components\n",
    "        api_name=\"process_csv\",\n",
    "    ).then(\n",
    "        # Make the download buttons visible only after processing is complete and file paths are returned\n",
    "        # The lambda function receives all outputs from the previous function.\n",
    "        # We need to map them correctly to the inputs of the next .then() call.\n",
    "        # The first output (df_output_display) is not passed to the .then() as it's already displayed.\n",
    "        # The next three are the file paths.\n",
    "        lambda rec_csv,\n",
    "        orig_csv,\n",
    "        upg_csv,\n",
    "        *args: (  # Catch extra args from previous outputs\n",
    "            gr.File(visible=True, value=rec_csv),\n",
    "            gr.File(visible=True, value=orig_csv),\n",
    "            gr.File(visible=True, value=upg_csv),\n",
    "        ),\n",
    "        inputs=[\n",
    "            download_recommendations_csv,\n",
    "            download_original_graph_csv,\n",
    "            download_upgraded_graph_csv,\n",
    "            total_rows_output,\n",
    "            successful_recs_output,\n",
    "            initial_graph_nodes_output,\n",
    "            final_graph_nodes_output,\n",
    "            new_nodes_added_output,\n",
    "            original_edges_output,\n",
    "            upgraded_edges_output,\n",
    "            edge_difference_output,\n",
    "        ],  # Pass all outputs from previous step\n",
    "        outputs=[\n",
    "            download_recommendations_csv,\n",
    "            download_original_graph_csv,\n",
    "            download_upgraded_graph_csv,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
