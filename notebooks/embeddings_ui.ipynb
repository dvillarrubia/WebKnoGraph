{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xw4QDh41TRn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlhbx_4z1WEx"
      },
      "source": [
        "# pip install -r requirements.txt first, before continuing with the rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SRE7N21AKE6"
      },
      "outputs": [],
      "source": [
        "# Installing necessary packages!\n",
        "# These should ideally be in requirements.txt and installed once for the environment.\n",
        "# But for a notebook that's meant to be self-contained for easy sharing/running,\n",
        "# it's common to keep them here.\n",
        "# !pip install trafilatura sentence-transformers torch pandas pyarrow duckdb scipy -q\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress a common warning from the sentence-transformers library\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=FutureWarning, module=\"huggingface_hub.file_download\"\n",
        ")\n",
        "\n",
        "# Add the project root to the Python path so we can import from src\n",
        "# Adjust this path if your notebook is located differently relative to the 'src' folder\n",
        "# This assumes your project root is '/content/drive/My Drive/WebKnoGraph'\n",
        "project_root = \"/content/drive/My Drive/WebKnoGraph\"  # Explicitly set\n",
        "\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "print(f\"Project root added to sys.path: {project_root}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(f\"sys.path: {sys.path}\")\n",
        "\n",
        "# Google Colab Drive Mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    # Check if already mounted before attempting to mount again (as in embeddings_ui.ipynb Cell 1)\n",
        "    if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "        drive.mount(\"/content/drive/\")\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    else:\n",
        "        print(\"Google Drive already mounted.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "# Import from your refactored backend and shared modules\n",
        "import gradio as gr\n",
        "import io\n",
        "import duckdb\n",
        "import pandas as pd  # Added pandas import\n",
        "from tqdm.auto import tqdm  # Added tqdm import for progress bar\n",
        "import traceback  # Added traceback for error logging\n",
        "\n",
        "# Specific imports for the Embedding Pipeline\n",
        "from src.backend.config.embeddings_config import EmbeddingConfig\n",
        "from src.backend.data.embedding_state_manager import EmbeddingStateManager\n",
        "from src.backend.data.embeddings_loader import DataLoader\n",
        "from src.backend.data.embeddings_saver import DataSaver\n",
        "from src.backend.utils.text_processing import TextExtractor\n",
        "from src.backend.utils.embedding_generation import EmbeddingGenerator\n",
        "from src.backend.services.embeddings_service import EmbeddingPipeline\n",
        "from src.shared.logging_config import (\n",
        "    ConsoleAndGradioLogger,\n",
        ")  # Using the updated generic logger\n",
        "\n",
        "print(\"All modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ5OJHuPALDo"
      },
      "outputs": [],
      "source": [
        "# File: embeddings_ui.ipynb - Cell 3\n",
        "def run_gradio_interface(input_path: str, output_path: str, batch_size: int):\n",
        "    \"\"\"Wires up all components and runs the pipeline, yielding UI updates.\"\"\"\n",
        "    log_stream = io.StringIO()\n",
        "    logger = ConsoleAndGradioLogger(\n",
        "        log_stream, logger_name=\"EmbeddingLogger\"\n",
        "    )  # Pass logger_name\n",
        "\n",
        "    config = EmbeddingConfig(\n",
        "        input_path=input_path, output_path=output_path, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Instantiate all our components\n",
        "    state_manager = EmbeddingStateManager(config.output_path, logger)\n",
        "    data_loader = DataLoader(config.input_path, logger)\n",
        "    text_extractor = TextExtractor()\n",
        "    embedding_generator = EmbeddingGenerator(config.model_name, logger)\n",
        "    data_saver = DataSaver(config.output_path, logger)\n",
        "\n",
        "    # Create a modified EmbeddingPipeline class within this scope that includes the fix\n",
        "    # Alternatively, you would apply this fix directly in src/backend/services/embeddings_service.py\n",
        "    class FixedEmbeddingPipeline(EmbeddingPipeline):\n",
        "        def run(self):\n",
        "            \"\"\"\n",
        "            Orchestrates the embedding generation pipeline.\n",
        "            Yields status updates for Gradio UI.\n",
        "            \"\"\"\n",
        "            self.logger.info(\"Starting embedding pipeline...\")\n",
        "            yield \"Status: Initializing Pipeline...\"\n",
        "\n",
        "            try:\n",
        "                processed_urls = self.state_manager.get_processed_urls()\n",
        "                self.logger.info(\n",
        "                    f\"Found {len(processed_urls)} URLs that have already been processed. They will be skipped.\"\n",
        "                )\n",
        "                yield f\"Status: Resuming, skipping {len(processed_urls)} already processed URLs.\"\n",
        "\n",
        "                self.logger.info(\"Querying for new pages to process...\")\n",
        "                data_iterator = self.data_loader.stream_unprocessed_data(\n",
        "                    processed_urls=processed_urls, batch_size=self.config.batch_size\n",
        "                )\n",
        "                yield \"Status: Loading new data...\"\n",
        "\n",
        "                total_processed_in_session = 0\n",
        "                for batch_num, df_batch_arrow in enumerate(data_iterator):\n",
        "                    if df_batch_arrow.num_rows == 0:\n",
        "                        self.logger.info(f\"Batch {batch_num + 1} is empty, skipping.\")\n",
        "                        yield f\"Status: Processed Batch {batch_num + 1}: Empty.\"\n",
        "                        continue\n",
        "\n",
        "                    self.logger.info(\n",
        "                        f\"Processing Batch {batch_num + 1} ({len(df_batch_arrow)} pages)...\"\n",
        "                    )\n",
        "\n",
        "                    # Convert PyArrow RecordBatch to Pandas DataFrame for modification\n",
        "                    df_batch = df_batch_arrow.to_pandas()\n",
        "\n",
        "                    # Extract clean text\n",
        "                    df_batch[\"clean_text\"] = [\n",
        "                        self.text_extractor.extract(\n",
        "                            html_content\n",
        "                        )  # Changed from .extract_text to .extract\n",
        "                        for html_content in tqdm(\n",
        "                            df_batch[\"Content\"],  # Use \"Content\" as per DataLoader\n",
        "                            desc=\"Extracting Text\",\n",
        "                            leave=False,\n",
        "                            unit=\"docs\",\n",
        "                        )\n",
        "                    ]\n",
        "\n",
        "                    # Filter out pages where text extraction might have failed or resulted in empty strings\n",
        "                    original_count = len(df_batch)\n",
        "                    df_batch = df_batch[df_batch[\"clean_text\"].str.strip().astype(bool)]\n",
        "                    filtered_count = original_count - len(df_batch)\n",
        "                    if filtered_count > 0:\n",
        "                        self.logger.warning(\n",
        "                            f\"Filtered out {filtered_count} pages with no extractable text in Batch {batch_num + 1}.\"\n",
        "                        )\n",
        "\n",
        "                    if df_batch.empty:  # This .empty check is correct for the Pandas DataFrame after conversion\n",
        "                        self.logger.warning(\n",
        "                            f\"Batch {batch_num + 1} resulted in no extractable text after filtering, skipping.\"\n",
        "                        )\n",
        "                        yield f\"Status: Processed Batch {batch_num + 1}: No valid text extracted.\"\n",
        "                        continue\n",
        "\n",
        "                    # Generate embeddings\n",
        "                    self.logger.info(\n",
        "                        f\"Generating Embeddings for Batch {batch_num + 1}...\"\n",
        "                    )\n",
        "                    try:\n",
        "                        # Corrected: Use the actual method name 'generate' from EmbeddingGenerator\n",
        "                        df_batch[\"embedding\"] = self.embedding_generator.generate(\n",
        "                            df_batch[\"clean_text\"].tolist()\n",
        "                        ).tolist()\n",
        "                    except Exception as e:\n",
        "                        # Do not pass exc_info to ConsoleAndGradioLogger.error()\n",
        "                        # Instead, include the traceback in the message.\n",
        "                        error_message = f\"Error generating embeddings for Batch {batch_num + 1}: {e}\\n{traceback.format_exc()}\"\n",
        "                        self.logger.error(error_message)\n",
        "                        continue\n",
        "\n",
        "                    # --- ADDED SAVING LOGIC HERE ---\n",
        "                    # Save the generated embeddings after each batch\n",
        "                    self.data_saver.save_embeddings_batch(df_batch)  # Added this line\n",
        "                    # --- END OF ADDED LOGIC ---\n",
        "\n",
        "                    # The update_processed_urls method is not in the provided EmbeddingStateManager.\n",
        "                    # You will need to implement an 'update_processed_urls' method in\n",
        "                    # src/backend/data/embedding_state_manager.py if you intend to save the state.\n",
        "                    # For now, this line is commented out.\n",
        "                    # If you need resume functionality, ensure this method is implemented in EmbeddingStateManager:\n",
        "                    # def update_processed_urls(self, new_urls: list):\n",
        "                    #     \"\"\"\n",
        "                    #     Updates the persistent record of processed URLs.\n",
        "                    #     This method should append or merge `new_urls` with the existing state\n",
        "                    #     and save it to a durable storage (e.g., a JSON file or a dedicated DuckDB table).\n",
        "                    #     \"\"\"\n",
        "                    #     pass # Placeholder for actual implementation in embedding_state_manager.py\n",
        "\n",
        "                    # self.state_manager.update_processed_urls(df_batch[\"URL\"].tolist())\n",
        "                    total_processed_in_session += len(df_batch)\n",
        "\n",
        "                    yield f\"Status: Processed Batch {batch_num + 1} ({len(df_batch)} embeddings generated). Total in session: {total_processed_in_session}\"\n",
        "\n",
        "                self.logger.info(\"Embedding pipeline finished successfully.\")\n",
        "                yield \"Status: Pipeline Finished Successfully!\"\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = (\n",
        "                    f\"A critical pipeline error occurred: {e}\\n{traceback.format_exc()}\"\n",
        "                )\n",
        "                self.logger.error(error_message)\n",
        "                yield f\"Status: Critical Error! Check logs. Error: {e}\"\n",
        "                raise\n",
        "\n",
        "    pipeline = FixedEmbeddingPipeline(  # Use the fixed pipeline\n",
        "        config,\n",
        "        logger,\n",
        "        state_manager,\n",
        "        data_loader,\n",
        "        text_extractor,\n",
        "        embedding_generator,\n",
        "        data_saver,\n",
        "    )\n",
        "\n",
        "    final_status = \"Initializing...\"\n",
        "    for status in pipeline.run():\n",
        "        final_status = status\n",
        "        # Yield the current status and the full log content\n",
        "        yield status, log_stream.getvalue(), \"\"\n",
        "\n",
        "    # Generate final summary after the pipeline finishes\n",
        "    try:\n",
        "        # Ensure output_glob_path uses forward slashes for DuckDB even on Windows\n",
        "        output_glob_path = os.path.join(output_path, \"*.parquet\").replace(os.sep, \"/\")\n",
        "        total_embeddings = duckdb.query(\n",
        "            f\"SELECT COUNT(URL) FROM read_parquet('{output_glob_path}')\"\n",
        "        ).fetchone()[0]\n",
        "        summary_md = f\"### âœ… Pipeline Finished\\n\\n- **Final Status:** {final_status}\\n- **Total embeddings saved:** {total_embeddings}\\n- **Output location:** `{output_path}`\"\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Could not generate final summary. Error: {e}\")\n",
        "        summary_md = (\n",
        "            f\"### Pipeline Finished\\n\\n- Could not generate summary. Error: {e}\"\n",
        "        )\n",
        "\n",
        "    yield final_status, log_stream.getvalue(), summary_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KFXkND3AO9Z"
      },
      "outputs": [],
      "source": [
        "# File: embeddings_ui.ipynb - Cell 4\n",
        "# Build the Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ¤– Resumable Embedding Pipeline\")\n",
        "    gr.Markdown(\n",
        "        \"This tool reads HTML from Parquet files, cleans it, generates embeddings, and saves the results in batches. It can be stopped and resumed at any time.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## 1. Configuration\")\n",
        "            input_path_box = gr.Textbox(\n",
        "                label=\"Input Parquet Folder Path\", value=EmbeddingConfig.input_path\n",
        "            )\n",
        "            output_path_box = gr.Textbox(\n",
        "                label=\"Output Embeddings Directory Path\",\n",
        "                value=EmbeddingConfig.output_path,\n",
        "            )\n",
        "            batch_size_input = gr.Number(\n",
        "                minimum=1,\n",
        "                maximum=5,\n",
        "                value=EmbeddingConfig.batch_size,\n",
        "                step=1,\n",
        "                label=\"Batch Size\",\n",
        "                info=\"How many pages to process in memory at a time (1/2 recommended).\",\n",
        "            )\n",
        "            start_button = gr.Button(\n",
        "                \"ðŸš€ Start/Resume Embedding Generation\", variant=\"primary\"\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## 2. Status & Results\")\n",
        "            status_output = gr.Textbox(label=\"Current Status\", interactive=False)\n",
        "            log_output = gr.Textbox(\n",
        "                label=\"Detailed Logs\", interactive=False, lines=10, max_lines=20\n",
        "            )\n",
        "            summary_output = gr.Markdown(\"---\")\n",
        "\n",
        "    start_button.click(\n",
        "        fn=run_gradio_interface,\n",
        "        inputs=[input_path_box, output_path_box, batch_size_input],\n",
        "        outputs=[status_output, log_output, summary_output],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvi9yg7QARNf"
      },
      "outputs": [],
      "source": [
        "# File: embeddings_ui.ipynb - Cell 5\n",
        "# --- Launch the Application ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        # It's better to mount once at the very start of the notebook\n",
        "        # or main.py. If it's already mounted, no need to force_remount unless necessary.\n",
        "        # Check if already mounted before attempting to mount again (as in embeddings_ui.ipynb Cell 1)\n",
        "        if not os.path.exists(\"/content/drive/My Drive\"):\n",
        "            drive.mount(\"/content/drive/\")\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "        else:\n",
        "            print(\"Google Drive already mounted.\")\n",
        "\n",
        "        demo.launch(debug=True, share=True)\n",
        "    except Exception as e:\n",
        "        print(\"Could not launch Gradio demo in this environment.\")\n",
        "        print(e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
